<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>heavy tails | Fundamenta Nova</title>
    <link>/tag/heavy-tails/</link>
      <atom:link href="/tag/heavy-tails/index.xml" rel="self" type="application/rss+xml" />
    <description>heavy tails</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 01 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>heavy tails</title>
      <link>/tag/heavy-tails/</link>
    </image>
    
    <item>
      <title>Data and their misbehavior</title>
      <link>/post/006_samplestats/main/</link>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/006_samplestats/main/</guid>
      <description>


&lt;p&gt;To be honest, I use the clickbaity word “data” in the title when I really mean “sample statistics”. The point of this post is first illustrated using a sample mean, but applies to any estimate computed from data.&lt;/p&gt;
&lt;p&gt;If you ask a student in statistics to write a formula for “the mean” they may write the expression:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\bar{X} := \frac{1}{N}\sum_{i = 1}^N X_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are at least two “problems” with this answer. The first problem is that the above expression is not the mean of a distribution, but it is a sample statistic from some sample data &lt;span class=&#34;math inline&#34;&gt;\(\{X_i\}\)&lt;/span&gt;. It is not simply a constant number but a random variable in its own right. It has a distribution, a mean, a variance, quantiles, etc. If we had done a different “experiment” and had obtained different data the variable &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; may likely have come out with a different value. If the data were generated by a particularly misbehaved random process, then the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; may have little to do with a measure of centrality and may tell us less little about the distribution of any future data element &lt;span class=&#34;math inline&#34;&gt;\(X_{N+1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lesson:&lt;/strong&gt; Quantities estimated from data are usually random variables, and it’s necessary to understand their distributional properties before &lt;em&gt;inferring&lt;/em&gt; anything from the observed value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second problem however comes from the fact that in some cases the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; makes it indeed a very good approximation (in a precise sense) to a mean. If the data &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; are generated IID, with “small” variance, then the central limit theorem tells us that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; has a distribution that is approximately normal with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu = E[X_i]\)&lt;/span&gt;, and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = \frac{Var[X_i]}{N}\)&lt;/span&gt;. If the data is truly IID and generated from a distribution that is thin tailed then this limit is indeed a good approximation, even for modest data sizes. If this is the case and if we assume the limiting distribution is correct then the fact that normal distributions themselves have &lt;em&gt;very&lt;/em&gt; thin tails means that we can construct short yet extremely strong confidence intervals for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; centered on &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt;. So in this case, thin tails make it easy (&lt;strong&gt;far too easy&lt;/strong&gt;) for a student to forget the difference between the sample statistic &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; and the distributional parameter &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Unfortunately, in many interesting situations, data is not generated IID with small variance. In the previous sentence I personally would replace the word “many” by “most”. Nassim Taleb, the author of Fooled by Randomness and the Black Swan, many very well replace it by “all”. In any case, dividing data into “IID” and “non-IID”, or “definitely thin tailed” and “possibly heavy tailed” is like dividing animals into “elephants” and “non-elephants”.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: It’s true that IID data with finite variance will have a sample mean that converges in distribution to a normal. But the rate of convergence may be very slow for data with large (or numerically infinite) variance, or for data that is not exactly IID. Do not assume asymptotic normality unless 1) you are &lt;strong&gt;&lt;em&gt;absolutely&lt;/em&gt;&lt;/strong&gt; sure, or 2) you are absolutely sure that the mistake you would make from such an assumption would not be fatal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Economics and finance abound with examples. Market data is almost never IID in any observable sense almost as a corollary to market participants seeking out arbitrage opportunities, causing previously stable relationships to degrade (a variation of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Lucas_critique&#34;&gt;Lucas Critique&lt;/a&gt;). Insurance and Operational Risk are replete with examples of data that may well be generated IID (there would be no real way to know) but whose distributions have numerically infinite variance.&lt;/p&gt;
&lt;p&gt;In such interesting cases it could be a grave mistake to confuse sample statistics as an approximation for a particular parametric constant. For example, a Cauchy distribution is so heavy tailed that it has no finite mean. So the sample mean is not an approximation to any measure of central tendency of data generated by a Cauchy distribution, and would itself show enormous variation. In one experiment the sample mean may be 100. In the next experiment its value may very well be 100,000.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: There are no true tests for stationarity, path-independence, and IID-ness unless you make strong assumptions about the distributional properties of the data. In finance and economics, such assumptions are almost never justifiable. On the contrary, it’s very simple to give an argument for the non-stationarity, path-dependence, non-ARIMA structure of financial time series. Thus in finance and economics, in these cases, one is missing the one thing that makes data useful: limit theorems. No, the neural net you’re training will not solve the markets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Understanding the distributional properties of sample statistics is probably the most important part of the applied field of statistics. Mistaking sample statistics for constants throws away all of their distributional properties, such as their tendency to vary or to change with time, and assuming their distribution to follow from the central limit theorem may very well be incorrect.&lt;/p&gt;
</description>
      <content:encoded>


<p>To be honest, I use the clickbaity word “data” in the title when I really mean “sample statistics”. The point of this post is first illustrated using a sample mean, but applies to any estimate computed from data.</p>
<p>If you ask a student in statistics to write a formula for “the mean” they may write the expression:</p>
<p><span class="math display">\[
\bar{X} := \frac{1}{N}\sum_{i = 1}^N X_i
\]</span></p>
<p>There are at least two “problems” with this answer. The first problem is that the above expression is not the mean of a distribution, but it is a sample statistic from some sample data <span class="math inline">\(\{X_i\}\)</span>. It is not simply a constant number but a random variable in its own right. It has a distribution, a mean, a variance, quantiles, etc. If we had done a different “experiment” and had obtained different data the variable <span class="math inline">\(\bar{X}\)</span> may likely have come out with a different value. If the data were generated by a particularly misbehaved random process, then the distribution of <span class="math inline">\(\bar{X}\)</span> may have little to do with a measure of centrality and may tell us less little about the distribution of any future data element <span class="math inline">\(X_{N+1}\)</span>.</p>
<ul>
<li><strong>Lesson:</strong> Quantities estimated from data are usually random variables, and it’s necessary to understand their distributional properties before <em>inferring</em> anything from the observed value.</li>
</ul>
<p>The second problem however comes from the fact that in some cases the distribution of <span class="math inline">\(\bar{X}\)</span> makes it indeed a very good approximation (in a precise sense) to a mean. If the data <span class="math inline">\(X_i\)</span> are generated IID, with “small” variance, then the central limit theorem tells us that <span class="math inline">\(\bar{X}\)</span> has a distribution that is approximately normal with mean <span class="math inline">\(\mu = E[X_i]\)</span>, and variance <span class="math inline">\(\sigma^2 = \frac{Var[X_i]}{N}\)</span>. If the data is truly IID and generated from a distribution that is thin tailed then this limit is indeed a good approximation, even for modest data sizes. If this is the case and if we assume the limiting distribution is correct then the fact that normal distributions themselves have <em>very</em> thin tails means that we can construct short yet extremely strong confidence intervals for <span class="math inline">\(\mu\)</span> centered on <span class="math inline">\(\bar{X}\)</span>. So in this case, thin tails make it easy (<strong>far too easy</strong>) for a student to forget the difference between the sample statistic <span class="math inline">\(\bar{X}\)</span> and the distributional parameter <span class="math inline">\(\mu\)</span>.</p>
<p>Unfortunately, in many interesting situations, data is not generated IID with small variance. In the previous sentence I personally would replace the word “many” by “most”. Nassim Taleb, the author of Fooled by Randomness and the Black Swan, many very well replace it by “all”. In any case, dividing data into “IID” and “non-IID”, or “definitely thin tailed” and “possibly heavy tailed” is like dividing animals into “elephants” and “non-elephants”.</p>
<ul>
<li><strong>Lesson</strong>: It’s true that IID data with finite variance will have a sample mean that converges in distribution to a normal. But the rate of convergence may be very slow for data with large (or numerically infinite) variance, or for data that is not exactly IID. Do not assume asymptotic normality unless 1) you are <strong><em>absolutely</em></strong> sure, or 2) you are absolutely sure that the mistake you would make from such an assumption would not be fatal.</li>
</ul>
<p>Economics and finance abound with examples. Market data is almost never IID in any observable sense almost as a corollary to market participants seeking out arbitrage opportunities, causing previously stable relationships to degrade (a variation of the <a href="https://en.wikipedia.org/wiki/Lucas_critique">Lucas Critique</a>). Insurance and Operational Risk are replete with examples of data that may well be generated IID (there would be no real way to know) but whose distributions have numerically infinite variance.</p>
<p>In such interesting cases it could be a grave mistake to confuse sample statistics as an approximation for a particular parametric constant. For example, a Cauchy distribution is so heavy tailed that it has no finite mean. So the sample mean is not an approximation to any measure of central tendency of data generated by a Cauchy distribution, and would itself show enormous variation. In one experiment the sample mean may be 100. In the next experiment its value may very well be 100,000.</p>
<ul>
<li><strong>Lesson</strong>: There are no true tests for stationarity, path-independence, and IID-ness unless you make strong assumptions about the distributional properties of the data. In finance and economics, such assumptions are almost never justifiable. On the contrary, it’s very simple to give an argument for the non-stationarity, path-dependence, non-ARIMA structure of financial time series. Thus in finance and economics, in these cases, one is missing the one thing that makes data useful: limit theorems. No, the neural net you’re training will not solve the markets.</li>
</ul>
<p>Understanding the distributional properties of sample statistics is probably the most important part of the applied field of statistics. Mistaking sample statistics for constants throws away all of their distributional properties, such as their tendency to vary or to change with time, and assuming their distribution to follow from the central limit theorem may very well be incorrect.</p>
</content:encoded>	
    </item>
    
  </channel>
</rss>
