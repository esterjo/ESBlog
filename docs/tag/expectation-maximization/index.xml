<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>expectation maximization | Fundamenta Nova</title>
    <link>/tag/expectation-maximization/</link>
      <atom:link href="/tag/expectation-maximization/index.xml" rel="self" type="application/rss+xml" />
    <description>expectation maximization</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 01 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>expectation maximization</title>
      <link>/tag/expectation-maximization/</link>
    </image>
    
    <item>
      <title>Expectation Maximization, Part 2: Fitting Regularized Probit Regression using EM in C&#43;&#43;</title>
      <link>/post/005_em2_probit/main/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/post/005_em2_probit/main/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the &lt;a href=&#34;https://edsterjo.netlify.app/post/003_em1/main/&#34;&gt;first post&lt;/a&gt; in this series we discussed Expectation Maximization (EM) type algorithms. In the post &lt;a href=&#34;https://edsterjo.netlify.app/post/004_regularization/main/&#34;&gt;prior to this one&lt;/a&gt; we discussed regularization and showed how it leads to a bias-variance trade off in OLS models. Here we implement fitting for &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularized &lt;a href=&#34;https://en.wikipedia.org/wiki/Probit_model&#34;&gt;probit regression&lt;/a&gt; using EM. To make it more interesting we will code everything from scratch using the &lt;a href=&#34;http://eigen.tuxfamily.org/index.php?title=Main_Page&#34;&gt;Eigen&lt;/a&gt; linear algebra library, via &lt;a href=&#34;https://cran.r-project.org/web/packages/RcppEigen/index.html&#34;&gt;RcppEigen&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;probit-regression-as-a-censored-ols-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Probit Regression as a censored OLS model&lt;/h2&gt;
&lt;p&gt;In our first post on EM algorithms we emphasized that EM is particularly useful for models that have censored data. Suppose we have the following censored model. Suppose &lt;span class=&#34;math inline&#34;&gt;\(Y^*\)&lt;/span&gt; is a real valued random variable and &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; is a random vector with values in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^p\)&lt;/span&gt;. Suppose that we have the conditional relationship&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y^* \ \ | \ \ \vec{X} \sim \mathcal{N}(\ \langle \vec{X}  , \beta\rangle \ ,\ 1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(\mu, \sigma^2)\)&lt;/span&gt; denotes the univariate normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Here (and everywhere else) the symbol &lt;span class=&#34;math inline&#34;&gt;\(\langle v,w\rangle\)&lt;/span&gt; represents the Euclidean inner product (aka, &lt;a href=&#34;https://en.wikipedia.org/wiki/Dot_product&#34;&gt;dot product&lt;/a&gt;) of two vectors &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In this case we may write &lt;span class=&#34;math inline&#34;&gt;\(Y^* = \langle \vec{X} , \beta\rangle - \epsilon\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon \sim \mathcal{N}(0,1)\)&lt;/span&gt; is standard normal, which can be taken independent of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; or this distribution can be simply assumed conditional on &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Instead of observing &lt;span class=&#34;math inline&#34;&gt;\(Y^*\)&lt;/span&gt; in the data however, we observe the censored variable
&lt;span class=&#34;math display&#34;&gt;\[
Y := 
\begin{cases}
1, &amp;amp; \text{if} \ \ Y^* &amp;gt; 0 \\
0, &amp;amp; \text{otherwise}
\end{cases}
\]&lt;/span&gt;
Hence we have that &lt;span class=&#34;math inline&#34;&gt;\(Y \ |\vec{X} \sim \text{Bernoulli}(p)\)&lt;/span&gt; where
&lt;span class=&#34;math display&#34;&gt;\[
p = P(Y = 1 | \vec{X}) = P(Y^* &amp;gt; 0 | \vec{X}) = P\bigg(Y^* - \langle \vec{X},\beta\rangle &amp;gt; - \langle \vec{X},\beta\rangle\ \ \bigg|\ \ \vec{X}\bigg) = P(\epsilon &amp;lt; \langle \vec{X},\beta\rangle\ | \vec{X})
\]&lt;/span&gt;
Since &lt;span class=&#34;math inline&#34;&gt;\(\epsilon \sim \mathcal{N}(0,1)\)&lt;/span&gt; this last probability is equal to &lt;span class=&#34;math inline&#34;&gt;\(\Phi(\langle \vec{X},\beta\rangle)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; denotes the standard normal CDF. This derives the probit model.&lt;/p&gt;
&lt;p&gt;Before we proceed, notice 2 points which we won’t dwell on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the variance of &lt;span class=&#34;math inline&#34;&gt;\(Y^*\)&lt;/span&gt; had been &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 \ne 1\)&lt;/span&gt; then the value of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; would not be able to be estimated from data without knowing &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; since &lt;span class=&#34;math inline&#34;&gt;\(P(Y^* &amp;gt; 0 |\vec{X}) = P(Y^*/\sigma &amp;gt; 0 | \vec{X})\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;If the distributional relationship between &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; had been such that the error term &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; where a logistic random variable, instead of a normal one, then the censored problem would have become logistic regression.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the regression itself we assume that we have a data set &lt;span class=&#34;math inline&#34;&gt;\(\{ (y_i, \vec{x}_i)\}_{i = 1}^N\)&lt;/span&gt; consisting of samples generated independently of one another from a fixed multivariate distribution for &lt;span class=&#34;math inline&#34;&gt;\((Y, \vec{X})\)&lt;/span&gt; (i.e. we assume our data was sampled IID).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-probit-regression-via-em&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting Probit Regression via EM&lt;/h2&gt;
&lt;p&gt;Since probit regression arises from a censored normal OLS model, and since OLS is relatively easy to fit, probit regression is an excellent candidate for applying Expectation Maximization for fitting. A small difference will be that all of the probability densities involved will be conditional on the observed covariates &lt;span class=&#34;math inline&#34;&gt;\(\{\vec{x}_i\}_{i = 1}^N\)&lt;/span&gt; since regression is a conditional relationship.&lt;/p&gt;
&lt;p&gt;Let’s recall the standard EM algorithm for the case of the regression problem above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(m = 0\)&lt;/span&gt;. Make an initial estimate &lt;span class=&#34;math inline&#34;&gt;\(\beta_m\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Given the observed data &lt;span class=&#34;math inline&#34;&gt;\(\{(y_i, \vec{x}_i)\}_{i = 1}^N\)&lt;/span&gt; and pretending for the moment that our current guess &lt;span class=&#34;math inline&#34;&gt;\(\beta_m\)&lt;/span&gt; is correct, construct the conditional probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\{y^*_i\}\ \ |\ \ \{(y_i, \vec{x}_i)\},\ \beta_m)\)&lt;/span&gt; of the hidden data &lt;span class=&#34;math inline&#34;&gt;\(\{Y^*_i\}\)&lt;/span&gt; given all known information.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Using the distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\{y^*_i\}\ \ |\ \ \{(y_i, \vec{x}_i)\},\ \beta_m)\)&lt;/span&gt; construct the following estimator/approximation of the desired log-likelihood &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\}))\)&lt;/span&gt; for arbitrary values of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q(\beta | \beta_m) \ := \ \text{E}_{\{Y^*_i\}\ \ |\ \ \{(Y_i=y_i,\ \vec{X}_i=\vec{x}_i)\},\ \beta_m}  \big[ \log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\})) \big] 
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
= \int_{\mathcal{Y^*}} \log(p(\{y^*_i\} \ | \ \beta, \{\vec{x}_i\})) \ p(\{y^*_i\}\  |\  \{(y_i, \vec{x}_i)\}, \beta_m) \ dy_1^*...dy^*_N 
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Set &lt;span class=&#34;math inline&#34;&gt;\(\beta_{m+1}\)&lt;/span&gt; equal to a value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; that maximizes the current approximation &lt;span class=&#34;math inline&#34;&gt;\(Q(\beta|\beta_m)\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\}))\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Return to step 2 and repeat until some stopping criteria is met.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These formulas may seem difficult at the moment because they are in such a general form. As we specify things for our particular problem things will become more concrete. Now because &lt;span class=&#34;math inline&#34;&gt;\(Y^*|\vec{X}\sim \mathcal{N}(\ \langle \vec{X} , \beta\rangle \ ,\ 1)\)&lt;/span&gt; is a normal linear regression relationship we have
&lt;span class=&#34;math display&#34;&gt;\[
p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\}) = \frac{1}{(2\pi)^{N/2}}e^{\sum_{i=1}^N-\frac{1}{2}\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence
&lt;span class=&#34;math display&#34;&gt;\[
\log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\})) = const -\frac{1}{2} \sum_{i=1}^N\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and so the &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;-function is
&lt;span class=&#34;math display&#34;&gt;\[
Q(\beta | \beta_m) \ = \ \text{E}_{\{Y^*_i\}\ \ |\ \ \{(Y_i=y_i,\ \vec{X}_i=\vec{x}_i)\},\ \beta_m}  \bigg[ -\frac{1}{2} \sum_{i=1}^N\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2 \bigg] + const
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since the data samples are assumed IID we can apply the &lt;a href=&#34;https://edsterjo.netlify.app/post/003_em1/main/&#34;&gt;representation we derived in the first post in the series&lt;/a&gt; where instead of taking the expectation over all samples, we sum over the expectations of each individual sample:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q(\beta | \beta_m) \ = \ const - \frac{1}{2}\sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \bigg[\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2 \bigg]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that in the EM algorithm we do not actually need to evaluate this function. Instead in step 4 we simply want to find the value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; that maximizes it. In addition, as we discussed in the first post, if we wanted to incorporate a prior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\beta)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; for the purpose of regularization we would replace the problem of maximizing &lt;span class=&#34;math inline&#34;&gt;\(Q(\beta|\beta_m)\)&lt;/span&gt; by maximizing &lt;span class=&#34;math inline&#34;&gt;\(Q(\beta|\beta_m) + \log(p(\beta))\)&lt;/span&gt; instead. For the purpose of &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularization we could simply take &lt;span class=&#34;math inline&#34;&gt;\(\beta \sim \mathcal{N}(\vec{0}, \frac{1}{\lambda})\)&lt;/span&gt;. In that case
&lt;span class=&#34;math display&#34;&gt;\[
Q(\beta|\beta_m) + \log(p(\beta)) = \ const - \frac{1}{2}\sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \bigg[\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2 \bigg] - \frac{\lambda}{2}\langle \beta, \beta\rangle
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(the &lt;span class=&#34;math inline&#34;&gt;\(const\)&lt;/span&gt; may now depend on &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; which we always treat as a constant). We will focus on maximizing this regularized function, knowing that we can simply let &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 0\)&lt;/span&gt; to remove the regularization. At the maximizing point we need the gradient with respect to &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; to equal 0:
&lt;span class=&#34;math display&#34;&gt;\[
\nabla_{\beta} \ \big(Q(\beta|\beta_m) + \log(p(\beta))\big) = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(where the &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; represents the zero vector in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^p\)&lt;/span&gt;). Interchanging gradients first with the summation, then with the expectation (since all random variables have nice distributions) gives
&lt;span class=&#34;math display&#34;&gt;\[
0 = \sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \bigg[\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)\vec{x}_i \bigg] - \lambda \beta
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
= \sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \big[Y^*_i\big]\vec{x}_i - \sum_{i = 1}^N\langle \vec{x}_i  , \beta\rangle\vec{x}_i  - \lambda \beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X\in\mathbb{R}^{N\times p}\)&lt;/span&gt; be an &lt;span class=&#34;math inline&#34;&gt;\(N\times p\)&lt;/span&gt; matrix whose &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th row is &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}_i\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(Z\in \mathbb{R}^{N\times 1}\)&lt;/span&gt; be a vector with &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th component &lt;span class=&#34;math inline&#34;&gt;\(z_i = \text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m} \big[Y^*_i\big]\)&lt;/span&gt;. Then in matrix notation the above becomes
&lt;span class=&#34;math display&#34;&gt;\[
0 = X^TZ - X^TX\beta - \lambda\beta \\= X^TZ - (X^TX\beta + \lambda I)\beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This looks very familiar! It looks exactly like the normal equations of OLS if the target variable had been &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;. The value of &lt;span class=&#34;math inline&#34;&gt;\(Z = \big(\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m} \big[Y^*_i\big]\big)_{i=1}^N\)&lt;/span&gt; is just the value of &lt;span class=&#34;math inline&#34;&gt;\(Y^*\)&lt;/span&gt; we would guess given the values &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; of the censored variable and &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}_i\)&lt;/span&gt; of the covariates. Basically EM is telling us to impute a conditional average for the missing data &lt;span class=&#34;math inline&#34;&gt;\(Y^*\)&lt;/span&gt;, fit OLS, and repeat. Solving for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; gives
&lt;span class=&#34;math display&#34;&gt;\[
\beta_{m+1} = (X^TX + \lambda I)^{-1}X^TZ
\]&lt;/span&gt;
This can be implemented once we know the value of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(Z_i = \text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m} \big[Y^*_i\big]\)&lt;/span&gt; then this is just the mean of a truncated normal distribution:
&lt;span class=&#34;math display&#34;&gt;\[
\text{E}_{Y^*_i\ \ |\ \ Y_i=1,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \big[Y^*_i\big] = \langle \vec{x}_i  , \beta_m\rangle + \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{1 - \Phi(-\langle \vec{x}_i  , \beta_m\rangle)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{E}_{Y^*_i\ \ |\ \ Y_i=0,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \big[Y^*_i\big] = \langle \vec{x}_i  , \beta_m\rangle - \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{\Phi(-\langle \vec{x}_i  , \beta_m\rangle)}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; are the standard normal PDF and CDF respectively. Therefore, we can summarize the EM algorithm for Probit Regression as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(m = 0\)&lt;/span&gt;. Make an initial estimate &lt;span class=&#34;math inline&#34;&gt;\(\beta_m\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Impute the censored data according to&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
z_i =
\begin{cases}
\langle \vec{x}_i  , \beta_m\rangle + \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{1 - \Phi(-\langle \vec{x}_i  , \beta_m\rangle)}, &amp;amp; \text{if} \ \ y_i = 1 \\
\langle \vec{x}_i  , \beta_m\rangle - \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{\Phi(-\langle \vec{x}_i  , \beta_m\rangle)}, &amp;amp; \text{if} \ \ y_i = 0
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Solve the regularized OLS problem to update &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\beta_{m+1} = (X^TX + \lambda I)^{-1}X^TZ
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Return to step 2 and repeat until some stopping criteria is met.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;implementing-in-c-using-eigen&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implementing in C++ using Eigen&lt;/h2&gt;
&lt;p&gt;The algorithm above is easily implementable in R, Numpy, Matlab, etc., but for fun we’ll implement it in C++ using the Eigen linear algebra library. We do this in Rmarkdown using the &lt;a href=&#34;https://cran.r-project.org/web/packages/RcppEigen/index.html&#34;&gt;RcppEigen&lt;/a&gt; package in R.&lt;/p&gt;
&lt;p&gt;First we include the necessary header files. Here &lt;code&gt;RcppEigen.h&lt;/code&gt; includes the Eigen library itself, as well as all the necessary boilerplate code of Rcpp to integrate Eigen (and C++) with R.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;RcppEigen.h&amp;gt;
#include &amp;lt;algorithm&amp;gt;
#include &amp;lt;limits&amp;gt;

// [[Rcpp::depends(RcppEigen)]]

using namespace Eigen;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we define two functions that will be needed in computing the vector &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, whose computation requires the normal distribution’s PDF and CDF. We’ll use R’s own built-in functions &lt;code&gt;dnorm&lt;/code&gt; and &lt;code&gt;pnorm&lt;/code&gt;. These functions are written in C or Fortran (and hence can be called from any language with a C interface) and are very well tested. So instead of rolling our own versions we may as well use R’s.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// [[Rcpp::export]]
double positive(const double mu) 
{
    double num = R::dnorm(mu, 0, 1, false);
    double den = 1 - R::pnorm(-mu, 0, 1, true, false);
    
    return mu + num/den;
}

// [[Rcpp::export]]
double negative(double mu)
{
    double num = R::dnorm(mu, 0, 1, false);
    double den = R::pnorm(-mu, 0, 1, true, false);
    
    return mu - num/den;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we want a function that actually computes the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; vector, given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_m\)&lt;/span&gt;. We could use some of Eigen’s nifty &lt;a href=&#34;https://eigen.tuxfamily.org/dox/TopicCustomizing_NullaryExpr.html&#34;&gt;nullary expressions&lt;/a&gt;, but a simple for-loop with a ternay conditional will do. We also create a wrapper function so that we can test from R:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;VectorXd impute(const MatrixXd&amp;amp; X,
                const VectorXd&amp;amp; beta,
                const VectorXi&amp;amp; Y)
{
    VectorXd Z = X * beta;
    
    // If Y(i) is non-zero use the function 
    // positive, else use negative
    for(int i = 0; i != Z.size(); ++i)
        Z(i) = (Y(i) == (int)1) ? (positive(Z(i))) : (negative(Z(i)));
    
    return Z;
}

// A wrapper function to test from R
// [[Rcpp::export]]
VectorXd impute_test(const Map&amp;lt;MatrixXd&amp;gt; X,
                     const Map&amp;lt;VectorXd&amp;gt; beta,
                     const Map&amp;lt;VectorXi&amp;gt; Y)
{
    return impute(X, beta, Y);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comparing to an R implementation is trivial:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X = matrix(rnorm(100*10), ncol = 10)
beta = matrix(rnorm(10), ncol = 1)
Y = matrix(sample(c(0L,1L), size = 100, replace = T), ncol = 1)

# Using the C++ implementation
Z.cpp = impute_test(X, beta, Y)

# Building one in in R
imputeR = function(X, beta, Y)
{
    positiveR = function(Z){Z + dnorm(Z)/(1-pnorm(-Z))}
    negativeR = function(Z){Z - dnorm(Z)/pnorm(-Z)}
    
    Z = X %*% beta
    return(ifelse((Y == 1L), positiveR(Z), negativeR(Z)))
}

Z.r = imputeR(X, beta, Y)

# Checking range of values
print(summary(Z.cpp - Z.r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        V1            
##  Min.   :-5.773e-15  
##  1st Qu.:-2.776e-17  
##  Median : 0.000e+00  
##  Mean   : 4.119e-16  
##  3rd Qu.: 0.000e+00  
##  Max.   : 3.375e-14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not bad. Next we need a function to compute (regularized) least squares. For the solution we use the normal equations. According to the &lt;a href=&#34;https://eigen.tuxfamily.org/dox-devel/group__LeastSquares.html&#34;&gt;Eigen tutorial on the matter&lt;/a&gt; the normal equations are the fastest but least numerically stable option. For us this is good enough. Again we create a small wrapper to test from R:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;VectorXd RLS(const VectorXd&amp;amp; Z, 
             const MatrixXd&amp;amp; X, 
             const double lambda = 0.0)
{
    // Creating an identity matrix
    MatrixXd lambda_eye = lambda * MatrixXd::Identity(X.cols(), X.cols());
    
    // Using (regularized) normal equations
    if(lambda &amp;gt;= std::numeric_limits&amp;lt;double&amp;gt;::min())
        return (X.transpose() * X + lambda_eye).ldlt().solve(X.transpose() * Z);
    
    return (X.transpose() * X).ldlt().solve(X.transpose() * Z);
}

// [[Rcpp::export]]
VectorXd RLS_test(const Map&amp;lt;VectorXd&amp;gt; Z, 
                  const Map&amp;lt;MatrixXd&amp;gt; X,
                  double lambda = 0.0)
{
    return RLS(Z, X, lambda);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Testing in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
X = matrix(rnorm(100*10), ncol = 10)
beta = matrix(rnorm(10), ncol = 1)
Z = X %*% beta + matrix(rnorm(100), ncol = 1)

beta_hat.r = lm.fit(X, Z, method = &amp;quot;qr&amp;quot;)$coefficients
beta_hat.cpp = RLS_test(Z, X, 0)

print(data.frame(beta = beta, 
                 beta_hat.r = beta_hat.r, 
                 beta_hat.cpp = beta_hat.cpp))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           beta beta_hat.r beta_hat.cpp
## x1  -1.2053334 -1.2721727   -1.2721727
## x2   0.3014667  0.2774444    0.2774444
## x3  -1.5391452 -1.5961346   -1.5961346
## x4   0.6353707  0.5781086    0.5781086
## x5   0.7029518  0.8563414    0.8563414
## x6  -1.9058829 -2.0154512   -2.0154512
## x7   0.9389214  0.8108957    0.8108957
## x8  -0.2244921 -0.3825277   -0.3825277
## x9  -0.6738168 -0.7562696   -0.7562696
## x10  0.4457874  0.4209543    0.4209543&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(beta_hat.cpp - beta_hat.r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -8.882e-16 -6.800e-16 -5.551e-17  4.996e-17  3.053e-16  2.220e-15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not bad at all. Note that R’s very powerful &lt;code&gt;lm.fit&lt;/code&gt; function uses QR decomposition to solve the least squares problem. This method is a bit slower in principle than the &lt;span class=&#34;math inline&#34;&gt;\(LDL^T\)&lt;/span&gt; decomposition we used for the normal equations above, but it’s also higher quality numerically.&lt;/p&gt;
&lt;p&gt;As for a test with a non-trivial regularization constant:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lambda = 15
beta_hat_reg.r = solve(t(X) %*% X + lambda*diag(10), t(X) %*% Z)
beta_hat_reg.cpp = RLS_test(Z, X, lambda)

print(data.frame(beta = beta, 
                 beta_hat_reg.r = beta_hat_reg.r, 
                 beta_hat_reg.cpp = beta_hat_reg.cpp))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          beta beta_hat_reg.r beta_hat_reg.cpp
## 1  -1.2053334     -1.1372786       -1.1372786
## 2   0.3014667      0.2299293        0.2299293
## 3  -1.5391452     -1.3128364       -1.3128364
## 4   0.6353707      0.4366746        0.4366746
## 5   0.7029518      0.7691213        0.7691213
## 6  -1.9058829     -1.7122424       -1.7122424
## 7   0.9389214      0.7415632        0.7415632
## 8  -0.2244921     -0.2756274       -0.2756274
## 9  -0.6738168     -0.5224541       -0.5224541
## 10  0.4457874      0.4061769        0.4061769&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(beta_hat_reg.cpp - beta_hat_reg.r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        V1            
##  Min.   :-7.772e-16  
##  1st Qu.:-1.110e-16  
##  Median : 0.000e+00  
##  Mean   :-2.776e-17  
##  3rd Qu.: 2.082e-16  
##  Max.   : 3.331e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Very good. Again, we see that the regularized least squares estimates are biased away from the true values, and towards the 0 vector.&lt;/p&gt;
&lt;p&gt;Now we bring it all together into one algorithm:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// [[Rcpp::export]]
VectorXd Probit(const Map&amp;lt;VectorXi&amp;gt; Y, 
                const Map&amp;lt;MatrixXd&amp;gt; X, 
                double lambda = 0.0,
                int num_iter = 100)
{

    // Making sure Lambda is non-negative;
    lambda = std::max(lambda, 0.0);
    
    // Making sure the number of rows of X is the 
    // same as the number of rows of Y
    assert(Y.size() == X.rows());
    
    // Initialize beta to 0 values
    VectorXd beta = VectorXd::Zero(X.cols());
    
    // Iteration
    for(int i = 0; i &amp;lt; num_iter; ++i)
    {
        // Impute the Z vector
        VectorXd Z = impute(X, beta, Y);
        
        // Solve (regularized) Least Squares 
        beta = RLS(Z, X, lambda);
    }
    
    return beta;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we carry out a comparison of the base R implementation of (unregularized) probit regression against our implementation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)

N = 10000
p = 4

set.seed(1234)
S = matrix(rnorm(p*p), ncol = p)
S = t(S) %*% S

X = MASS::mvrnorm(n = N, mu = rep(0.0, times = p), Sigma = S)

beta = matrix((1:p)/2, ncol = 1)

Z = X %*% beta + matrix(rnorm(N))
Y = as.integer(Z &amp;gt; 0)

system.time(probit.cpp100 &amp;lt;- Probit(Y, X, 0.0, 100))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##    0.21    0.00    0.20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(probit.cpp10000 &amp;lt;- Probit(Y, X, 0.0, 10000))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   21.33    0.00   21.33&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(probit.glm &amp;lt;- glm(Y ~ X - 1, family = binomial(link = &amp;quot;probit&amp;quot;))$coefficients)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##    0.07    0.00    0.06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(data.frame(beta = beta, 
                 Cpp_100_iter = probit.cpp100,
                 Cpp_10000_iter = probit.cpp10000,
                 R = probit.glm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    beta Cpp_100_iter Cpp_10000_iter         R
## X1  0.5    0.4128896      0.5002905 0.5002889
## X2  1.0    0.8632397      0.9845270 0.9845285
## X3  1.5    1.2932036      1.5056988 1.5056998
## X4  2.0    1.7173029      1.9814520 1.9814547&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see the base R version is much faster, and converges much more quickly. Note, this is not due to weak compiler flags on our part. A local Makevars file in &lt;code&gt;Documents/.R&lt;/code&gt; overrides the default R build flags to use the optimizations &lt;code&gt;-O3 -march=native&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;No, instead &lt;a href=&#34;https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm&#34;&gt;&lt;code&gt;glm.fit&lt;/code&gt;&lt;/a&gt; uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares&#34;&gt;iteratively reweighted least squares (IRLS)&lt;/a&gt; to fit the model, not EM as can be seen in the &lt;a href=&#34;https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/glm.R&#34;&gt;source code&lt;/a&gt;. So EM is not a very fast algorithm to fit probit models. In the future we’ll implement IRLS in Eigen (or Fortran, or Julia).&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In the <a href="https://edsterjo.netlify.app/post/003_em1/main/">first post</a> in this series we discussed Expectation Maximization (EM) type algorithms. In the post <a href="https://edsterjo.netlify.app/post/004_regularization/main/">prior to this one</a> we discussed regularization and showed how it leads to a bias-variance trade off in OLS models. Here we implement fitting for <span class="math inline">\(L^2\)</span>-regularized <a href="https://en.wikipedia.org/wiki/Probit_model">probit regression</a> using EM. To make it more interesting we will code everything from scratch using the <a href="http://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a> linear algebra library, via <a href="https://cran.r-project.org/web/packages/RcppEigen/index.html">RcppEigen</a>.</p>
</div>
<div id="probit-regression-as-a-censored-ols-model" class="section level2">
<h2>Probit Regression as a censored OLS model</h2>
<p>In our first post on EM algorithms we emphasized that EM is particularly useful for models that have censored data. Suppose we have the following censored model. Suppose <span class="math inline">\(Y^*\)</span> is a real valued random variable and <span class="math inline">\(\vec{X}\)</span> is a random vector with values in <span class="math inline">\(\mathbb{R}^p\)</span>. Suppose that we have the conditional relationship</p>
<p><span class="math display">\[
Y^* \ \ | \ \ \vec{X} \sim \mathcal{N}(\ \langle \vec{X}  , \beta\rangle \ ,\ 1)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span> denotes the univariate normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Here (and everywhere else) the symbol <span class="math inline">\(\langle v,w\rangle\)</span> represents the Euclidean inner product (aka, <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a>) of two vectors <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span>.</p>
<p>In this case we may write <span class="math inline">\(Y^* = \langle \vec{X} , \beta\rangle - \epsilon\)</span>, where <span class="math inline">\(\epsilon \sim \mathcal{N}(0,1)\)</span> is standard normal, which can be taken independent of <span class="math inline">\(\vec{X}\)</span> or this distribution can be simply assumed conditional on <span class="math inline">\(\vec{X}\)</span>.</p>
<p>Instead of observing <span class="math inline">\(Y^*\)</span> in the data however, we observe the censored variable
<span class="math display">\[
Y := 
\begin{cases}
1, &amp; \text{if} \ \ Y^* &gt; 0 \\
0, &amp; \text{otherwise}
\end{cases}
\]</span>
Hence we have that <span class="math inline">\(Y \ |\vec{X} \sim \text{Bernoulli}(p)\)</span> where
<span class="math display">\[
p = P(Y = 1 | \vec{X}) = P(Y^* &gt; 0 | \vec{X}) = P\bigg(Y^* - \langle \vec{X},\beta\rangle &gt; - \langle \vec{X},\beta\rangle\ \ \bigg|\ \ \vec{X}\bigg) = P(\epsilon &lt; \langle \vec{X},\beta\rangle\ | \vec{X})
\]</span>
Since <span class="math inline">\(\epsilon \sim \mathcal{N}(0,1)\)</span> this last probability is equal to <span class="math inline">\(\Phi(\langle \vec{X},\beta\rangle)\)</span> where <span class="math inline">\(\Phi\)</span> denotes the standard normal CDF. This derives the probit model.</p>
<p>Before we proceed, notice 2 points which we won’t dwell on:</p>
<ul>
<li>If the variance of <span class="math inline">\(Y^*\)</span> had been <span class="math inline">\(\sigma^2 \ne 1\)</span> then the value of <span class="math inline">\(\sigma\)</span> would not be able to be estimated from data without knowing <span class="math inline">\(\beta\)</span> since <span class="math inline">\(P(Y^* &gt; 0 |\vec{X}) = P(Y^*/\sigma &gt; 0 | \vec{X})\)</span>.</li>
<li>If the distributional relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(\vec{X}\)</span> had been such that the error term <span class="math inline">\(\epsilon\)</span> where a logistic random variable, instead of a normal one, then the censored problem would have become logistic regression.</li>
</ul>
<p>For the regression itself we assume that we have a data set <span class="math inline">\(\{ (y_i, \vec{x}_i)\}_{i = 1}^N\)</span> consisting of samples generated independently of one another from a fixed multivariate distribution for <span class="math inline">\((Y, \vec{X})\)</span> (i.e. we assume our data was sampled IID).</p>
</div>
<div id="fitting-probit-regression-via-em" class="section level2">
<h2>Fitting Probit Regression via EM</h2>
<p>Since probit regression arises from a censored normal OLS model, and since OLS is relatively easy to fit, probit regression is an excellent candidate for applying Expectation Maximization for fitting. A small difference will be that all of the probability densities involved will be conditional on the observed covariates <span class="math inline">\(\{\vec{x}_i\}_{i = 1}^N\)</span> since regression is a conditional relationship.</p>
<p>Let’s recall the standard EM algorithm for the case of the regression problem above:</p>
<ul>
<li><p><strong>Step 1:</strong> Let <span class="math inline">\(m = 0\)</span>. Make an initial estimate <span class="math inline">\(\beta_m\)</span> for <span class="math inline">\(\beta\)</span>.</p></li>
<li><p><strong>Step 2:</strong> Given the observed data <span class="math inline">\(\{(y_i, \vec{x}_i)\}_{i = 1}^N\)</span> and pretending for the moment that our current guess <span class="math inline">\(\beta_m\)</span> is correct, construct the conditional probability distribution <span class="math inline">\(p(\{y^*_i\}\ \ |\ \ \{(y_i, \vec{x}_i)\},\ \beta_m)\)</span> of the hidden data <span class="math inline">\(\{Y^*_i\}\)</span> given all known information.</p></li>
<li><p><strong>Step 3:</strong> Using the distribution <span class="math inline">\(p(\{y^*_i\}\ \ |\ \ \{(y_i, \vec{x}_i)\},\ \beta_m)\)</span> construct the following estimator/approximation of the desired log-likelihood <span class="math inline">\(\log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\}))\)</span> for arbitrary values of <span class="math inline">\(\beta\)</span>:</p></li>
</ul>
<p><span class="math display">\[
Q(\beta | \beta_m) \ := \ \text{E}_{\{Y^*_i\}\ \ |\ \ \{(Y_i=y_i,\ \vec{X}_i=\vec{x}_i)\},\ \beta_m}  \big[ \log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\})) \big] 
\]</span>
<span class="math display">\[
= \int_{\mathcal{Y^*}} \log(p(\{y^*_i\} \ | \ \beta, \{\vec{x}_i\})) \ p(\{y^*_i\}\  |\  \{(y_i, \vec{x}_i)\}, \beta_m) \ dy_1^*...dy^*_N 
\]</span></p>
<ul>
<li><p><strong>Step 4:</strong> Set <span class="math inline">\(\beta_{m+1}\)</span> equal to a value of <span class="math inline">\(\beta\)</span> that maximizes the current approximation <span class="math inline">\(Q(\beta|\beta_m)\)</span> of <span class="math inline">\(\log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\}))\)</span>.</p></li>
<li><p><strong>Step 5:</strong> Return to step 2 and repeat until some stopping criteria is met.</p></li>
</ul>
<p>These formulas may seem difficult at the moment because they are in such a general form. As we specify things for our particular problem things will become more concrete. Now because <span class="math inline">\(Y^*|\vec{X}\sim \mathcal{N}(\ \langle \vec{X} , \beta\rangle \ ,\ 1)\)</span> is a normal linear regression relationship we have
<span class="math display">\[
p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\}) = \frac{1}{(2\pi)^{N/2}}e^{\sum_{i=1}^N-\frac{1}{2}\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2}
\]</span></p>
<p>Hence
<span class="math display">\[
\log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\})) = const -\frac{1}{2} \sum_{i=1}^N\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2
\]</span></p>
<p>and so the <span class="math inline">\(Q\)</span>-function is
<span class="math display">\[
Q(\beta | \beta_m) \ = \ \text{E}_{\{Y^*_i\}\ \ |\ \ \{(Y_i=y_i,\ \vec{X}_i=\vec{x}_i)\},\ \beta_m}  \bigg[ -\frac{1}{2} \sum_{i=1}^N\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2 \bigg] + const
\]</span></p>
<p>Since the data samples are assumed IID we can apply the <a href="https://edsterjo.netlify.app/post/003_em1/main/">representation we derived in the first post in the series</a> where instead of taking the expectation over all samples, we sum over the expectations of each individual sample:</p>
<p><span class="math display">\[
Q(\beta | \beta_m) \ = \ const - \frac{1}{2}\sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \bigg[\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2 \bigg]
\]</span></p>
<p>Note that in the EM algorithm we do not actually need to evaluate this function. Instead in step 4 we simply want to find the value of <span class="math inline">\(\beta\)</span> that maximizes it. In addition, as we discussed in the first post, if we wanted to incorporate a prior distribution <span class="math inline">\(p(\beta)\)</span> on <span class="math inline">\(\beta\)</span> for the purpose of regularization we would replace the problem of maximizing <span class="math inline">\(Q(\beta|\beta_m)\)</span> by maximizing <span class="math inline">\(Q(\beta|\beta_m) + \log(p(\beta))\)</span> instead. For the purpose of <span class="math inline">\(L^2\)</span>-regularization we could simply take <span class="math inline">\(\beta \sim \mathcal{N}(\vec{0}, \frac{1}{\lambda})\)</span>. In that case
<span class="math display">\[
Q(\beta|\beta_m) + \log(p(\beta)) = \ const - \frac{1}{2}\sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \bigg[\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2 \bigg] - \frac{\lambda}{2}\langle \beta, \beta\rangle
\]</span></p>
<p>(the <span class="math inline">\(const\)</span> may now depend on <span class="math inline">\(\lambda\)</span> which we always treat as a constant). We will focus on maximizing this regularized function, knowing that we can simply let <span class="math inline">\(\lambda = 0\)</span> to remove the regularization. At the maximizing point we need the gradient with respect to <span class="math inline">\(\beta\)</span> to equal 0:
<span class="math display">\[
\nabla_{\beta} \ \big(Q(\beta|\beta_m) + \log(p(\beta))\big) = 0
\]</span></p>
<p>(where the <span class="math inline">\(0\)</span> represents the zero vector in <span class="math inline">\(\mathbb{R}^p\)</span>). Interchanging gradients first with the summation, then with the expectation (since all random variables have nice distributions) gives
<span class="math display">\[
0 = \sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \bigg[\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)\vec{x}_i \bigg] - \lambda \beta
\]</span>
<span class="math display">\[
= \sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \big[Y^*_i\big]\vec{x}_i - \sum_{i = 1}^N\langle \vec{x}_i  , \beta\rangle\vec{x}_i  - \lambda \beta
\]</span></p>
<p>Let <span class="math inline">\(X\in\mathbb{R}^{N\times p}\)</span> be an <span class="math inline">\(N\times p\)</span> matrix whose <span class="math inline">\(i\)</span>-th row is <span class="math inline">\(\vec{x}_i\)</span>, and let <span class="math inline">\(Z\in \mathbb{R}^{N\times 1}\)</span> be a vector with <span class="math inline">\(i\)</span>-th component <span class="math inline">\(z_i = \text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m} \big[Y^*_i\big]\)</span>. Then in matrix notation the above becomes
<span class="math display">\[
0 = X^TZ - X^TX\beta - \lambda\beta \\= X^TZ - (X^TX\beta + \lambda I)\beta
\]</span></p>
<p>This looks very familiar! It looks exactly like the normal equations of OLS if the target variable had been <span class="math inline">\(Z\)</span>. The value of <span class="math inline">\(Z = \big(\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m} \big[Y^*_i\big]\big)_{i=1}^N\)</span> is just the value of <span class="math inline">\(Y^*\)</span> we would guess given the values <span class="math inline">\(y_i\)</span> of the censored variable and <span class="math inline">\(\vec{x}_i\)</span> of the covariates. Basically EM is telling us to impute a conditional average for the missing data <span class="math inline">\(Y^*\)</span>, fit OLS, and repeat. Solving for <span class="math inline">\(\beta\)</span> gives
<span class="math display">\[
\beta_{m+1} = (X^TX + \lambda I)^{-1}X^TZ
\]</span>
This can be implemented once we know the value of <span class="math inline">\(Z\)</span>. Since <span class="math inline">\(Z_i = \text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m} \big[Y^*_i\big]\)</span> then this is just the mean of a truncated normal distribution:
<span class="math display">\[
\text{E}_{Y^*_i\ \ |\ \ Y_i=1,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \big[Y^*_i\big] = \langle \vec{x}_i  , \beta_m\rangle + \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{1 - \Phi(-\langle \vec{x}_i  , \beta_m\rangle)}
\]</span></p>
<p><span class="math display">\[
\text{E}_{Y^*_i\ \ |\ \ Y_i=0,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \big[Y^*_i\big] = \langle \vec{x}_i  , \beta_m\rangle - \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{\Phi(-\langle \vec{x}_i  , \beta_m\rangle)}
\]</span>
where <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\Phi\)</span> are the standard normal PDF and CDF respectively. Therefore, we can summarize the EM algorithm for Probit Regression as:</p>
<ul>
<li><strong>Step 1:</strong> Let <span class="math inline">\(m = 0\)</span>. Make an initial estimate <span class="math inline">\(\beta_m\)</span> for <span class="math inline">\(\beta\)</span>.</li>
<li><strong>Step 2:</strong> Impute the censored data according to</li>
</ul>
<p><span class="math display">\[
z_i =
\begin{cases}
\langle \vec{x}_i  , \beta_m\rangle + \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{1 - \Phi(-\langle \vec{x}_i  , \beta_m\rangle)}, &amp; \text{if} \ \ y_i = 1 \\
\langle \vec{x}_i  , \beta_m\rangle - \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{\Phi(-\langle \vec{x}_i  , \beta_m\rangle)}, &amp; \text{if} \ \ y_i = 0
\end{cases}
\]</span></p>
<ul>
<li><strong>Step 3:</strong> Solve the regularized OLS problem to update <span class="math inline">\(\beta\)</span>:</li>
</ul>
<p><span class="math display">\[
\beta_{m+1} = (X^TX + \lambda I)^{-1}X^TZ
\]</span></p>
<ul>
<li><strong>Step 4:</strong> Return to step 2 and repeat until some stopping criteria is met.</li>
</ul>
</div>
<div id="implementing-in-c-using-eigen" class="section level2">
<h2>Implementing in C++ using Eigen</h2>
<p>The algorithm above is easily implementable in R, Numpy, Matlab, etc., but for fun we’ll implement it in C++ using the Eigen linear algebra library. We do this in Rmarkdown using the <a href="https://cran.r-project.org/web/packages/RcppEigen/index.html">RcppEigen</a> package in R.</p>
<p>First we include the necessary header files. Here <code>RcppEigen.h</code> includes the Eigen library itself, as well as all the necessary boilerplate code of Rcpp to integrate Eigen (and C++) with R.</p>
<pre class="cpp"><code>#include &lt;RcppEigen.h&gt;
#include &lt;algorithm&gt;
#include &lt;limits&gt;

// [[Rcpp::depends(RcppEigen)]]

using namespace Eigen;</code></pre>
<p>Next we define two functions that will be needed in computing the vector <span class="math inline">\(Z\)</span>, whose computation requires the normal distribution’s PDF and CDF. We’ll use R’s own built-in functions <code>dnorm</code> and <code>pnorm</code>. These functions are written in C or Fortran (and hence can be called from any language with a C interface) and are very well tested. So instead of rolling our own versions we may as well use R’s.</p>
<pre class="cpp"><code>// [[Rcpp::export]]
double positive(const double mu) 
{
    double num = R::dnorm(mu, 0, 1, false);
    double den = 1 - R::pnorm(-mu, 0, 1, true, false);
    
    return mu + num/den;
}

// [[Rcpp::export]]
double negative(double mu)
{
    double num = R::dnorm(mu, 0, 1, false);
    double den = R::pnorm(-mu, 0, 1, true, false);
    
    return mu - num/den;
}</code></pre>
<p>Next we want a function that actually computes the <span class="math inline">\(Z\)</span> vector, given <span class="math inline">\(X\)</span> and <span class="math inline">\(\beta_m\)</span>. We could use some of Eigen’s nifty <a href="https://eigen.tuxfamily.org/dox/TopicCustomizing_NullaryExpr.html">nullary expressions</a>, but a simple for-loop with a ternay conditional will do. We also create a wrapper function so that we can test from R:</p>
<pre class="cpp"><code>VectorXd impute(const MatrixXd&amp; X,
                const VectorXd&amp; beta,
                const VectorXi&amp; Y)
{
    VectorXd Z = X * beta;
    
    // If Y(i) is non-zero use the function 
    // positive, else use negative
    for(int i = 0; i != Z.size(); ++i)
        Z(i) = (Y(i) == (int)1) ? (positive(Z(i))) : (negative(Z(i)));
    
    return Z;
}

// A wrapper function to test from R
// [[Rcpp::export]]
VectorXd impute_test(const Map&lt;MatrixXd&gt; X,
                     const Map&lt;VectorXd&gt; beta,
                     const Map&lt;VectorXi&gt; Y)
{
    return impute(X, beta, Y);
}</code></pre>
<p>Comparing to an R implementation is trivial:</p>
<pre class="r"><code>X = matrix(rnorm(100*10), ncol = 10)
beta = matrix(rnorm(10), ncol = 1)
Y = matrix(sample(c(0L,1L), size = 100, replace = T), ncol = 1)

# Using the C++ implementation
Z.cpp = impute_test(X, beta, Y)

# Building one in in R
imputeR = function(X, beta, Y)
{
    positiveR = function(Z){Z + dnorm(Z)/(1-pnorm(-Z))}
    negativeR = function(Z){Z - dnorm(Z)/pnorm(-Z)}
    
    Z = X %*% beta
    return(ifelse((Y == 1L), positiveR(Z), negativeR(Z)))
}

Z.r = imputeR(X, beta, Y)

# Checking range of values
print(summary(Z.cpp - Z.r))</code></pre>
<pre><code>##        V1            
##  Min.   :-5.773e-15  
##  1st Qu.:-2.776e-17  
##  Median : 0.000e+00  
##  Mean   : 4.119e-16  
##  3rd Qu.: 0.000e+00  
##  Max.   : 3.375e-14</code></pre>
<p>Not bad. Next we need a function to compute (regularized) least squares. For the solution we use the normal equations. According to the <a href="https://eigen.tuxfamily.org/dox-devel/group__LeastSquares.html">Eigen tutorial on the matter</a> the normal equations are the fastest but least numerically stable option. For us this is good enough. Again we create a small wrapper to test from R:</p>
<pre class="cpp"><code>VectorXd RLS(const VectorXd&amp; Z, 
             const MatrixXd&amp; X, 
             const double lambda = 0.0)
{
    // Creating an identity matrix
    MatrixXd lambda_eye = lambda * MatrixXd::Identity(X.cols(), X.cols());
    
    // Using (regularized) normal equations
    if(lambda &gt;= std::numeric_limits&lt;double&gt;::min())
        return (X.transpose() * X + lambda_eye).ldlt().solve(X.transpose() * Z);
    
    return (X.transpose() * X).ldlt().solve(X.transpose() * Z);
}

// [[Rcpp::export]]
VectorXd RLS_test(const Map&lt;VectorXd&gt; Z, 
                  const Map&lt;MatrixXd&gt; X,
                  double lambda = 0.0)
{
    return RLS(Z, X, lambda);
}</code></pre>
<p>Testing in R:</p>
<pre class="r"><code>set.seed(1234)
X = matrix(rnorm(100*10), ncol = 10)
beta = matrix(rnorm(10), ncol = 1)
Z = X %*% beta + matrix(rnorm(100), ncol = 1)

beta_hat.r = lm.fit(X, Z, method = &quot;qr&quot;)$coefficients
beta_hat.cpp = RLS_test(Z, X, 0)

print(data.frame(beta = beta, 
                 beta_hat.r = beta_hat.r, 
                 beta_hat.cpp = beta_hat.cpp))</code></pre>
<pre><code>##           beta beta_hat.r beta_hat.cpp
## x1  -1.2053334 -1.2721727   -1.2721727
## x2   0.3014667  0.2774444    0.2774444
## x3  -1.5391452 -1.5961346   -1.5961346
## x4   0.6353707  0.5781086    0.5781086
## x5   0.7029518  0.8563414    0.8563414
## x6  -1.9058829 -2.0154512   -2.0154512
## x7   0.9389214  0.8108957    0.8108957
## x8  -0.2244921 -0.3825277   -0.3825277
## x9  -0.6738168 -0.7562696   -0.7562696
## x10  0.4457874  0.4209543    0.4209543</code></pre>
<pre class="r"><code>print(summary(beta_hat.cpp - beta_hat.r))</code></pre>
<pre><code>##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -8.882e-16 -6.800e-16 -5.551e-17  4.996e-17  3.053e-16  2.220e-15</code></pre>
<p>Not bad at all. Note that R’s very powerful <code>lm.fit</code> function uses QR decomposition to solve the least squares problem. This method is a bit slower in principle than the <span class="math inline">\(LDL^T\)</span> decomposition we used for the normal equations above, but it’s also higher quality numerically.</p>
<p>As for a test with a non-trivial regularization constant:</p>
<pre class="r"><code>lambda = 15
beta_hat_reg.r = solve(t(X) %*% X + lambda*diag(10), t(X) %*% Z)
beta_hat_reg.cpp = RLS_test(Z, X, lambda)

print(data.frame(beta = beta, 
                 beta_hat_reg.r = beta_hat_reg.r, 
                 beta_hat_reg.cpp = beta_hat_reg.cpp))</code></pre>
<pre><code>##          beta beta_hat_reg.r beta_hat_reg.cpp
## 1  -1.2053334     -1.1372786       -1.1372786
## 2   0.3014667      0.2299293        0.2299293
## 3  -1.5391452     -1.3128364       -1.3128364
## 4   0.6353707      0.4366746        0.4366746
## 5   0.7029518      0.7691213        0.7691213
## 6  -1.9058829     -1.7122424       -1.7122424
## 7   0.9389214      0.7415632        0.7415632
## 8  -0.2244921     -0.2756274       -0.2756274
## 9  -0.6738168     -0.5224541       -0.5224541
## 10  0.4457874      0.4061769        0.4061769</code></pre>
<pre class="r"><code>print(summary(beta_hat_reg.cpp - beta_hat_reg.r))</code></pre>
<pre><code>##        V1            
##  Min.   :-7.772e-16  
##  1st Qu.:-1.110e-16  
##  Median : 0.000e+00  
##  Mean   :-2.776e-17  
##  3rd Qu.: 2.082e-16  
##  Max.   : 3.331e-16</code></pre>
<p>Very good. Again, we see that the regularized least squares estimates are biased away from the true values, and towards the 0 vector.</p>
<p>Now we bring it all together into one algorithm:</p>
<pre class="cpp"><code>// [[Rcpp::export]]
VectorXd Probit(const Map&lt;VectorXi&gt; Y, 
                const Map&lt;MatrixXd&gt; X, 
                double lambda = 0.0,
                int num_iter = 100)
{

    // Making sure Lambda is non-negative;
    lambda = std::max(lambda, 0.0);
    
    // Making sure the number of rows of X is the 
    // same as the number of rows of Y
    assert(Y.size() == X.rows());
    
    // Initialize beta to 0 values
    VectorXd beta = VectorXd::Zero(X.cols());
    
    // Iteration
    for(int i = 0; i &lt; num_iter; ++i)
    {
        // Impute the Z vector
        VectorXd Z = impute(X, beta, Y);
        
        // Solve (regularized) Least Squares 
        beta = RLS(Z, X, lambda);
    }
    
    return beta;
}</code></pre>
<p>Below we carry out a comparison of the base R implementation of (unregularized) probit regression against our implementation.</p>
<pre class="r"><code>library(MASS)

N = 10000
p = 4

set.seed(1234)
S = matrix(rnorm(p*p), ncol = p)
S = t(S) %*% S

X = MASS::mvrnorm(n = N, mu = rep(0.0, times = p), Sigma = S)

beta = matrix((1:p)/2, ncol = 1)

Z = X %*% beta + matrix(rnorm(N))
Y = as.integer(Z &gt; 0)

system.time(probit.cpp100 &lt;- Probit(Y, X, 0.0, 100))</code></pre>
<pre><code>##    user  system elapsed 
##    0.21    0.00    0.20</code></pre>
<pre class="r"><code>system.time(probit.cpp10000 &lt;- Probit(Y, X, 0.0, 10000))</code></pre>
<pre><code>##    user  system elapsed 
##   21.33    0.00   21.33</code></pre>
<pre class="r"><code>system.time(probit.glm &lt;- glm(Y ~ X - 1, family = binomial(link = &quot;probit&quot;))$coefficients)</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>##    user  system elapsed 
##    0.07    0.00    0.06</code></pre>
<pre class="r"><code>print(data.frame(beta = beta, 
                 Cpp_100_iter = probit.cpp100,
                 Cpp_10000_iter = probit.cpp10000,
                 R = probit.glm))</code></pre>
<pre><code>##    beta Cpp_100_iter Cpp_10000_iter         R
## X1  0.5    0.4128896      0.5002905 0.5002889
## X2  1.0    0.8632397      0.9845270 0.9845285
## X3  1.5    1.2932036      1.5056988 1.5056998
## X4  2.0    1.7173029      1.9814520 1.9814547</code></pre>
<p>We see the base R version is much faster, and converges much more quickly. Note, this is not due to weak compiler flags on our part. A local Makevars file in <code>Documents/.R</code> overrides the default R build flags to use the optimizations <code>-O3 -march=native</code>.</p>
<p>No, instead <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm"><code>glm.fit</code></a> uses <a href="https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares">iteratively reweighted least squares (IRLS)</a> to fit the model, not EM as can be seen in the <a href="https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/glm.R">source code</a>. So EM is not a very fast algorithm to fit probit models. In the future we’ll implement IRLS in Eigen (or Fortran, or Julia).</p>
</div>
</content:encoded>	
    </item>
    
    <item>
      <title>Expectation Maximization, Part 1: Motivation and Recipe</title>
      <link>/post/003_em1/main/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/003_em1/main/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is the first in a series of posts on Expectation Maximization (EM) type algorithms. Our goal will be to motivate some of the theory behind these algorithms. In later posts we will implement examples in C++, often with the help of the &lt;a href=&#34;http://eigen.tuxfamily.org/index.php?title=Main_Page&#34;&gt;Eigen&lt;/a&gt; linear algebra library.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-likelihood&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximum likelihood&lt;/h2&gt;
&lt;p&gt;A large subset of statistics is concerned with determining properties of a distribution by using data that is assumed to be generated by that distribution. A common example is &lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34;&gt;Maximum Likelihood Estimation&lt;/a&gt; (MLE). Here one assumes that a vector of observed data &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\in\mathbb{R}^N\)&lt;/span&gt; is the realization of a random vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; with a probability density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{X} \ | \ \theta)\)&lt;/span&gt; that depends on a vector of parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. MLE amounts to estimating &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; with the value that makes this probability density has high as possible for the observed data:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} := \text{argmax}_{\theta} \ \ p(\vec{x} \ | \ \theta)
\]&lt;/span&gt;
As a function of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the density &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}(\theta; \vec{x}) := p(\vec{x} \ | \ \theta)\)&lt;/span&gt; is called the likelihood. Because probability densities are positive for the realized values &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, the above problem is equivalent to maximizing the logarithm of the likelihood:
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{x} \ | \ \theta))
\]&lt;/span&gt;
(The main practical reason behind this log transformation is that it often makes the problem easier numerically. The theoretical advantage is that it ties MLE to the theory of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fisher_information&#34;&gt;Fisher Information&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dependence-structures-and-problems-with-hidden-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dependence structures and problems with hidden variables&lt;/h2&gt;
&lt;p&gt;The situation in the last section can be summarized by the simple dependence structure (or &lt;em&gt;Markov&lt;/em&gt; diagram) &lt;span class=&#34;math inline&#34;&gt;\(\theta \to \vec{X}\)&lt;/span&gt;. That is, given the value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; we can determined the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, namely &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{X} \ | \ \theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, in many applications we may only have partial observations of the data we want, with some of the relevant information remaining unobserved/hidden. For example, suppose 100 identical and independent dice are thrown in an experiment. The dice are not necessarily uniformly weighted, with probabilities of landing 1,2,…,6 given by &lt;span class=&#34;math inline&#34;&gt;\(\theta = [p_1, p_2,...,p_6]\)&lt;/span&gt;. Suppose the dice land with values represented by &lt;span class=&#34;math inline&#34;&gt;\(\vec{X} = [X_1, X_2, ... X_{100}]\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; being the number the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; die lands on. Suppose also that in the experiment we are only able to observe whether each die landed on an even or odd number. That is, we observe a vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}\)&lt;/span&gt; given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = X_i \mod 2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i \in \{1, 2, ... 100\}\)&lt;/span&gt;. In this case the dependence structure is a little more complex: &lt;span class=&#34;math inline&#34;&gt;\(\theta \to \vec{X} \to \vec{Y}\)&lt;/span&gt;. That is, once we know the value of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; we can fully specify the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}\)&lt;/span&gt; without knowing the value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. We would have the Markov property for densities:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(\vec{y} \ | \ \vec{x}, \theta) = p(\vec{y} \ | \ \vec{x})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In general, we have a dependence structure given by &lt;span class=&#34;math inline&#34;&gt;\(\theta \to \vec{X} \to \vec{Y}\)&lt;/span&gt;, we observe only &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}=\vec{y}\)&lt;/span&gt; and we want to estimate the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. The MLE estimator would be:
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{y} \ | \ \theta))
\]&lt;/span&gt;
All of the theory of MLE applies in this case. In the example above this would be relatively easy. However, there are times this maximization problem is very difficult. Often the density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{y} \ | \ \theta)\)&lt;/span&gt; may be much more complicated than the density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x} \ | \ \theta)\)&lt;/span&gt; of the hidden data that we wish we had.&lt;/p&gt;
&lt;p&gt;In such as situation, if we knew &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; then we can replace the above problem with &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt;. In fact, we wouldn’t even need to know exactly what the value of &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; is but only what the value of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; is for a given &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-general-recipe-for-em-algorithms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A general recipe for EM algorithms&lt;/h2&gt;
&lt;p&gt;The idea of EM is indeed to try and maximize &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{y} \ | \ \theta))\)&lt;/span&gt;, but because we do not know &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; to instead use an approximation/estimate of it.&lt;/p&gt;
&lt;p&gt;How to approximate such an expression? The quantity &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; is a random variable (depending on the unknown value &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;). To estimate it in a meaningful way we need to use the most informative distribution related to &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt;. Because we know &lt;span class=&#34;math inline&#34;&gt;\(\vec{y}\)&lt;/span&gt; the best such distribution is &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The problem is this distribution (or any other) will necessarily depend &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, which we do not know! At first this seems like a circular trap, because to estimate &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta)\)&lt;/span&gt; we need to know &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, but to estimate &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; we need to know &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt;. However the trap hints at a solution: simply alternate between estimating the random variable &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; using a current guess of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and then use this updated estimate of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; to update our guess of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. More formally we can summarize EM in 5 steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(m = 0\)&lt;/span&gt;. Make an initial estimate &lt;span class=&#34;math inline&#34;&gt;\(\theta_m\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Given the observed data &lt;span class=&#34;math inline&#34;&gt;\(\vec{y}\)&lt;/span&gt; and pretending for the moment that our current guess &lt;span class=&#34;math inline&#34;&gt;\(\theta_m\)&lt;/span&gt; is correct, construct the conditional probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt; of the hidden data &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; given all known information.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Using the distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt; construct an estimator/approximation of the desired log-likelihood &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. We denote this approximation by &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Set &lt;span class=&#34;math inline&#34;&gt;\(\theta_{m+1}\)&lt;/span&gt; equal to a value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that maximizes the current approximation &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m)\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Return to step 2 and repeat until some stopping criteria is met.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Practically speaking, this algorithm would be applied when each of these steps is significantly easier than the original MLE problem of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{y} \ | \ \theta))\)&lt;/span&gt;. As a general example, this is often the case when the model is linear with respect to &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, but the information loss of going from &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}\)&lt;/span&gt; is nonlinear and non-invertible (we’ll give examples in later posts).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;constructing-an-estimator-for-logpvecx-theta&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Constructing an estimator for &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{X} \ | \ \theta))\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;How do we fill in the blank left by step 3 above? That is, how do we use the probability density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt; to estimate the value of the random variable &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{X} \ | \ \theta))\)&lt;/span&gt;? Two possibilities come to mind.&lt;/p&gt;
&lt;div id=&#34;point-estimate-type-em&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Point-estimate type EM&lt;/h3&gt;
&lt;p&gt;One possibility is to let
&lt;span class=&#34;math display&#34;&gt;\[
\vec{x}_m = \vec{x}_m(\vec{y}, \theta_m) := \text{argmax}_{\vec{x}} \ p(\vec{x}|\vec{y},\theta_m)
\]&lt;/span&gt;
and then define
&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta | \theta_m) := \log(p(\vec{x}_m \ | \ \theta))
\]&lt;/span&gt;
This is called point-estimate EM. Here we use “the most likely” value of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; as determined by the density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt; and then impute this value into our log-likelihood that we want to maximize. Another possibility would be to let&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{x}_m = \vec{x}_m(\vec{y}, \theta_m) := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \vec{X}\big]
\]&lt;/span&gt;
and let &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; be as before.&lt;/p&gt;
&lt;p&gt;The idea of these type of EM algorithms is to first estimate the missing data &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; and then impute the result into &lt;span class=&#34;math inline&#34;&gt;\(log(p(\vec{x}\ |\ \theta))\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;expectation-em-i.e.-standard-em&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Expectation EM (i.e. standard EM)&lt;/h3&gt;
&lt;p&gt;There is a theoretically more elegant way. As mentioned earlier, we do not in fact need an estimate of the missing data &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt;. One of the best ways to estimate the value of a random variable with respect to a conditional distribution is to simply compute the conditional expectation of that variable with respect to that conditional distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta | \theta_m) \ := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] = \int_{\mathcal{X}} \log(p(\vec{x} \ | \ \theta)) \ p(\vec{x}|\vec{y},\theta_m) \ d\vec{x} 
\]&lt;/span&gt;
Here we’re computing the mean of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{X} \ | \ \theta))\)&lt;/span&gt; with respect to the density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt;. As is common when using expectations, this second method has some advantages we’ll see later. When we refer to EM we will always mean this case, unless otherwise specified.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;qthetatheta_m-for-i.i.d.-samples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m)\)&lt;/span&gt; for I.I.D. samples&lt;/h2&gt;
&lt;p&gt;So far everything has been rather general, applying to any random vectors &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}\)&lt;/span&gt;. Many problems assume that data are generated independently and identically distributed (I.I.D.) so it’s helpful to have a formulation for this particular case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proposition:&lt;/strong&gt; Suppose that the components of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; are IID (given &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;), that is:
&lt;span class=&#34;math display&#34;&gt;\[
p(\vec{x}|\theta) = \prod_{i = 1}^N p(x_i|\theta) \  \qquad \forall x, \theta
\]&lt;/span&gt;
Suppose also that the dependence structure &lt;span class=&#34;math inline&#34;&gt;\(\theta \to \vec{X} \to \vec{Y}\)&lt;/span&gt; splits into subgraphs as &lt;span class=&#34;math inline&#34;&gt;\(\theta \to X_i \to Y_i\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i = 1, 2, ..., N\)&lt;/span&gt;. This just means that given &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;, the distribution &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; is independent of all other variables:
&lt;span class=&#34;math display&#34;&gt;\[
p(y_i|\vec{x}, \theta, y_1, y_2, ..., y_{i-1}, y_{i+1}, ..., y_N) = p(y_i|x_i)
\]&lt;/span&gt;
Then &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m):= \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m} \big[ \log(p(\vec{X} \ | \ \theta)) \big]\)&lt;/span&gt; can be written as:
&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta|\theta_m) = \sum_{i=1}^N Q_i(\theta|\theta_m)
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
Q_i(\theta|\theta_m) := \ \text{E}_{X_i \ | \ Y_i \ = \ y_i, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big].
\]&lt;/span&gt;
&lt;strong&gt;Proof:&lt;/strong&gt; The proof begins by showing that the joint elements &lt;span class=&#34;math inline&#34;&gt;\((X_i,Y_i)\)&lt;/span&gt; are independent across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, that is:
&lt;span class=&#34;math display&#34;&gt;\[
p(\vec{x},\vec{y}|\theta) = \prod_{i = 1}^N p(x_i, y_i|\theta). 
\]&lt;/span&gt;
To prove this we start by applying the multiplication theorem for probability densities:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \begin{aligned}

p(\vec{x},\vec{y}|\theta) &amp;amp;= p(y_1|y_2,y_3, ..., y_N, \vec{x},\theta)...p(y_N|\vec{x},\theta)p(\vec{x}|\theta) \qquad \text{by the multiplication theorem}\\

    &amp;amp;= p(y_1|x_1,\theta)...p(y_N|x_N,\theta)p(\vec{x}|\theta) \qquad \text{by conditional independence but keeping theta} \\
    &amp;amp;= p(\vec{x}|\theta)\prod_{i=1}^Np(y_i|x_i,\theta) \\
    &amp;amp;=\prod_{i=1}^Np(y_i|x_i,\theta)p(x_i|\theta) \qquad \text{by independence of the x&amp;#39;s} \\
    &amp;amp;= \prod_{i=1}^Np(y_i,x_i|\theta).
  \end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Next we have that for each &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \begin{aligned}
  
  p(x_i|\vec{y},\theta) &amp;amp;= \frac{p(x_i,\vec{y}|\theta)}{p(\vec{y}|\theta)} \qquad \text{by Bayes} \\
  
  &amp;amp;= \frac{\int p(\vec{x},\vec{y}|\theta)dx_1...dx_{i-1}dx_{i+1}...dx_n}{\int p(\vec{x},\vec{y}|\theta)d\vec{x}} \\
  
  &amp;amp;= \frac{\int \prod_{j=1}^Np(y_j,x_j|\theta)dx_1...dx_{i-1}dx_{i+1}...dx_n}{\int \prod_{j=1}^Np(y_j,x_j|\theta)d\vec{x}} \\
  
  &amp;amp;= \frac{p(x_i,y_i|\theta) \prod_{j=1, j \ne i}^N \int p(y_j,x_j|\theta)dx_j}{\prod_{j=1}^N\int p(y_j,x_j|\theta) dx_j} \\
  
  &amp;amp;= \frac{p(x_i,y_i|\theta) \prod_{j=1, j \ne i}^N p(y_j|\theta)}{\prod_{j=1}^N p(y_j|\theta)}\\
  
  &amp;amp;= \frac{p(x_i,y_i|\theta)}{p(y_i|\theta)}\\
  
  &amp;amp;= p(x_i|y_i,\theta)

  \end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence &lt;span class=&#34;math inline&#34;&gt;\(p(x_i|\vec{y},\theta) = p(x_i|y_i,\theta)\)&lt;/span&gt;. Therefore we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \begin{aligned}
  
  Q(\theta, \theta_m) &amp;amp;= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] \\
  
  &amp;amp;= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(\prod_{i = 1}^N p(X_i \ | \ \theta)) \big] \\
  
  &amp;amp;= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \sum_{i = 1}^N \log(p(X_i \ | \ \theta)) \big]\\
  
  &amp;amp;= \sum_{i = 1}^N \text{E}_{X_i \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big] \\
  
  &amp;amp;= \sum_{i = 1}^N \text{E}_{X_i \ | \ Y_i \ = \ y_i, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big]
  
  = \sum_{i=1}^N Q_i(\theta|\theta_m)

  \end{aligned}
\end{equation}\]&lt;/span&gt;
Where we used &lt;span class=&#34;math inline&#34;&gt;\(p(x_i|\vec{y},\theta) = p(x_i|y_i,\theta)\)&lt;/span&gt; in the 2nd to last equality. &lt;strong&gt;QED&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-a-posteriori-em-and-regularizing-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximum A Posteriori EM and regularizing priors&lt;/h2&gt;
&lt;p&gt;The EM algorithm is easily extendable to regularized MLE. This is usually referred to as Maximum A Posteriori (MAP) in the Bayesian setting, which we adopt. Here the penalty term is interpreted as the log of the prior density on &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, and the function to be maximized is the posterior density of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; given the data. By Bayes’ Theorem
&lt;span class=&#34;math display&#34;&gt;\[
p(\theta|\vec{y}) \propto p(\vec{y}|\theta)p(\theta)
\]&lt;/span&gt;
where the proportionality constant is independent of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Thus the MAP estimator is
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta}_{MAP} := \text{argmax}_{\theta} \ \log(p(\theta|\vec{y})) \ = \text{argmax}_{\theta} \ \log(p(\vec{y}|\theta)) + \log(p(\theta))
\]&lt;/span&gt;
The way to extend EM to this situation is clear (at least formally): Simply replace the maximization step (step 4 above) in EM with maximizing &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m) + \log(p(\theta))\)&lt;/span&gt; instead of simply &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m)\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta_{m+1} := \text{argmax}_{\theta} \ \ \ Q(\theta|\theta_m) + \log(p(\theta))
\]&lt;/span&gt;
where as before &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta | \theta_m) \ := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] = \int_{\mathcal{X}} \log(p(\vec{x} \ | \ \theta)) \ p(\vec{x}|\vec{y},\theta_m) \ d\vec{x} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;monotonicity-of-the-em-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monotonicity of the EM algorithm&lt;/h2&gt;
&lt;p&gt;At this point the reader has all the theory needed to begin applying EM where they believe it’s a good fit. Before we end the post though let’s mention at least one result that shows that EM is indeed a generalization of MLE to the case of hidden data: under relatively weak assumptions of the algorithm causes the log-likelihood &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{y}|\theta_m))\)&lt;/span&gt; to be an &lt;a href=&#34;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Proof_of_correctness&#34;&gt;increasing sequence in &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Step 2 allows for a whole family of such algorithms, one for each possible approximator to &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt;. Step 4 can also be generalized. Since the value of &lt;span class=&#34;math inline&#34;&gt;\(\theta_{m+1}\)&lt;/span&gt; maximizes &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta | \theta_m)\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta_{m+1} | \theta_m) \ge Q(\theta_m | \theta_m)\)&lt;/span&gt;. Instead of seeking to maximize &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta | \theta_m)\)&lt;/span&gt; we may simply seek a value of &lt;span class=&#34;math inline&#34;&gt;\(\theta_{m+1}\)&lt;/span&gt; that improves on &lt;span class=&#34;math inline&#34;&gt;\(\theta_m\)&lt;/span&gt; in the sense of this inequality. For step 5, the stopping criteria are up to the implementer.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      <content:encoded>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This is the first in a series of posts on Expectation Maximization (EM) type algorithms. Our goal will be to motivate some of the theory behind these algorithms. In later posts we will implement examples in C++, often with the help of the <a href="http://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a> linear algebra library.</p>
</div>
<div id="maximum-likelihood" class="section level2">
<h2>Maximum likelihood</h2>
<p>A large subset of statistics is concerned with determining properties of a distribution by using data that is assumed to be generated by that distribution. A common example is <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximum Likelihood Estimation</a> (MLE). Here one assumes that a vector of observed data <span class="math inline">\(\vec{x}\in\mathbb{R}^N\)</span> is the realization of a random vector <span class="math inline">\(\vec{X}\)</span> with a probability density <span class="math inline">\(p(\vec{X} \ | \ \theta)\)</span> that depends on a vector of parameters <span class="math inline">\(\theta\)</span>. MLE amounts to estimating <span class="math inline">\(\theta\)</span> with the value that makes this probability density has high as possible for the observed data:</p>
<p><span class="math display">\[
\hat{\theta} := \text{argmax}_{\theta} \ \ p(\vec{x} \ | \ \theta)
\]</span>
As a function of <span class="math inline">\(\theta\)</span>, the density <span class="math inline">\(\mathcal{L}(\theta; \vec{x}) := p(\vec{x} \ | \ \theta)\)</span> is called the likelihood. Because probability densities are positive for the realized values <span class="math inline">\(\vec{x}\)</span> of <span class="math inline">\(\vec{X}\)</span>, the above problem is equivalent to maximizing the logarithm of the likelihood:
<span class="math display">\[
\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{x} \ | \ \theta))
\]</span>
(The main practical reason behind this log transformation is that it often makes the problem easier numerically. The theoretical advantage is that it ties MLE to the theory of the <a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher Information</a>).</p>
</div>
<div id="dependence-structures-and-problems-with-hidden-variables" class="section level2">
<h2>Dependence structures and problems with hidden variables</h2>
<p>The situation in the last section can be summarized by the simple dependence structure (or <em>Markov</em> diagram) <span class="math inline">\(\theta \to \vec{X}\)</span>. That is, given the value of <span class="math inline">\(\theta\)</span> we can determined the distribution of <span class="math inline">\(\vec{X}\)</span>, namely <span class="math inline">\(p(\vec{X} \ | \ \theta)\)</span>.</p>
<p>However, in many applications we may only have partial observations of the data we want, with some of the relevant information remaining unobserved/hidden. For example, suppose 100 identical and independent dice are thrown in an experiment. The dice are not necessarily uniformly weighted, with probabilities of landing 1,2,…,6 given by <span class="math inline">\(\theta = [p_1, p_2,...,p_6]\)</span>. Suppose the dice land with values represented by <span class="math inline">\(\vec{X} = [X_1, X_2, ... X_{100}]\)</span> with <span class="math inline">\(X_i\)</span> being the number the <span class="math inline">\(i^{th}\)</span> die lands on. Suppose also that in the experiment we are only able to observe whether each die landed on an even or odd number. That is, we observe a vector <span class="math inline">\(\vec{Y}\)</span> given by</p>
<p><span class="math display">\[
Y_i = X_i \mod 2
\]</span></p>
<p>for <span class="math inline">\(i \in \{1, 2, ... 100\}\)</span>. In this case the dependence structure is a little more complex: <span class="math inline">\(\theta \to \vec{X} \to \vec{Y}\)</span>. That is, once we know the value of <span class="math inline">\(\vec{X}\)</span> we can fully specify the distribution of <span class="math inline">\(\vec{Y}\)</span> without knowing the value of <span class="math inline">\(\theta\)</span>. We would have the Markov property for densities:</p>
<p><span class="math display">\[
p(\vec{y} \ | \ \vec{x}, \theta) = p(\vec{y} \ | \ \vec{x})
\]</span></p>
<p>In general, we have a dependence structure given by <span class="math inline">\(\theta \to \vec{X} \to \vec{Y}\)</span>, we observe only <span class="math inline">\(\vec{Y}=\vec{y}\)</span> and we want to estimate the parameters <span class="math inline">\(\theta\)</span>. The MLE estimator would be:
<span class="math display">\[
\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{y} \ | \ \theta))
\]</span>
All of the theory of MLE applies in this case. In the example above this would be relatively easy. However, there are times this maximization problem is very difficult. Often the density <span class="math inline">\(p(\vec{y} \ | \ \theta)\)</span> may be much more complicated than the density <span class="math inline">\(p(\vec{x} \ | \ \theta)\)</span> of the hidden data that we wish we had.</p>
<p>In such as situation, if we knew <span class="math inline">\(\vec{x}\)</span> then we can replace the above problem with <span class="math inline">\(\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{x} \ | \ \theta))\)</span>. In fact, we wouldn’t even need to know exactly what the value of <span class="math inline">\(\vec{x}\)</span> is but only what the value of <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> is for a given <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="a-general-recipe-for-em-algorithms" class="section level2">
<h2>A general recipe for EM algorithms</h2>
<p>The idea of EM is indeed to try and maximize <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> instead of <span class="math inline">\(\log(p(\vec{y} \ | \ \theta))\)</span>, but because we do not know <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> to instead use an approximation/estimate of it.</p>
<p>How to approximate such an expression? The quantity <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> is a random variable (depending on the unknown value <span class="math inline">\(\vec{x}\)</span> of <span class="math inline">\(\vec{X}\)</span>). To estimate it in a meaningful way we need to use the most informative distribution related to <span class="math inline">\(\vec{x}\)</span>. Because we know <span class="math inline">\(\vec{y}\)</span> the best such distribution is <span class="math inline">\(p(\vec{x}|\vec{y},\theta)\)</span>.</p>
<p>The problem is this distribution (or any other) will necessarily depend <span class="math inline">\(\theta\)</span>, which we do not know! At first this seems like a circular trap, because to estimate <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> using <span class="math inline">\(p(\vec{x}|\vec{y},\theta)\)</span> we need to know <span class="math inline">\(\theta\)</span>, but to estimate <span class="math inline">\(\theta\)</span> we need to know <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span>. However the trap hints at a solution: simply alternate between estimating the random variable <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> using a current guess of <span class="math inline">\(\theta\)</span> and then use this updated estimate of <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> to update our guess of <span class="math inline">\(\theta\)</span>. More formally we can summarize EM in 5 steps:</p>
<ul>
<li><p><strong>Step 1:</strong> Let <span class="math inline">\(m = 0\)</span>. Make an initial estimate <span class="math inline">\(\theta_m\)</span> for <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><strong>Step 2:</strong> Given the observed data <span class="math inline">\(\vec{y}\)</span> and pretending for the moment that our current guess <span class="math inline">\(\theta_m\)</span> is correct, construct the conditional probability distribution <span class="math inline">\(p(\vec{x}|\vec{y},\theta_m)\)</span> of the hidden data <span class="math inline">\(\vec{x}\)</span> given all known information.</p></li>
<li><p><strong>Step 3:</strong> Using the distribution <span class="math inline">\(p(\vec{x}|\vec{y},\theta_m)\)</span> construct an estimator/approximation of the desired log-likelihood <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> for arbitrary <span class="math inline">\(\theta\)</span>. We denote this approximation by <span class="math inline">\(Q(\theta|\theta_m)\)</span>.</p></li>
<li><p><strong>Step 4:</strong> Set <span class="math inline">\(\theta_{m+1}\)</span> equal to a value of <span class="math inline">\(\theta\)</span> that maximizes the current approximation <span class="math inline">\(Q(\theta|\theta_m)\)</span> of <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span>.</p></li>
<li><p><strong>Step 5:</strong> Return to step 2 and repeat until some stopping criteria is met.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p></li>
</ul>
<p>Practically speaking, this algorithm would be applied when each of these steps is significantly easier than the original MLE problem of <span class="math inline">\(\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{y} \ | \ \theta))\)</span>. As a general example, this is often the case when the model is linear with respect to <span class="math inline">\(\vec{X}\)</span>, but the information loss of going from <span class="math inline">\(\vec{X}\)</span> to <span class="math inline">\(\vec{Y}\)</span> is nonlinear and non-invertible (we’ll give examples in later posts).</p>
</div>
<div id="constructing-an-estimator-for-logpvecx-theta" class="section level2">
<h2>Constructing an estimator for <span class="math inline">\(\log(p(\vec{X} \ | \ \theta))\)</span></h2>
<p>How do we fill in the blank left by step 3 above? That is, how do we use the probability density <span class="math inline">\(p(\vec{x}|\vec{y},\theta_m)\)</span> to estimate the value of the random variable <span class="math inline">\(\log(p(\vec{X} \ | \ \theta))\)</span>? Two possibilities come to mind.</p>
<div id="point-estimate-type-em" class="section level3">
<h3>Point-estimate type EM</h3>
<p>One possibility is to let
<span class="math display">\[
\vec{x}_m = \vec{x}_m(\vec{y}, \theta_m) := \text{argmax}_{\vec{x}} \ p(\vec{x}|\vec{y},\theta_m)
\]</span>
and then define
<span class="math display">\[
Q(\theta | \theta_m) := \log(p(\vec{x}_m \ | \ \theta))
\]</span>
This is called point-estimate EM. Here we use “the most likely” value of <span class="math inline">\(\vec{X}\)</span> as determined by the density <span class="math inline">\(p(\vec{x}|\vec{y},\theta_m)\)</span> and then impute this value into our log-likelihood that we want to maximize. Another possibility would be to let</p>
<p><span class="math display">\[
\vec{x}_m = \vec{x}_m(\vec{y}, \theta_m) := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \vec{X}\big]
\]</span>
and let <span class="math inline">\(Q\)</span> be as before.</p>
<p>The idea of these type of EM algorithms is to first estimate the missing data <span class="math inline">\(\vec{x}\)</span> and then impute the result into <span class="math inline">\(log(p(\vec{x}\ |\ \theta))\)</span>.</p>
</div>
<div id="expectation-em-i.e.-standard-em" class="section level3">
<h3>Expectation EM (i.e. standard EM)</h3>
<p>There is a theoretically more elegant way. As mentioned earlier, we do not in fact need an estimate of the missing data <span class="math inline">\(\vec{x}\)</span>. One of the best ways to estimate the value of a random variable with respect to a conditional distribution is to simply compute the conditional expectation of that variable with respect to that conditional distribution:</p>
<p><span class="math display">\[
Q(\theta | \theta_m) \ := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] = \int_{\mathcal{X}} \log(p(\vec{x} \ | \ \theta)) \ p(\vec{x}|\vec{y},\theta_m) \ d\vec{x} 
\]</span>
Here we’re computing the mean of <span class="math inline">\(\log(p(\vec{X} \ | \ \theta))\)</span> with respect to the density <span class="math inline">\(p(\vec{x}|\vec{y},\theta_m)\)</span>. As is common when using expectations, this second method has some advantages we’ll see later. When we refer to EM we will always mean this case, unless otherwise specified.</p>
</div>
</div>
<div id="qthetatheta_m-for-i.i.d.-samples" class="section level2">
<h2><span class="math inline">\(Q(\theta|\theta_m)\)</span> for I.I.D. samples</h2>
<p>So far everything has been rather general, applying to any random vectors <span class="math inline">\(\vec{X}\)</span> and <span class="math inline">\(\vec{Y}\)</span>. Many problems assume that data are generated independently and identically distributed (I.I.D.) so it’s helpful to have a formulation for this particular case.</p>
<p><strong>Proposition:</strong> Suppose that the components of <span class="math inline">\(\vec{X}\)</span> are IID (given <span class="math inline">\(\theta\)</span>), that is:
<span class="math display">\[
p(\vec{x}|\theta) = \prod_{i = 1}^N p(x_i|\theta) \  \qquad \forall x, \theta
\]</span>
Suppose also that the dependence structure <span class="math inline">\(\theta \to \vec{X} \to \vec{Y}\)</span> splits into subgraphs as <span class="math inline">\(\theta \to X_i \to Y_i\)</span> for all <span class="math inline">\(i = 1, 2, ..., N\)</span>. This just means that given <span class="math inline">\(X_i\)</span>, the distribution <span class="math inline">\(Y_i\)</span> is independent of all other variables:
<span class="math display">\[
p(y_i|\vec{x}, \theta, y_1, y_2, ..., y_{i-1}, y_{i+1}, ..., y_N) = p(y_i|x_i)
\]</span>
Then <span class="math inline">\(Q(\theta|\theta_m):= \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m} \big[ \log(p(\vec{X} \ | \ \theta)) \big]\)</span> can be written as:
<span class="math display">\[
Q(\theta|\theta_m) = \sum_{i=1}^N Q_i(\theta|\theta_m)
\]</span>
where
<span class="math display">\[
Q_i(\theta|\theta_m) := \ \text{E}_{X_i \ | \ Y_i \ = \ y_i, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big].
\]</span>
<strong>Proof:</strong> The proof begins by showing that the joint elements <span class="math inline">\((X_i,Y_i)\)</span> are independent across <span class="math inline">\(i\)</span>, that is:
<span class="math display">\[
p(\vec{x},\vec{y}|\theta) = \prod_{i = 1}^N p(x_i, y_i|\theta). 
\]</span>
To prove this we start by applying the multiplication theorem for probability densities:
<span class="math display">\[\begin{equation}
  \begin{aligned}

p(\vec{x},\vec{y}|\theta) &amp;= p(y_1|y_2,y_3, ..., y_N, \vec{x},\theta)...p(y_N|\vec{x},\theta)p(\vec{x}|\theta) \qquad \text{by the multiplication theorem}\\

    &amp;= p(y_1|x_1,\theta)...p(y_N|x_N,\theta)p(\vec{x}|\theta) \qquad \text{by conditional independence but keeping theta} \\
    &amp;= p(\vec{x}|\theta)\prod_{i=1}^Np(y_i|x_i,\theta) \\
    &amp;=\prod_{i=1}^Np(y_i|x_i,\theta)p(x_i|\theta) \qquad \text{by independence of the x&#39;s} \\
    &amp;= \prod_{i=1}^Np(y_i,x_i|\theta).
  \end{aligned}
\end{equation}\]</span></p>
<p>Next we have that for each <span class="math inline">\(i\)</span></p>
<p><span class="math display">\[\begin{equation}
  \begin{aligned}
  
  p(x_i|\vec{y},\theta) &amp;= \frac{p(x_i,\vec{y}|\theta)}{p(\vec{y}|\theta)} \qquad \text{by Bayes} \\
  
  &amp;= \frac{\int p(\vec{x},\vec{y}|\theta)dx_1...dx_{i-1}dx_{i+1}...dx_n}{\int p(\vec{x},\vec{y}|\theta)d\vec{x}} \\
  
  &amp;= \frac{\int \prod_{j=1}^Np(y_j,x_j|\theta)dx_1...dx_{i-1}dx_{i+1}...dx_n}{\int \prod_{j=1}^Np(y_j,x_j|\theta)d\vec{x}} \\
  
  &amp;= \frac{p(x_i,y_i|\theta) \prod_{j=1, j \ne i}^N \int p(y_j,x_j|\theta)dx_j}{\prod_{j=1}^N\int p(y_j,x_j|\theta) dx_j} \\
  
  &amp;= \frac{p(x_i,y_i|\theta) \prod_{j=1, j \ne i}^N p(y_j|\theta)}{\prod_{j=1}^N p(y_j|\theta)}\\
  
  &amp;= \frac{p(x_i,y_i|\theta)}{p(y_i|\theta)}\\
  
  &amp;= p(x_i|y_i,\theta)

  \end{aligned}
\end{equation}\]</span></p>
<p>Hence <span class="math inline">\(p(x_i|\vec{y},\theta) = p(x_i|y_i,\theta)\)</span>. Therefore we have</p>
<p><span class="math display">\[\begin{equation}
  \begin{aligned}
  
  Q(\theta, \theta_m) &amp;= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] \\
  
  &amp;= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(\prod_{i = 1}^N p(X_i \ | \ \theta)) \big] \\
  
  &amp;= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \sum_{i = 1}^N \log(p(X_i \ | \ \theta)) \big]\\
  
  &amp;= \sum_{i = 1}^N \text{E}_{X_i \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big] \\
  
  &amp;= \sum_{i = 1}^N \text{E}_{X_i \ | \ Y_i \ = \ y_i, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big]
  
  = \sum_{i=1}^N Q_i(\theta|\theta_m)

  \end{aligned}
\end{equation}\]</span>
Where we used <span class="math inline">\(p(x_i|\vec{y},\theta) = p(x_i|y_i,\theta)\)</span> in the 2nd to last equality. <strong>QED</strong></p>
</div>
<div id="maximum-a-posteriori-em-and-regularizing-priors" class="section level2">
<h2>Maximum A Posteriori EM and regularizing priors</h2>
<p>The EM algorithm is easily extendable to regularized MLE. This is usually referred to as Maximum A Posteriori (MAP) in the Bayesian setting, which we adopt. Here the penalty term is interpreted as the log of the prior density on <span class="math inline">\(\theta\)</span>, and the function to be maximized is the posterior density of <span class="math inline">\(\theta\)</span> given the data. By Bayes’ Theorem
<span class="math display">\[
p(\theta|\vec{y}) \propto p(\vec{y}|\theta)p(\theta)
\]</span>
where the proportionality constant is independent of <span class="math inline">\(\theta\)</span>. Thus the MAP estimator is
<span class="math display">\[
\hat{\theta}_{MAP} := \text{argmax}_{\theta} \ \log(p(\theta|\vec{y})) \ = \text{argmax}_{\theta} \ \log(p(\vec{y}|\theta)) + \log(p(\theta))
\]</span>
The way to extend EM to this situation is clear (at least formally): Simply replace the maximization step (step 4 above) in EM with maximizing <span class="math inline">\(Q(\theta|\theta_m) + \log(p(\theta))\)</span> instead of simply <span class="math inline">\(Q(\theta|\theta_m)\)</span>:</p>
<p><span class="math display">\[
\theta_{m+1} := \text{argmax}_{\theta} \ \ \ Q(\theta|\theta_m) + \log(p(\theta))
\]</span>
where as before <span class="math inline">\(Q\)</span> is given by</p>
<p><span class="math display">\[
Q(\theta | \theta_m) \ := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] = \int_{\mathcal{X}} \log(p(\vec{x} \ | \ \theta)) \ p(\vec{x}|\vec{y},\theta_m) \ d\vec{x} 
\]</span></p>
</div>
<div id="monotonicity-of-the-em-algorithm" class="section level2">
<h2>Monotonicity of the EM algorithm</h2>
<p>At this point the reader has all the theory needed to begin applying EM where they believe it’s a good fit. Before we end the post though let’s mention at least one result that shows that EM is indeed a generalization of MLE to the case of hidden data: under relatively weak assumptions of the algorithm causes the log-likelihood <span class="math inline">\(\log(p(\vec{y}|\theta_m))\)</span> to be an <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Proof_of_correctness">increasing sequence in <span class="math inline">\(m\)</span></a>.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Step 2 allows for a whole family of such algorithms, one for each possible approximator to <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span>. Step 4 can also be generalized. Since the value of <span class="math inline">\(\theta_{m+1}\)</span> maximizes <span class="math inline">\(Q(\theta | \theta_m)\)</span> then <span class="math inline">\(Q(\theta_{m+1} | \theta_m) \ge Q(\theta_m | \theta_m)\)</span>. Instead of seeking to maximize <span class="math inline">\(Q(\theta | \theta_m)\)</span> we may simply seek a value of <span class="math inline">\(\theta_{m+1}\)</span> that improves on <span class="math inline">\(\theta_m\)</span> in the sense of this inequality. For step 5, the stopping criteria are up to the implementer.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
</content:encoded>	
    </item>
    
  </channel>
</rss>
