<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Fundamenta Nova</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 01 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Expectation Maximization, Part 1: Theory</title>
      <link>/post/003_em1/main/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/003_em1/main/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is the first in a series of posts on Expectation Maximization (EM) type algorithms. Our goal will be to motivate some of the theory behind these algorithms. In later posts we will implement examples in C++, often with the help of the &lt;a href=&#34;http://eigen.tuxfamily.org/index.php?title=Main_Page&#34;&gt;Eigen&lt;/a&gt; linear algebra library.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-likelihood&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximum likelihood&lt;/h2&gt;
&lt;p&gt;A large subset of statistics is concerned with determining properties of a distribution by using data that is assumed to be generated by that distribution. A common example is &lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34;&gt;Maximum Likelihood Estimation&lt;/a&gt; (MLE). Here one assumes that a vector of observed data &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\in\mathbb{R}^N\)&lt;/span&gt; is the realization of a random vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; with a probability density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{X} \ | \ \theta)\)&lt;/span&gt; that depends on a vector of parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. MLE amounts to estimating &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; with the value that makes this probability density has high as possible for the observed data:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} := \text{argmax}_{\theta} \ \ p(\vec{x} \ | \ \theta)
\]&lt;/span&gt;
As a function of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the density &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}(\theta; \vec{x}) := p(\vec{x} \ | \ \theta)\)&lt;/span&gt; is called the likelihood. Because probability densities are positive for the realized values &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, the above problem is equivalent to maximizing the logarithm of the likelihood:
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{x} \ | \ \theta))
\]&lt;/span&gt;
(The main practical reason behind this log transformation is that it often makes the problem easier numerically. The theoretical advantage is that it ties MLE to the theory of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fisher_information&#34;&gt;Fisher Information&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dependence-structures-and-problems-with-hidden-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dependence structures and problems with hidden variables&lt;/h2&gt;
&lt;p&gt;The situation in the last section can be summarized by the simple dependence structure (or &lt;em&gt;Markov&lt;/em&gt; diagram) &lt;span class=&#34;math inline&#34;&gt;\(\theta \to \vec{X}\)&lt;/span&gt;. That is, given the value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; we can determined the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, namely &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{X} \ | \ \theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, in many applications we may only have partial observations of the data we want, with some of the relevant information remaining unobserved/hidden. For example, suppose 100 identical and independent dice are thrown in an experiment. The dice are not necessarily uniformly weighted, with probabilities of landing 1,2,…,6 given by &lt;span class=&#34;math inline&#34;&gt;\(\theta = [p_1, p_2,...,p_6]\)&lt;/span&gt;. Suppose the dice land with values represented by &lt;span class=&#34;math inline&#34;&gt;\(\vec{X} = [X_1, X_2, ... X_{100}]\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; being the number the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; die lands on. Suppose also that in the experiment we are only able to observe whether each die landed on an even or odd number. That is, we observe a vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}\)&lt;/span&gt; given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = X_i \mod 2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i \in \{1, 2, ... 100\}\)&lt;/span&gt;. In this case the dependence structure is a little more complex: &lt;span class=&#34;math inline&#34;&gt;\(\theta \to \vec{X} \to \vec{Y}\)&lt;/span&gt;. That is, once we know the value of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; we can fully specify the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}\)&lt;/span&gt; without knowing the value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. We would have the Markov property for densities:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(\vec{y} \ | \ \vec{x}, \theta) = p(\vec{y} \ | \ \vec{x})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In general, we have a dependence structure given by &lt;span class=&#34;math inline&#34;&gt;\(\theta \to \vec{X} \to \vec{Y}\)&lt;/span&gt;, we observe only &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}=\vec{y}\)&lt;/span&gt; and we want to estimate the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. The MLE estimator would be:
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{y} \ | \ \theta))
\]&lt;/span&gt;
All of the theory of MLE applies in this case. In the example above this would be relatively easy. However, there are times this maximization problem is very difficult. Often the density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{y} \ | \ \theta)\)&lt;/span&gt; may be much more complicated than the density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x} \ | \ \theta)\)&lt;/span&gt; of the hidden data that we wish we had.&lt;/p&gt;
&lt;p&gt;If we knew &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; then we can replace the above problem with &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt;. In fact, we wouldn’t even need to know exactly what the value of &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; is but only what the value of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; is for a given &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-general-recipe-for-em-algorithms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A general recipe for EM algorithms&lt;/h2&gt;
&lt;p&gt;The idea of EM is indeed to try and maximize &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{y} \ | \ \theta))\)&lt;/span&gt;, but because we do not know &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; to instead use an approximation/estimate of it. The quantity &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; is a random variable (depending on the unknown value &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;). To estimate it in a meaningful way we need to use the most informative distribution related to &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt;. The problem is this distribution will necessarily depend &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, which we do not know! At first this seems like a circular trap, but it hints at a solution: simply alternate between estimating the random variable &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; using a current guess of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and then use this updated estimate of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; to update our guess of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. More formally we can summarize EM in 5 steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(m = 0\)&lt;/span&gt;. Make an initial estimate &lt;span class=&#34;math inline&#34;&gt;\(\theta_m\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Given the observed data &lt;span class=&#34;math inline&#34;&gt;\(\vec{y}\)&lt;/span&gt; and pretending for the moment that our current guess &lt;span class=&#34;math inline&#34;&gt;\(\theta_m\)&lt;/span&gt; is correct, construct the conditional probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt; of the hidden data &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; given all known information.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Using the distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt; construct an estimator/approximation of the desired log-likelihood &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. We denote this approximation by &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Set &lt;span class=&#34;math inline&#34;&gt;\(\theta_{m+1}\)&lt;/span&gt; equal to a value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that maximizes the current approximation &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m)\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Return to step 2 and repeat until some stopping criteria is met.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Practically speaking, this algorithm would be applied when each of these steps is significantly easier than the original MLE problem of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{y} \ | \ \theta))\)&lt;/span&gt;. As a general example, this is often the case when the model is linear with respect to &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, but the information loss of going from &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}\)&lt;/span&gt; is nonlinear and non-invertible (we’ll give examples in later posts).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;constructing-an-estimator-for-logpvecx-theta&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Constructing an estimator for &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{X} \ | \ \theta))\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;How do we fill in the blank left by step 3 above? That is, how do we use the probability density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt; to estimate the value of the random variable &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{X} \ | \ \theta))\)&lt;/span&gt;? Two possibilities come to mind.&lt;/p&gt;
&lt;div id=&#34;point-estimate-type-em&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Point-estimate type EM&lt;/h3&gt;
&lt;p&gt;One possibility is to let
&lt;span class=&#34;math display&#34;&gt;\[
\vec{x}_m = \vec{x}_m(\vec{y}, \theta_m) := \text{argmax}_{\vec{x}} \ p(\vec{x}|\vec{y},\theta_m)
\]&lt;/span&gt;
and then define
&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta | \theta_m) := \log(p(\vec{x}_m \ | \ \theta))
\]&lt;/span&gt;
This is called point-estimate EM. Here we use “the most likely” value of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; as determined by the density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt; and then impute this value into our log-likelihood that we want to maximize. Another possibility would be to let&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{x}_m = \vec{x}_m(\vec{y}, \theta_m) := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \vec{X}\big]
\]&lt;/span&gt;
and let &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; be as before.&lt;/p&gt;
&lt;p&gt;The idea of these type of EM algorithms is to first estimate the missing data &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; and then impute the result into &lt;span class=&#34;math inline&#34;&gt;\(log(p(\vec{x}\ |\ \theta))\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;expectation-em-i.e.-standard-em&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Expectation EM (i.e. standard EM)&lt;/h3&gt;
&lt;p&gt;There is a theoretically more elegant way. As mentioned earlier, we do not in fact need an estimate of the missing data &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt;. One of the best ways to estimate the value of a random variable with respect to a conditional distribution is to simply compute the conditional expectation of that variable with respect to that conditional distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta | \theta_m) \ := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] = \int_{\mathcal{X}} \log(p(\vec{x} \ | \ \theta)) \ p(\vec{x}|\vec{y},\theta_m) \ d\vec{x} 
\]&lt;/span&gt;
Here we’re computing the mean of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{X} \ | \ \theta))\)&lt;/span&gt; with respect to the density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt;. As is common when using expectations, this second method has some advantages we’ll see later. When we refer to EM we will always mean this case, unless otherwise specified.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;qthetatheta_m-for-i.i.d.-samples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m)\)&lt;/span&gt; for I.I.D. samples&lt;/h2&gt;
&lt;p&gt;So far everything has been rather general, applying to any random vectors &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}\)&lt;/span&gt;. Many problems assume that data are generated independently and identically distributed (I.I.D.) so it’s helpful to have a formulation for this particular case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proposition:&lt;/strong&gt; Suppose that the components of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; are IID (given &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;), that is:
&lt;span class=&#34;math display&#34;&gt;\[
p(\vec{x}|\theta) = \prod_{i = 1}^N p(x_i|\theta) \  \qquad \forall x, \theta
\]&lt;/span&gt;
Suppose also that the dependence structure &lt;span class=&#34;math inline&#34;&gt;\(\theta \to \vec{X} \to \vec{Y}\)&lt;/span&gt; splits into subgraphs as &lt;span class=&#34;math inline&#34;&gt;\(\theta \to X_i \to Y_i\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i = 1, 2, ..., N\)&lt;/span&gt;. This just means that given &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;, the distribution &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; is independent of all other variables:
&lt;span class=&#34;math display&#34;&gt;\[
p(y_i|\vec{x}, \theta, y_1, y_2, ..., y_{i-1}, y_{i+1}, ..., y_N) = p(y_i|x_i)
\]&lt;/span&gt;
Then &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m):= \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m} \big[ \log(p(\vec{X} \ | \ \theta)) \big]\)&lt;/span&gt; can be written as:
&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta|\theta_m) = \sum_{i=1}^N Q_i(\theta|\theta_m)
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
Q_i(\theta|\theta_m) := \ \text{E}_{X_i \ | \ Y_i \ = \ y_i, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big].
\]&lt;/span&gt;
&lt;strong&gt;Proof:&lt;/strong&gt; The proof begins by showing that the joint elements &lt;span class=&#34;math inline&#34;&gt;\((X_i,Y_i)\)&lt;/span&gt; are independent across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, that is:
&lt;span class=&#34;math display&#34;&gt;\[
p(\vec{x},\vec{y}|\theta) = \prod_{i = 1}^N p(x_i, y_i|\theta). 
\]&lt;/span&gt;
To prove this we start by applying the multiplication theorem for probability densities:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \begin{aligned}

p(\vec{x},\vec{y}|\theta) &amp;amp;= p(y_1|y_2,y_3, ..., y_N, \vec{x},\theta)...p(y_N|\vec{x},\theta)p(\vec{x}|\theta) \qquad \text{by the multiplication theorem}\\

    &amp;amp;= p(y_1|\vec{x},\theta)...p(y_N|\vec{x},\theta)p(\vec{x}|\theta) \qquad \text{by conditional independence but keeping theta} \\
    &amp;amp;= p(\vec{x}|\theta)\prod_{i=1}^Np(y_i|x_i,\theta) \\
    &amp;amp;=\prod_{i=1}^Np(y_i|x_i,\theta)p(x_i|\theta) \qquad \text{by independence of the x&amp;#39;s} \\
    &amp;amp;= \prod_{i=1}^Np(y_i,x_i|\theta).
  \end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Next we have that for each &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \begin{aligned}
  
  p(x_i|\vec{y},\theta) &amp;amp;= \frac{p(x_i,y_i|\theta)}{p(y_i|\theta)} \qquad \text{by Bayes} \\
  
  &amp;amp;= \frac{\int p(\vec{x},\vec{y}|\theta)dx_1...dx_{i-1}dx_{i+1}...dx_n}{\int p(\vec{x},\vec{y}|\theta)d\vec{x}} \\
  
  &amp;amp;= \frac{\int \prod_{j=1}^Np(y_j,x_j|\theta)dx_1...dx_{i-1}dx_{i+1}...dx_n}{\int \prod_{j=1}^Np(y_j,x_j|\theta)d\vec{x}} \\
  
  &amp;amp;= \frac{p(x_i,y_i|\theta) \prod_{j=1, j \ne i}^N \int p(y_j,x_j|\theta)dx_j}{\prod_{j=1}^N\int p(y_j,x_j|\theta) dx_j} \\
  
  &amp;amp;= \frac{p(x_i,y_i|\theta) \prod_{j=1, j \ne i}^N p(y_j|\theta)}{\prod_{j=1}^N p(y_j|\theta)}\\
  
  &amp;amp;= \frac{p(x_i,y_i|\theta)}{p(y_i|\theta)}\\
  
  &amp;amp;= p(x_i|y_i,\theta)

  \end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence &lt;span class=&#34;math inline&#34;&gt;\(p(x_i|\vec{y},\theta) = p(x_i|y_i,\theta)\)&lt;/span&gt;. Therefore we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \begin{aligned}
  
  Q(\theta, \theta_m) &amp;amp;= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] \\
  
  &amp;amp;= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(\prod_{i = 1}^N p(X_i \ | \ \theta)) \big] \\
  
  &amp;amp;= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \sum_{i = 1}^N \log(p(X_i \ | \ \theta)) \big]\\
  
  &amp;amp;= \sum_{i = 1}^N \text{E}_{X_i \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big] \\
  
  &amp;amp;= \sum_{i = 1}^N \text{E}_{X_i \ | \ Y_i \ = \ y_i, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big]
  
  = \sum_{i=1}^N Q_i(\theta|\theta_m)

  \end{aligned}
\end{equation}\]&lt;/span&gt;
Where we used &lt;span class=&#34;math inline&#34;&gt;\(p(x_i|\vec{y},\theta) = p(x_i|y_i,\theta)\)&lt;/span&gt; in the 2nd to last equality. &lt;strong&gt;QED&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-a-posteriori-em-and-regularizing-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximum A Posteriori EM and regularizing priors&lt;/h2&gt;
&lt;p&gt;The EM algorithm is easily extendable to regularized MLE. This is usually referred to as Maximum A Posteriori (MAP) in the Bayesian setting, which we adopt. Here the penalty term is interpreted as the log of the prior density on &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, and the function to be maximized is the posterior density of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; given the data. By Bayes’ Theorem
&lt;span class=&#34;math display&#34;&gt;\[
p(\theta|\vec{y}) \propto p(\vec{y}|\theta)p(\theta)
\]&lt;/span&gt;
where the proportionality constant is independent of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Thus the MAP estimator is
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta}_{MAP} := \text{argmax}_{\theta} \ \log(p(\theta|\vec{y})) \ = \text{argmax}_{\theta} \ \log(p(\vec{y}|\theta)) + \log(p(\theta))
\]&lt;/span&gt;
The way to extend EM to this situation is clear (at least formally): Simply replace the maximization step (step 4 above) in EM with maximizing &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m) + \log(p(\theta))\)&lt;/span&gt; instead of simply &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m)\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta_{m+1} := \text{argmax}_{\theta} \ \ \ Q(\theta|\theta_m) + \log(p(\theta))
\]&lt;/span&gt;
where as before &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta | \theta_m) \ := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] = \int_{\mathcal{X}} \log(p(\vec{x} \ | \ \theta)) \ p(\vec{x}|\vec{y},\theta_m) \ d\vec{x} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;monotonicity-of-the-em-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monotonicity of the EM algorithm&lt;/h2&gt;
&lt;p&gt;At this point the reader has all the theory needed to begin applying EM where they believe it’s a good fit. Before we end the post though let’s mention at least one result that shows that EM is indeed a generalization of MLE to the case of hidden data: under relatively weak assumptions of the algorithm causes the log-likelihood &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{y}|\theta_m))\)&lt;/span&gt; to be an &lt;a href=&#34;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Proof_of_correctness&#34;&gt;increasing sequence in &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Step 2 allows for a whole family of such algorithms, one for each possible approximator to &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt;. Step 4 can also be generalized. Since the value of &lt;span class=&#34;math inline&#34;&gt;\(\theta_{m+1}\)&lt;/span&gt; maximizes &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta | \theta_m)\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta_{m+1} | \theta_m) \ge Q(\theta_m | \theta_m)\)&lt;/span&gt;. Instead of seeking to maximize &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta | \theta_m)\)&lt;/span&gt; we may simply seek a value of &lt;span class=&#34;math inline&#34;&gt;\(\theta_{m+1}\)&lt;/span&gt; that improves on &lt;span class=&#34;math inline&#34;&gt;\(\theta_m\)&lt;/span&gt; in the sense of this inequality. For step 5, the stopping criteria are up to the implementer.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Passing expressions and data from R to C&#43;&#43; at compile-time in Rmarkdown</title>
      <link>/post/002_compile_time_data_r_to_cpp/main/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/002_compile_time_data_r_to_cpp/main/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post we give a simple illustrative example of how data generated by R code can be used by compiled languages such as C++ at compile time, instead of run-time, inside Rmarkdown.&lt;/p&gt;
&lt;p&gt;This is an example of inter-language code generation. Metaprogramming/code generation is an extremely powerful technique but it’s also one that is very easy to overdo. This is just a fun example to learn from. Thorough testing is very important for any production code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-other-languages-in-rmarkdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using other languages in Rmarkdown&lt;/h2&gt;
&lt;p&gt;Out of the box Rmarkdown can work with the following languages assuming a proper back-end is available:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(knitr::knit_engines$get())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;awk&amp;quot;         &amp;quot;bash&amp;quot;        &amp;quot;coffee&amp;quot;      &amp;quot;gawk&amp;quot;        &amp;quot;groovy&amp;quot;     
##  [6] &amp;quot;haskell&amp;quot;     &amp;quot;lein&amp;quot;        &amp;quot;mysql&amp;quot;       &amp;quot;node&amp;quot;        &amp;quot;octave&amp;quot;     
## [11] &amp;quot;perl&amp;quot;        &amp;quot;psql&amp;quot;        &amp;quot;Rscript&amp;quot;     &amp;quot;ruby&amp;quot;        &amp;quot;sas&amp;quot;        
## [16] &amp;quot;scala&amp;quot;       &amp;quot;sed&amp;quot;         &amp;quot;sh&amp;quot;          &amp;quot;stata&amp;quot;       &amp;quot;zsh&amp;quot;        
## [21] &amp;quot;highlight&amp;quot;   &amp;quot;Rcpp&amp;quot;        &amp;quot;tikz&amp;quot;        &amp;quot;dot&amp;quot;         &amp;quot;c&amp;quot;          
## [26] &amp;quot;fortran&amp;quot;     &amp;quot;fortran95&amp;quot;   &amp;quot;asy&amp;quot;         &amp;quot;cat&amp;quot;         &amp;quot;asis&amp;quot;       
## [31] &amp;quot;stan&amp;quot;        &amp;quot;block&amp;quot;       &amp;quot;block2&amp;quot;      &amp;quot;js&amp;quot;          &amp;quot;css&amp;quot;        
## [36] &amp;quot;sql&amp;quot;         &amp;quot;go&amp;quot;          &amp;quot;python&amp;quot;      &amp;quot;julia&amp;quot;       &amp;quot;sass&amp;quot;       
## [41] &amp;quot;scss&amp;quot;        &amp;quot;theorem&amp;quot;     &amp;quot;lemma&amp;quot;       &amp;quot;corollary&amp;quot;   &amp;quot;proposition&amp;quot;
## [46] &amp;quot;conjecture&amp;quot;  &amp;quot;definition&amp;quot;  &amp;quot;example&amp;quot;     &amp;quot;exercise&amp;quot;    &amp;quot;proof&amp;quot;      
## [51] &amp;quot;remark&amp;quot;      &amp;quot;solution&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although we can use R’s native foreign function interface to call compiled code, for C++ a higher level alternative is to use &lt;a href=&#34;https://cran.r-project.org/web/packages/Rcpp/index.html&#34;&gt;Rcpp&lt;/a&gt;. In Rmarkdown we can &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/language-engines.html#rcpp&#34;&gt;compile C++ code chunks using Rcpp and export the compiled functions to be available for use in R&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As a common example, we can compile the following code&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector timesTwo(NumericVector x) 
{
    return x * 2;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and use the exported function in R&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;timesTwo(1:10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  2  4  6  8 10 12 14 16 18 20&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;registering-a-user-defined-language-engine-in-knitr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Registering a user-defined language engine in Knitr&lt;/h2&gt;
&lt;p&gt;We can create &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/custom-engine.html&#34;&gt;user-defined engines&lt;/a&gt; to control exactly how the code chunk is sourced, or even modify existing engines. To get an idea we can look at the default Rcpp engine used by knitr:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::knit_engines$get()$Rcpp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (options) 
## {
##     sourceCpp = getFromNamespace(&amp;quot;sourceCpp&amp;quot;, &amp;quot;Rcpp&amp;quot;)
##     code = one_string(options$code)
##     opts = options$engine.opts
##     cache = options$cache &amp;amp;&amp;amp; (&amp;quot;cacheDir&amp;quot; %in% names(formals(sourceCpp)))
##     if (cache) {
##         opts$cacheDir = paste(valid_path(options$cache.path, 
##             options$label), &amp;quot;sourceCpp&amp;quot;, sep = &amp;quot;_&amp;quot;)
##         opts$cleanupCacheDir = TRUE
##     }
##     if (!is.environment(opts$env)) 
##         opts$env = knit_global()
##     if (options$eval) {
##         message(&amp;quot;Building shared library for Rcpp code chunk...&amp;quot;)
##         do.call(sourceCpp, c(list(code = code), opts))
##     }
##     options$engine = &amp;quot;cpp&amp;quot;
##     engine_output(options, code, &amp;quot;&amp;quot;)
## }
## &amp;lt;environment: namespace:knitr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the default engine above as a template we can define a new knitr engine for compiling C++. One that can read and make use of more dynamic R data in C++ before compilation (or even dynamically create &lt;code&gt;Makevars&lt;/code&gt; files to control compilation flags). First let’s include the knitr package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(knitr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next let’s take a crack at defining a new engine to compile C++ code. In this example we will modify the current Rcpp engine to take in an &lt;code&gt;extra&lt;/code&gt; field (but otherwise behave the same).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knit_engines$set(RcppFoo = function(options) {
    
    extra = options$extra
    
    sourceCpp = getFromNamespace(&amp;quot;sourceCpp&amp;quot;, &amp;quot;Rcpp&amp;quot;)
    
    ## Code is read as a list of strings, one list element per line
    ## Here we append extra code that may be defined in R to the 
    ## code written in the chunk
    code = c(extra, options$code)
    code = paste(code, collapse = &amp;#39;\n&amp;#39;)
    opts = options$engine.opts
    
    if (!is.environment(opts$env)) 
        opts$env = knit_global()

    if (options$eval) {    
        message(&amp;quot;Building shared library for Rcpp code chunk...&amp;quot;)
        do.call(sourceCpp, c(list(code = code), opts))
    }
    options$engine = &amp;quot;cpp&amp;quot;
    engine_output(options, 
                  options$code, 
                  paste(&amp;quot;Added the lines:\n&amp;quot;, 
                      paste(extra, collapse = &amp;#39;\n&amp;#39;), 
                      sep = &amp;#39;\n&amp;#39;))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we test by creating some data in R and using that as a compile time constant in C++. Here we pass values of pi and e as static const doubles to C++ (a much cleaner API is possible of course).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;constants = list(
    paste(&amp;#39;static const double Pi =&amp;#39;, pi, &amp;#39;;&amp;#39;),
    paste(&amp;#39;static const double Euler =&amp;#39;, exp(1),&amp;#39;;&amp;#39;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This already highlights a danger as we have not considered exactly how R might convert these double precision floating point numbers to strings. Regardless, we proceed. To use the new engine we run the engine as &lt;code&gt;{RcppFoo test_chunk, extra = constants}&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector timesFoo(NumericVector x) 
{
    return x * Pi + Euler;
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Added the lines:
## 
## static const double Pi = 3.14159265358979 ;
## static const double Euler = 2.71828182845905 ;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = timesFoo(1:10)
print(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  5.859874  9.001467 12.143060 15.284652 18.426245 21.567838 24.709430
##  [8] 27.851023 30.992616 34.134208&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get &lt;strong&gt;almost&lt;/strong&gt; the same result as in R&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y = pi*(1:10)+exp(1)
print(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  5.859874  9.001467 12.143060 15.284652 18.426245 21.567838 24.709430
##  [8] 27.851023 30.992616 34.134208&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But metaprogramming can be dangerous when mixed with floating point arithmetic. In this case some loss of precision occurred with the doubles when converting to strings:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x - y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1.776357e-15  0.000000e+00 -3.552714e-15 -7.105427e-15 -1.065814e-14
##  [6] -1.421085e-14 -1.776357e-14 -1.776357e-14 -2.131628e-14 -2.842171e-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.double(as.character(pi))*(1:10) + as.double(as.character(exp(1))) - x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
##  [6] 3.552714e-15 3.552714e-15 0.000000e+00 0.000000e+00 0.000000e+00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Anyway this was just a small example. There are many many directions one can choose to take with metaprogramming. Even creating new preprocessing directives such as unrolling loops, defining constexprs, etc.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deriving Principal Component Analysis and implementing in C&#43;&#43; using Eigen</title>
      <link>/post/001_deriving_pca/main/</link>
      <pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/001_deriving_pca/main/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;Principal component analysis&lt;/a&gt; is one of the most commonly used techniques in statistical modeling and machine learning. In typical applications it serves as a (linear) dimensionality reduction, allowing one to project high dimensional data onto a lower dimensional subspace. This can help make a problem that was previously computationally intractable easier, or can help transform feature variables into something more useful. However, most presentations fail to give a sense of “why” and students are left without an understanding of exactly what PCA is and what assumptions it makes. This can lead to model risk issues and prevent users from being able to modify the technique when different assumptions hold. The purpose of this post is to rectify this with a derivation for those that want to know why, which should be everyone. For fun we implement what we learn at the end in a few lines of C++.&lt;/p&gt;
&lt;div id=&#34;a-note-on-difficulty&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A note on difficulty&lt;/h3&gt;
&lt;p&gt;To understand what follows you need to understand linear algebra and undergraduate probability. &lt;strong&gt;The proof that follows is as clear, honest, and self-contained as I think is possible, but most will not find it easy&lt;/strong&gt;. In my opinion if a truly easy and theoretically honest proof were possible you would have already seen it.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;deriving-pca&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deriving PCA&lt;/h2&gt;
&lt;p&gt;As scientists our data is often times multidimensional because it involves measurements of many features of the world. Equally often, our data may have some “randomness” in it that we can not capture (so that if the experiment that was run to obtain the data were rerun the results may not be exactly the same).&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\vec{X} = [X^1, X^2, ..., X^d]\)&lt;/span&gt; be a &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-dimensional random vector &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; that represents the measured values of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; feature variables.&lt;/p&gt;
&lt;p&gt;We want to capture the “shape” of the randomness of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;. For example, in what directions does &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; vary the most? In what directions does it vary the least? This is important because if, for example, &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; had a lot of randomness in its first coordinate &lt;span class=&#34;math inline&#34;&gt;\(X^1\)&lt;/span&gt;, but had very little randomness in the other coordinates, then independent measurements of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; would differ a lot in the first coordinate, but not much in the others. The other coordinates would all give roughly the same values and hence roughly the same information. The other coordinates would in a sense be redundant: replacing &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(X^1\)&lt;/span&gt; would not lose a lot of information but would have the benefit of having to deal with only 1 feature as opposed to &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; features (i.e. a dimensionality reduction).&lt;/p&gt;
&lt;p&gt;To proceed we need to define some measure of variation or randomness. A good one is variance. Our goal is to decompose &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; into vectors along which &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; has the most variance. Directions are represented by unit vectors (i.e. vectors of length 1). If &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; is a non-random unit vector, then the component of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; along &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle\ \vec{\omega}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle\)&lt;/span&gt; denotes the inner product in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^d\)&lt;/span&gt; (aka, dot product). Since &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; is not random, the randomness of &lt;span class=&#34;math inline&#34;&gt;\(\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle\ \vec{\omega}\)&lt;/span&gt; is controlled entirely by the coefficient &lt;span class=&#34;math inline&#34;&gt;\(\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle\)&lt;/span&gt;. To find the direction of maximal variance is to simply find &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; that maximizes the variance of this inner product. In other words we want&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{\omega}_1 := \text{argmax} \ \ \text{Var}( \langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle ) 
\]&lt;/span&gt;
where the argmax is taken over all &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(||\vec{\omega}|| = 1\)&lt;/span&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. We begin:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \begin{aligned}

\text{Var}( \langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle )  &amp;amp;= 
\text{E}\bigg[\bigg(\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle - \text{E}[\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle]\bigg)^2\bigg] \\


    &amp;amp;= \text{E}[\langle\ \vec{\omega}\ , \ \vec{X} - \text{E}[\vec{X}] \ \rangle^2] \\
    
    
    &amp;amp;= \text{E}\bigg[\ \bigg(\sum_i\omega_i(X^i - \text{E}[X^i])\bigg)^2\bigg] \\
    
    
    &amp;amp;= \text{E}\bigg[ \sum_{i,j}\omega_i\omega_j(X^i - \text{E}[X^i])(X^j - \text{E}[X^j]) \bigg] \\
    
    
    &amp;amp;= \sum_{i,j}\omega_i\omega_j \ \text{E}\bigg[ \ (X^i - \text{E}[X^i])(X^j - \text{E}[X^j]) \ \bigg] \\
    
    
    &amp;amp;= \sum_{i,j}\omega_i\omega_j \ \text{Cov}(X^i, X^j) \\
    
    
    &amp;amp;= \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle
  \end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\vec{X})\)&lt;/span&gt; is the covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;. So&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{\omega}_1 := \text{argmax} \ \ \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle 
\]&lt;/span&gt;
again the argmax is taken over all &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(||\vec{\omega}|| = 1\)&lt;/span&gt;. This problem is called a “variational problem”, but why so is not important at the moment.&lt;/p&gt;
&lt;p&gt;This &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; must be the first eigenvector of the matrix &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\vec{X})\)&lt;/span&gt;. Why? &lt;strong&gt;This is the hard part. If you can understand what follows you’re golden&lt;/strong&gt;. There are multiple ways to see why this is the case:&lt;/p&gt;
&lt;p&gt;One is by Lagrange multipliers. If we write &lt;span class=&#34;math inline&#34;&gt;\(f(\vec{\omega}) := \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle\)&lt;/span&gt; then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\begin{aligned}

f(\vec{\omega} + \vec{h}) - f(\vec{\omega}) &amp;amp;=  \langle \ \vec{\omega} + \vec{h} \ , \ \text{Cov}(\vec{X})(\vec{\omega}+\vec{h}) \ \rangle - \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle \\

    &amp;amp;= \langle \ \vec{h} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle + \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{h}\rangle \ + \langle \ \vec{h} \ , \ \text{Cov}(\vec{X})\vec{h} \rangle \\
    
    &amp;amp;= 2\langle \ \vec{h} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle + \langle \ \vec{h} \ , \ \text{Cov}(\vec{X})\vec{h} \rangle

\end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we first expanded the first term using the bilinearity of the inner product, canceled like terms, and lastly used the symmetry of the covariance matrix to combine two terms. In the above expression the first order term in &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}\)&lt;/span&gt; is given by &lt;span class=&#34;math inline&#34;&gt;\(\langle \ \vec{h} \ , \ 2\text{Cov}(\vec{X})\vec{\omega} \ \rangle\)&lt;/span&gt;. The other term is quadratic in &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}\)&lt;/span&gt;. By definition the differential of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; is this linear term:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
df_{\vec{\omega}} \ (\vec{h}) = \langle \ \vec{h} \ , \ 2\text{Cov}(\vec{X})\vec{\omega} \ \rangle
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By definition&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; is just the vector in the above expression which the inner product with &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}\)&lt;/span&gt; is being taken:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nabla_{\vec{\omega}} f = 2 \ \text{Cov}(\vec{X}) \ \vec{\omega}
\]&lt;/span&gt;
Because our variational problem is to maximize &lt;span class=&#34;math inline&#34;&gt;\(f(\vec{\omega})\)&lt;/span&gt; on the unit sphere where &lt;span class=&#34;math inline&#34;&gt;\(||\vec{\omega}|| = 1\)&lt;/span&gt;, then the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at the maximizing point &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; must be orthogonal (i.e. perpendicular, i.e. normal) to the surface of the unit sphere at that point&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. The direction (i.e. unit vector) perpendicular to the unit sphere at &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; itself with its starting point translated to the surface!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/Spherical_unit_vectors.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Thus the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; must be collinear with (and hence a multiple of) &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nabla_{\vec{\omega}_1}f  = \lambda&amp;#39; \ \vec{\omega}_1
\]&lt;/span&gt;
for some number &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;#39;\)&lt;/span&gt;. Thus&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Cov}(\vec{X}) \ \vec{\omega}_1 = \frac{\lambda&amp;#39;}{2} \ \vec{\omega}_1 =: \lambda \ \vec{\omega}_1
\]&lt;/span&gt;
Hence &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; is an eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\vec{X})\)&lt;/span&gt;. We note that the eigenvalue &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is just the variance we wanted to maximize:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\langle \ \vec{\omega}_1 \ , \ \text{Cov}(\vec{X}) \ \vec{\omega}_1 \ \rangle = \langle \ \vec{\omega}_1 \ , \lambda \vec{\omega}_1 \ \rangle = \lambda\langle \ \vec{\omega}_1 \ , \vec{\omega}_1 \ \rangle = \lambda ||\vec{\omega}||^2 = \lambda
\]&lt;/span&gt;
Thus we see that eigenvectors capture directions of maximal variance and eigenvalues capture the value of the variance in that maximal direction! We can also see why the variance is a nice measure of variation/randomness. Because it’s &lt;strong&gt;quadratic&lt;/strong&gt; in its arguments, derivatives of it become &lt;strong&gt;linear&lt;/strong&gt;, leading to &lt;strong&gt;linear&lt;/strong&gt; eigenvalue problems, which are very well understood by mathematicians.&lt;/p&gt;
&lt;p&gt;We proceed as follows. Let &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}_{new} = \vec{X} - \langle\vec{\omega}_1,\vec{X}\rangle\vec{\omega}_1\)&lt;/span&gt;. This &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}_{new}\)&lt;/span&gt; is just the component of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt;. Intuitively it’s the part of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; that &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; can not explain.&lt;/p&gt;
&lt;p&gt;Just as before we want to capture the direction of maximal variance of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}_{new}\)&lt;/span&gt;. I.e. we want a vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_2\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(||\vec{\omega}_2||=1\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\langle \vec{\omega}_2, \vec{X}_{new}\rangle)\)&lt;/span&gt; is maximal.&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}_{new} \perp \vec{\omega}_1\)&lt;/span&gt;&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; then &lt;span class=&#34;math inline&#34;&gt;\(\langle \vec{\omega}, \vec{X}_{new}\rangle = \langle \vec{\omega} - \alpha\vec{\omega}_1, \vec{X}_{new}\rangle\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in \mathbb{R}\)&lt;/span&gt;. Therefore by replacing &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega} - \langle\vec{\omega}_1,\vec{\omega}\rangle\vec{\omega}_1\)&lt;/span&gt; we may restrict our maximization problem to maximizing &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\langle \vec{\omega}, \vec{X}_{new}\rangle)\)&lt;/span&gt; over all &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(||\vec{\omega}|| = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega} \perp \vec{\omega}_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We transform this expression as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\begin{aligned}

\text{Var}(\langle \vec{\omega}, \vec{X}_{new}\rangle) &amp;amp;= \text{Var}(\langle \vec{\omega}, \vec{X} - \langle\vec{\omega}_1,\vec{X}\rangle\vec{\omega}_1\rangle) \\

    &amp;amp;= \text{Var}(\langle \vec{\omega}, \vec{X}\rangle) \qquad \text{Since }\vec{\omega}\perp\vec{\omega}_1 \\
    
    &amp;amp;= \langle \vec{\omega}, \text{Cov}(\vec{X})\vec{\omega}\rangle \qquad \text{By the earlier computation}

\end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So the vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_2\)&lt;/span&gt; is given by the new variational problem&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{\omega}_2 = \text{argmax} \ \langle \vec{\omega}, \text{Cov}(\vec{X})\vec{\omega}\rangle
\]&lt;/span&gt;
where the argmax is taken over all &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(||\vec{\omega}|| = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega} \perp \vec{\omega}_1\)&lt;/span&gt;. Now &lt;span class=&#34;math inline&#34;&gt;\(\langle \vec{\omega}_2, \vec{X}_{new}\rangle = \langle \vec{\omega}_2, \vec{X}\rangle\)&lt;/span&gt; is of maximal variance in a direction perpendicular to &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Notice that this is the same maximization problem as before, but now restricted to a lower dimensional subspace (the subspace that is prependicular to &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt;). The same Lagrange multiplier calculation as before can be applied again in this subspace. This shows that &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_2\)&lt;/span&gt; is an eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\vec{X})\)&lt;/span&gt; with eigenvalue &lt;span class=&#34;math inline&#34;&gt;\(\langle \vec{\omega}_2, \text{Cov}(\vec{X})\vec{\omega}_2\rangle\)&lt;/span&gt;. This eigenvalue must be less than or equal to the eigenvalue of &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; because the maximum of the same expression is being taken over a smaller set for &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can continue this process until all eigenvectors are exhausted. By decomposing &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; into linear combinations of the eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_i\)&lt;/span&gt; we may choose to capture as much or as little of the variance of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; as we please. For example, by projecting onto the first k eigenvectors we may capture the k-dimensional variance of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{X}_k := \sum_{i = 1}^k\langle\vec{\omega}_i,\vec{X}\rangle\vec{\omega}_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sample-estimators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sample estimators&lt;/h2&gt;
&lt;p&gt;In practice we do not know the matrix &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\vec{X})\)&lt;/span&gt;, but instead have a data matrix &lt;span class=&#34;math inline&#34;&gt;\(\{ \vec{X}_j \}_{j=1}^N\)&lt;/span&gt; of row vectors representing realizations of the random vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Statistics is often concerned with constructing sample estimators of quantities. If our data rows are sampled IID from the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; then in lieu of &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(X^i,X^j)\)&lt;/span&gt; we construct the sample covariances:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S^2_{i,j} := \frac{1}{N-1}\sum_{n=1}^N\bigg(X^i_n - \bar{X}^i\bigg)\bigg(X^j_n - \bar{X}^j\bigg)
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}^i\)&lt;/span&gt; is the mean of the &lt;span class=&#34;math inline&#34;&gt;\(i^{\text{th}}\)&lt;/span&gt; feature column. This estimator is a statistic constructed for its favorable distributional properties under IID assumptions as &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; becomes large. In particular, it converges to &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(X^i,X^j)\)&lt;/span&gt; in some sense.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementing-in-eigen&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implementing in Eigen&lt;/h2&gt;
&lt;p&gt;The derivation above gives us one formula to carry out PCA: simply compute the sample covariance matrix of the data and extract its eigenvectors and eigenvalues. This may or may not be the most numerically efficient/stable algorithm to use (I haven’t checked), but this is easy enough to implement in most numerical computing languages. Here we implement it in C++ using the &lt;a href=&#34;http://eigen.tuxfamily.org/&#34;&gt;Eigen&lt;/a&gt; library. To make it more interactive we use the &lt;a href=&#34;https://cran.r-project.org/web/packages/RcppEigen/index.html&#34;&gt;RcppEigen&lt;/a&gt; package in R to allow using the function in R sessions:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;RcppEigen.h&amp;gt;

// [[Rcpp::depends(RcppEigen)]]

using namespace Eigen;


// [[Rcpp::export]]
Rcpp::List EigenDecomp(const Map&amp;lt;MatrixXd&amp;gt; M) 
{
    //Constructing sample covariance matrix 
    MatrixXd centered = M.rowwise() - M.colwise().mean();
    MatrixXd cov = centered.adjoint() * centered/(M.rows()-1);
    
    //Using Eigen&amp;#39;s eigensolver (with default settings)
    SelfAdjointEigenSolver&amp;lt;MatrixXd&amp;gt; eig(cov);
    
    VectorXd values = eig.eigenvalues();
    MatrixXd vectors = eig.eigenvectors();
    
    //Returning results as a R-list
    return Rcpp::List::create(Rcpp::Named(&amp;quot;Cov&amp;quot;) = cov,
                           Rcpp::Named(&amp;quot;values&amp;quot;) = values,
                           Rcpp::Named(&amp;quot;vectors&amp;quot;) = vectors);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note on compilation: I’m using a laptop with an i7-8750h CPU running Windows 10. The compiler is the version of &lt;a href=&#34;http://mingw-w64.org/doku.php&#34;&gt;mingw-w64&lt;/a&gt; that comes with &lt;a href=&#34;https://cran.r-project.org/bin/windows/Rtools/&#34;&gt;Rtools40&lt;/a&gt; (i.e. the Windows port of GCC 8.3.0). By creating a Makevars.win file in an &lt;code&gt;./Documents/.R&lt;/code&gt; folder file I altered R’s default flags for g++ to use:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CXXFLAGS = -march=native -O3 -Wno-ignored-attributes $(DEBUGFLAG)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Eigen is a template expression library that relies heavily on the compiler using the best options for the machine at hand. Here we’ve used &lt;code&gt;-march=native&lt;/code&gt; which enables all instruction subsets supported by my local machine. For more info running &lt;code&gt;g++ -march=native -Q --help=target&lt;/code&gt; in the command line will show you what compiler flags this turns on. For example mine enables flags targeting AVX2, as well as a variety of others. The &lt;code&gt;-Wno-ignored-attributes&lt;/code&gt; suppresses the large number of ignored attributes warnings that an expression template library like Eigen can produce. Let’s compare with R’s built in PCA function &lt;a href=&#34;https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp&#34;&gt;prcomp&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
X = matrix(rnorm(10000*4), 10000, 4)

R = prcomp(X)

Cpp = EigenDecomp(X)

print(R$sdev^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.036884 1.021022 1.013685 1.001778&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(Cpp$values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.001778 1.013685 1.021022 1.036884&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The eigenvalues are exactly the same, just in opposite order. Next time we might link an optimized BLAS library such as Intel’s MKL, but I suspect the plain Eigen version is quite competitive.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The exact definition of “random variable” or “random vector” is unimportant. For mathematicians this means that there is a probability space &lt;span class=&#34;math inline&#34;&gt;\((\Omega, \mathcal{M}, \mathbf{P})\)&lt;/span&gt; and that &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}:\Omega \mapsto \mathbb{R}^d\)&lt;/span&gt; is a Borel-measurable map.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Note on existence. A vector that attains the maximum must exist because the expression being maximized is continuous (in fact quadratic) in &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; and the unit sphere in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^d\)&lt;/span&gt; is compact.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;And this is indeed the true definition of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient#Differential_or_(exterior)_derivative&#34;&gt;gradient of a function&lt;/a&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;This is the method of &lt;a href=&#34;https://en.wikipedia.org/wiki/Lagrange_multiplier#Modern_formulation_via_differentiable_manifolds&#34;&gt;Lagrange multipliers&lt;/a&gt;. It can be proven easily as follows. Let &lt;span class=&#34;math inline&#34;&gt;\(\vec{v}\)&lt;/span&gt; be any vector tangent to the sphere at the maximizing point &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(\gamma(t)\)&lt;/span&gt; be a smooth curve on the sphere going through &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\gamma&amp;#39;(t) = \vec{v}\)&lt;/span&gt;. Then the function &lt;span class=&#34;math inline&#34;&gt;\(f(\gamma(t))\)&lt;/span&gt; achieves a maximum at the value of &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; at which &lt;span class=&#34;math inline&#34;&gt;\(\gamma(t) = \vec{\omega}_1\)&lt;/span&gt; so it’s derivative must be 0 there. Thus &lt;span class=&#34;math inline&#34;&gt;\(0 = d/dt(f(\gamma(t))) = df_{\vec{\omega}_1} \ (\gamma&amp;#39;(t)) = \langle \gamma&amp;#39;(t),\nabla_{\vec{\omega}_1}f\rangle = \langle\vec{v},\nabla_{\vec{\omega}_1}f\rangle\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(\vec{v}\)&lt;/span&gt; was an arbitrary tangent vector this shows that &lt;span class=&#34;math inline&#34;&gt;\(\nabla_{\vec{\omega}_1}f\)&lt;/span&gt; is orthogonal to every tangent vector and hence is a normal vector.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;The symbol &lt;span class=&#34;math inline&#34;&gt;\(\perp\)&lt;/span&gt; means “prependicular to”.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
