<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>r | Fundamenta Nova</title>
    <link>/tag/r/</link>
      <atom:link href="/tag/r/index.xml" rel="self" type="application/rss+xml" />
    <description>r</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 20 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>r</title>
      <link>/tag/r/</link>
    </image>
    
    <item>
      <title>Parallel Monte Carlo: Simulating Compound Poisson Processes using C&#43;&#43; and TBB</title>
      <link>/post/007_parallelmc1/main/</link>
      <pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/007_parallelmc1/main/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post we implement a function to simulate random samples of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Compound_Poisson_process&#34;&gt;Compound Poisson variable&lt;/a&gt;. A random variable &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is a compound Poisson (CP) random variable if there exists a Poisson random variable &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, and a random variable &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L = \sum_{i = 1}^N S_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(S_i\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i \in \mathbb{N}\)&lt;/span&gt; is an IID sequence of random variables with the same distribution as &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; and that are independent of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. This kind of expression is typical in operational risk modeling, insurance modeling, and &lt;a href=&#34;https://en.wikipedia.org/wiki/Ruin_theory&#34;&gt;ruin theory&lt;/a&gt;. Typically, the variable &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is a counter for the occurrence of loss events to an insurance portfolio over a given time period, and &lt;span class=&#34;math inline&#34;&gt;\(S_i\)&lt;/span&gt; is the severity of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th loss event. A CP variable/process is the most common approach banks take to model operational risk as part of their &lt;a href=&#34;https://en.wikipedia.org/wiki/Advanced_measurement_approach&#34;&gt;Advanced Measurement Approach&lt;/a&gt;. The advantage of this model in operational risk is that losses (and hence data) tend to be sparse in this domain. In addition, losses tend to be heavy tailed. By splitting event frequency (&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;) from event severity (&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;) the model developer can use more data to fit loss event distributions and loss frequency distributions independently (after accepting the assumptions of the CP process). Allowing one to focus on fitting the tail of the loss distribution, without having to worry about the frequency of occurrences.&lt;/p&gt;
&lt;p&gt;We’ll implement a function that simulates random samples of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; in R, serially in C++, and in parallel in C++ using the &lt;a href=&#34;https://github.com/oneapi-src/oneTBB&#34;&gt;Threading Building Blocks (TBB)&lt;/a&gt; library.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R Implementation&lt;/h2&gt;
&lt;p&gt;R is extremely expressive when it comes to mathematical, statistical, and graphing operations and the base implementation is quite simple:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Function to simulate random samples from a CP variable with 
## log-normal severities
rCP = function(lambda, mu, sigma, N)
{
    sapply(1:N, FUN = function(dummy){
        sum(rlnorm(n = rpois(1, lambda), meanlog = mu, sdlog = sigma))
    })
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;serial-c-implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Serial C++ Implementation&lt;/h2&gt;
&lt;p&gt;We now implement the sampler in C++ using the &lt;a href=&#34;http://www.rcpp.org/&#34;&gt;Rcpp package&lt;/a&gt;. We will also make use of the &lt;a href=&#34;https://cran.r-project.org/web/packages/dqrng/index.html&#34;&gt;dqrng&lt;/a&gt; package so that we can conveniently include the &lt;a href=&#34;https://www.pcg-random.org/&#34;&gt;PCG&lt;/a&gt; random number generator by Melissa O’Neil.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;

// [[Rcpp::depends(dqrng)]]
// [[Rcpp::plugins(cpp11)]]

#include &amp;lt;pcg_random.hpp&amp;gt;
#include &amp;lt;random&amp;gt;
#include &amp;lt;algorithm&amp;gt;

// [[Rcpp::export]]
Rcpp::NumericVector cppCP(const double lambda,
                          const double mu,
                          const double sigma,
                          const int N)
{
    // Seed with a real random value, if available
    pcg_extras::seed_seq_from&amp;lt;std::random_device&amp;gt; seed_source;

    // Make a random number engine
    pcg64 rng(seed_source);
    
    // Distribution for frequency
    std::poisson_distribution&amp;lt;int&amp;gt; Freq(lambda);
    
    // Distribution for severity
    std::lognormal_distribution&amp;lt;double&amp;gt; Sev(mu, sigma);
    
    // Allocate vector
    Rcpp::NumericVector out(N);

    // Simulate samples
    std::generate(out.begin(), out.end(), [&amp;amp;](){
        
        // Simulating loss event count
        int n = Freq(rng);
        
        // Accumulating loss severities
        double s = 0;
        for(int i = 0; i &amp;lt; n; ++i) s += Sev(rng);
        
        return s;
    });
    
    return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x_r = rCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6)
x_cpp = cppCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6)

ks.test(x = x_r, y = x_cpp, alternative = &amp;quot;two.sided&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in ks.test(x = x_r, y = x_cpp, alternative = &amp;quot;two.sided&amp;quot;): p-value will
## be approximate in the presence of ties&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two-sample Kolmogorov-Smirnov test
## 
## data:  x_r and x_cpp
## D = 0.001113, p-value = 0.5654
## alternative hypothesis: two-sided&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark::microbenchmark(
    &amp;quot;R&amp;quot; = rCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6),
    &amp;quot;C++&amp;quot; = cppCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6),
    times = 10L
    )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##  expr       min        lq      mean    median        uq       max neval
##     R 4493.2542 4681.6711 4823.3279 4786.7206 5051.4572 5104.8304    10
##   C++  152.9929  156.1746  158.0281  156.6728  159.5525  165.4695    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that a Kolmogorov-Smirnov test for equality of distributions shows that the two samples aren’t statistically distinct. Also the C++ version is ~30 times faster than the R version. Note that we added the compilation flags &lt;code&gt;-O3 -march=native&lt;/code&gt; to the Makevars file in our Documents/.R folder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parallel-c-implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parallel C++ Implementation&lt;/h2&gt;
&lt;p&gt;There are many libraries we can use to parallelize the C++ code above. These include &lt;a href=&#34;https://www.openmp.org/&#34;&gt;OpenMP&lt;/a&gt; and &lt;a href=&#34;https://www.openacc.org/&#34;&gt;OpenACC&lt;/a&gt; (both of which allow for standards based parallelization through directive based APIs, with newer standards allowing for GPU offloading), &lt;a href=&#34;https://www.open-mpi.org/&#34;&gt;MPI&lt;/a&gt; and &lt;a href=&#34;https://www.boost.org/doc/libs/1_74_0/doc/html/mpi.html&#34;&gt;Boost.MPI&lt;/a&gt; for distributed messaging passing, &lt;a href=&#34;https://github.com/kokkos/kokkos&#34;&gt;Kokkos&lt;/a&gt;, &lt;a href=&#34;https://github.com/taskflow/taskflow&#34;&gt;Taskflow&lt;/a&gt;, &lt;a href=&#34;https://github.com/boostorg/compute&#34;&gt;Boost.Compute&lt;/a&gt;, &lt;a href=&#34;https://github.com/STEllAR-GROUP/hpx/&#34;&gt;HPX&lt;/a&gt;, etc.&lt;/p&gt;
&lt;p&gt;However, &lt;a href=&#34;https://rcppcore.github.io/RcppParallel/tbb.html&#34;&gt;Threading Building Blocks (TBB)&lt;/a&gt; is very powerful, expressive, mature, and is very conveniently included in the &lt;a href=&#34;https://rcppcore.github.io/RcppParallel/&#34;&gt;RcppParallel&lt;/a&gt; package. I find TBB’s API to be very well designed. So TBB it is!&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;RcppParallel.h&amp;gt;

// [[Rcpp::depends(dqrng, RcppParallel)]]
// [[Rcpp::plugins(cpp11)]]

#include &amp;lt;pcg_random.hpp&amp;gt;
#include &amp;lt;random&amp;gt;
#include &amp;lt;algorithm&amp;gt;



// [[Rcpp::export]]
Rcpp::NumericVector tbbCP(const double lambda,
                          const double mu,
                          const double sigma,
                          const int N,
                          const uint64_t seed)
{
    using brange = tbb::blocked_range&amp;lt;size_t&amp;gt;;
    
    // Allocate vector
    Rcpp::NumericVector out(N);
    
    // Getting pointer to data
    auto begin = out.begin();
    
    tbb::parallel_for(brange(0, N), [=](brange&amp;amp; range){
        
        // Distribution for frequency
        std::poisson_distribution&amp;lt;int&amp;gt; Freq(lambda);
    
        // Distribution for severity
        std::lognormal_distribution&amp;lt;double&amp;gt; Sev(mu, sigma);
        
        // RNG local to thread, with unique stream
        pcg64 rng(seed, range.end());
        
        // Serial version of sampler
        auto seq_CP = [&amp;amp;](){
            
            // Simulating loss event count
            int n = Freq(rng);
            
            // Accumulating loss severities
            double s = 0;
            for(int i = 0; i &amp;lt; n; ++i) s += Sev(rng);
            
            return s;
        };
        
        // Loop to simulate samples
        std::generate(begin + range.begin(), begin + range.end(), seq_CP);
    });
    
    return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;C++11 and TBB allow for pretty parallel code. Let’s see if this function is as useful as it is pretty.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x_tbb = tbbCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6, seed = 42)

ks.test(x = x_cpp, y = x_tbb, alternative = &amp;quot;two.sided&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in ks.test(x = x_cpp, y = x_tbb, alternative = &amp;quot;two.sided&amp;quot;): p-value
## will be approximate in the presence of ties&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two-sample Kolmogorov-Smirnov test
## 
## data:  x_cpp and x_tbb
## D = 0.001094, p-value = 0.5877
## alternative hypothesis: two-sided&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ks.test(x = x_r, y = x_tbb, alternative = &amp;quot;two.sided&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in ks.test(x = x_r, y = x_tbb, alternative = &amp;quot;two.sided&amp;quot;): p-value will
## be approximate in the presence of ties&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two-sample Kolmogorov-Smirnov test
## 
## data:  x_r and x_tbb
## D = 0.001379, p-value = 0.2977
## alternative hypothesis: two-sided&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Kolmogorov-Smirnov test again can’t tell the samples apart between R, serial C++, and TBB implementations. As for performance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark::microbenchmark(
    &amp;quot;R&amp;quot; = rCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6),
    &amp;quot;C++&amp;quot; = cppCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6),
    &amp;quot;TBB&amp;quot; = tbbCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6, seed = 42),
    times = 10L
    )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##  expr       min        lq       mean    median        uq       max neval
##     R 4510.9116 4847.8254 4861.47488 4878.6472 4887.1488 5108.7465    10
##   C++  153.7255  155.1583  157.70383  155.8393  157.7901  167.4164    10
##   TBB   17.7385   17.8813   22.56139   19.3085   28.7204   29.5262    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;TBB ran ~8 times faster than the serial version. The algorithm’s runtime scales linearly with &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, so let’s compare the two C++ versions with a higher lambda:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark::microbenchmark(
    &amp;quot;C++&amp;quot; = cppCP(lambda = 17, mu = 5.5, sigma = 2.5, N = 10^6),
    &amp;quot;TBB&amp;quot; = tbbCP(lambda = 17, mu = 5.5, sigma = 2.5, N = 10^6, seed = 42),
    times = 50L
    )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##  expr       min        lq      mean    median        uq       max neval
##   C++ 1388.7126 1398.7598 1401.8384 1401.6124 1404.7762 1427.1751    50
##   TBB  147.0726  147.4528  148.7259  147.5522  148.9511  154.5965    50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;TBB ran ~9.5 times faster than the serial version.&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In this post we implement a function to simulate random samples of a <a href="https://en.wikipedia.org/wiki/Compound_Poisson_process">Compound Poisson variable</a>. A random variable <span class="math inline">\(L\)</span> is a compound Poisson (CP) random variable if there exists a Poisson random variable <span class="math inline">\(N\)</span>, and a random variable <span class="math inline">\(S\)</span> such that</p>
<p><span class="math display">\[
L = \sum_{i = 1}^N S_i
\]</span></p>
<p>where <span class="math inline">\(S_i\)</span> for <span class="math inline">\(i \in \mathbb{N}\)</span> is an IID sequence of random variables with the same distribution as <span class="math inline">\(S\)</span> and that are independent of <span class="math inline">\(N\)</span>. This kind of expression is typical in operational risk modeling, insurance modeling, and <a href="https://en.wikipedia.org/wiki/Ruin_theory">ruin theory</a>. Typically, the variable <span class="math inline">\(N\)</span> is a counter for the occurrence of loss events to an insurance portfolio over a given time period, and <span class="math inline">\(S_i\)</span> is the severity of the <span class="math inline">\(i\)</span>-th loss event. A CP variable/process is the most common approach banks take to model operational risk as part of their <a href="https://en.wikipedia.org/wiki/Advanced_measurement_approach">Advanced Measurement Approach</a>. The advantage of this model in operational risk is that losses (and hence data) tend to be sparse in this domain. In addition, losses tend to be heavy tailed. By splitting event frequency (<span class="math inline">\(N\)</span>) from event severity (<span class="math inline">\(S\)</span>) the model developer can use more data to fit loss event distributions and loss frequency distributions independently (after accepting the assumptions of the CP process). Allowing one to focus on fitting the tail of the loss distribution, without having to worry about the frequency of occurrences.</p>
<p>We’ll implement a function that simulates random samples of <span class="math inline">\(L\)</span> in R, serially in C++, and in parallel in C++ using the <a href="https://github.com/oneapi-src/oneTBB">Threading Building Blocks (TBB)</a> library.</p>
</div>
<div id="r-implementation" class="section level2">
<h2>R Implementation</h2>
<p>R is extremely expressive when it comes to mathematical, statistical, and graphing operations and the base implementation is quite simple:</p>
<pre class="r"><code>## Function to simulate random samples from a CP variable with 
## log-normal severities
rCP = function(lambda, mu, sigma, N)
{
    sapply(1:N, FUN = function(dummy){
        sum(rlnorm(n = rpois(1, lambda), meanlog = mu, sdlog = sigma))
    })
}</code></pre>
</div>
<div id="serial-c-implementation" class="section level2">
<h2>Serial C++ Implementation</h2>
<p>We now implement the sampler in C++ using the <a href="http://www.rcpp.org/">Rcpp package</a>. We will also make use of the <a href="https://cran.r-project.org/web/packages/dqrng/index.html">dqrng</a> package so that we can conveniently include the <a href="https://www.pcg-random.org/">PCG</a> random number generator by Melissa O’Neil.</p>
<pre class="cpp"><code>#include &lt;Rcpp.h&gt;

// [[Rcpp::depends(dqrng)]]
// [[Rcpp::plugins(cpp11)]]

#include &lt;pcg_random.hpp&gt;
#include &lt;random&gt;
#include &lt;algorithm&gt;

// [[Rcpp::export]]
Rcpp::NumericVector cppCP(const double lambda,
                          const double mu,
                          const double sigma,
                          const int N)
{
    // Seed with a real random value, if available
    pcg_extras::seed_seq_from&lt;std::random_device&gt; seed_source;

    // Make a random number engine
    pcg64 rng(seed_source);
    
    // Distribution for frequency
    std::poisson_distribution&lt;int&gt; Freq(lambda);
    
    // Distribution for severity
    std::lognormal_distribution&lt;double&gt; Sev(mu, sigma);
    
    // Allocate vector
    Rcpp::NumericVector out(N);

    // Simulate samples
    std::generate(out.begin(), out.end(), [&amp;](){
        
        // Simulating loss event count
        int n = Freq(rng);
        
        // Accumulating loss severities
        double s = 0;
        for(int i = 0; i &lt; n; ++i) s += Sev(rng);
        
        return s;
    });
    
    return out;
}</code></pre>
<pre class="r"><code>x_r = rCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6)
x_cpp = cppCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6)

ks.test(x = x_r, y = x_cpp, alternative = &quot;two.sided&quot;)</code></pre>
<pre><code>## Warning in ks.test(x = x_r, y = x_cpp, alternative = &quot;two.sided&quot;): p-value will
## be approximate in the presence of ties</code></pre>
<pre><code>## 
##  Two-sample Kolmogorov-Smirnov test
## 
## data:  x_r and x_cpp
## D = 0.001113, p-value = 0.5654
## alternative hypothesis: two-sided</code></pre>
<pre class="r"><code>microbenchmark::microbenchmark(
    &quot;R&quot; = rCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6),
    &quot;C++&quot; = cppCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6),
    times = 10L
    )</code></pre>
<pre><code>## Unit: milliseconds
##  expr       min        lq      mean    median        uq       max neval
##     R 4493.2542 4681.6711 4823.3279 4786.7206 5051.4572 5104.8304    10
##   C++  152.9929  156.1746  158.0281  156.6728  159.5525  165.4695    10</code></pre>
<p>We see that a Kolmogorov-Smirnov test for equality of distributions shows that the two samples aren’t statistically distinct. Also the C++ version is ~30 times faster than the R version. Note that we added the compilation flags <code>-O3 -march=native</code> to the Makevars file in our Documents/.R folder.</p>
</div>
<div id="parallel-c-implementation" class="section level2">
<h2>Parallel C++ Implementation</h2>
<p>There are many libraries we can use to parallelize the C++ code above. These include <a href="https://www.openmp.org/">OpenMP</a> and <a href="https://www.openacc.org/">OpenACC</a> (both of which allow for standards based parallelization through directive based APIs, with newer standards allowing for GPU offloading), <a href="https://www.open-mpi.org/">MPI</a> and <a href="https://www.boost.org/doc/libs/1_74_0/doc/html/mpi.html">Boost.MPI</a> for distributed messaging passing, <a href="https://github.com/kokkos/kokkos">Kokkos</a>, <a href="https://github.com/taskflow/taskflow">Taskflow</a>, <a href="https://github.com/boostorg/compute">Boost.Compute</a>, <a href="https://github.com/STEllAR-GROUP/hpx/">HPX</a>, etc.</p>
<p>However, <a href="https://rcppcore.github.io/RcppParallel/tbb.html">Threading Building Blocks (TBB)</a> is very powerful, expressive, mature, and is very conveniently included in the <a href="https://rcppcore.github.io/RcppParallel/">RcppParallel</a> package. I find TBB’s API to be very well designed. So TBB it is!</p>
<pre class="cpp"><code>#include &lt;Rcpp.h&gt;
#include &lt;RcppParallel.h&gt;

// [[Rcpp::depends(dqrng, RcppParallel)]]
// [[Rcpp::plugins(cpp11)]]

#include &lt;pcg_random.hpp&gt;
#include &lt;random&gt;
#include &lt;algorithm&gt;



// [[Rcpp::export]]
Rcpp::NumericVector tbbCP(const double lambda,
                          const double mu,
                          const double sigma,
                          const int N,
                          const uint64_t seed)
{
    using brange = tbb::blocked_range&lt;size_t&gt;;
    
    // Allocate vector
    Rcpp::NumericVector out(N);
    
    // Getting pointer to data
    auto begin = out.begin();
    
    tbb::parallel_for(brange(0, N), [=](brange&amp; range){
        
        // Distribution for frequency
        std::poisson_distribution&lt;int&gt; Freq(lambda);
    
        // Distribution for severity
        std::lognormal_distribution&lt;double&gt; Sev(mu, sigma);
        
        // RNG local to thread, with unique stream
        pcg64 rng(seed, range.end());
        
        // Serial version of sampler
        auto seq_CP = [&amp;](){
            
            // Simulating loss event count
            int n = Freq(rng);
            
            // Accumulating loss severities
            double s = 0;
            for(int i = 0; i &lt; n; ++i) s += Sev(rng);
            
            return s;
        };
        
        // Loop to simulate samples
        std::generate(begin + range.begin(), begin + range.end(), seq_CP);
    });
    
    return out;
}</code></pre>
<p>C++11 and TBB allow for pretty parallel code. Let’s see if this function is as useful as it is pretty.</p>
<pre class="r"><code>x_tbb = tbbCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6, seed = 42)

ks.test(x = x_cpp, y = x_tbb, alternative = &quot;two.sided&quot;)</code></pre>
<pre><code>## Warning in ks.test(x = x_cpp, y = x_tbb, alternative = &quot;two.sided&quot;): p-value
## will be approximate in the presence of ties</code></pre>
<pre><code>## 
##  Two-sample Kolmogorov-Smirnov test
## 
## data:  x_cpp and x_tbb
## D = 0.001094, p-value = 0.5877
## alternative hypothesis: two-sided</code></pre>
<pre class="r"><code>ks.test(x = x_r, y = x_tbb, alternative = &quot;two.sided&quot;)</code></pre>
<pre><code>## Warning in ks.test(x = x_r, y = x_tbb, alternative = &quot;two.sided&quot;): p-value will
## be approximate in the presence of ties</code></pre>
<pre><code>## 
##  Two-sample Kolmogorov-Smirnov test
## 
## data:  x_r and x_tbb
## D = 0.001379, p-value = 0.2977
## alternative hypothesis: two-sided</code></pre>
<p>The Kolmogorov-Smirnov test again can’t tell the samples apart between R, serial C++, and TBB implementations. As for performance:</p>
<pre class="r"><code>microbenchmark::microbenchmark(
    &quot;R&quot; = rCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6),
    &quot;C++&quot; = cppCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6),
    &quot;TBB&quot; = tbbCP(lambda = 1.7, mu = 5.5, sigma = 2.5, N = 10^6, seed = 42),
    times = 10L
    )</code></pre>
<pre><code>## Unit: milliseconds
##  expr       min        lq       mean    median        uq       max neval
##     R 4510.9116 4847.8254 4861.47488 4878.6472 4887.1488 5108.7465    10
##   C++  153.7255  155.1583  157.70383  155.8393  157.7901  167.4164    10
##   TBB   17.7385   17.8813   22.56139   19.3085   28.7204   29.5262    10</code></pre>
<p>TBB ran ~8 times faster than the serial version. The algorithm’s runtime scales linearly with <span class="math inline">\(\lambda\)</span>, so let’s compare the two C++ versions with a higher lambda:</p>
<pre class="r"><code>microbenchmark::microbenchmark(
    &quot;C++&quot; = cppCP(lambda = 17, mu = 5.5, sigma = 2.5, N = 10^6),
    &quot;TBB&quot; = tbbCP(lambda = 17, mu = 5.5, sigma = 2.5, N = 10^6, seed = 42),
    times = 50L
    )</code></pre>
<pre><code>## Unit: milliseconds
##  expr       min        lq      mean    median        uq       max neval
##   C++ 1388.7126 1398.7598 1401.8384 1401.6124 1404.7762 1427.1751    50
##   TBB  147.0726  147.4528  148.7259  147.5522  148.9511  154.5965    50</code></pre>
<p>TBB ran ~9.5 times faster than the serial version.</p>
</div>
</content:encoded>	
    </item>
    
    <item>
      <title>Data and their misbehavior</title>
      <link>/post/006_samplestats/main/</link>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/006_samplestats/main/</guid>
      <description>


&lt;p&gt;To be honest, I use the clickbaity word “data” in the title when I really mean “sample statistics”. The point of this post is first illustrated using a sample mean, but applies to any estimate computed from data.&lt;/p&gt;
&lt;p&gt;If you ask a student in statistics to write a formula for “the mean” they may write the expression:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\bar{X} := \frac{1}{N}\sum_{i = 1}^N X_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are at least two “problems” with this answer. The first problem is that the above expression is not the mean of a distribution, but it is a sample statistic from some sample data &lt;span class=&#34;math inline&#34;&gt;\(\{X_i\}\)&lt;/span&gt;. It is not simply a constant number but a random variable in its own right. It has a distribution, a mean, a variance, quantiles, etc. If we had done a different “experiment” and had obtained different data the variable &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; may likely have come out with a different value. If the data were generated by a particularly misbehaved random process, then the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; may have little to do with a measure of centrality and may tell us less little about the distribution of any future data element &lt;span class=&#34;math inline&#34;&gt;\(X_{N+1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lesson:&lt;/strong&gt; Quantities estimated from data are usually random variables, and it’s necessary to understand their distributional properties before &lt;em&gt;inferring&lt;/em&gt; anything from the observed value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second problem however comes from the fact that in some cases the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; makes it indeed a very good approximation (in a precise sense) to a mean. If the data &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; are generated IID, with “small” variance, then the central limit theorem tells us that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; has a distribution that is approximately normal with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu = E[X_i]\)&lt;/span&gt;, and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = \frac{Var[X_i]}{N}\)&lt;/span&gt;. If the data is truly IID and generated from a distribution that is thin tailed then this limit is indeed a good approximation, even for modest data sizes. If this is the case and if we assume the limiting distribution is correct then the fact that normal distributions themselves have &lt;em&gt;very&lt;/em&gt; thin tails means that we can construct short yet extremely strong confidence intervals for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; centered on &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt;. So in this case, thin tails make it easy (&lt;strong&gt;far too easy&lt;/strong&gt;) for a student to forget the difference between the sample statistic &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; and the distributional parameter &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Unfortunately, in many interesting situations, data is not generated IID with small variance. In the previous sentence I personally would replace the word “many” by “most”. Nassim Taleb, the author of Fooled by Randomness and the Black Swan, many very well replace it by “all”. In any case, dividing data into “IID” and “non-IID”, or “definitely thin tailed” and “possibly heavy tailed” is like dividing animals into “elephants” and “non-elephants”.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: It’s true that IID data with finite variance will have a sample mean that converges in distribution to a normal. But the rate of convergence may be very slow for data with large (or numerically infinite) variance, or for data that is not exactly IID. Do not assume asymptotic normality unless 1) you are &lt;strong&gt;&lt;em&gt;absolutely&lt;/em&gt;&lt;/strong&gt; sure, or 2) you are absolutely sure that the mistake you would make from such an assumption would not be fatal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Economics and finance abound with examples. Market data is almost never IID in any observable sense almost as a corollary to market participants seeking out arbitrage opportunities, causing previously stable relationships to degrade (a variation of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Lucas_critique&#34;&gt;Lucas Critique&lt;/a&gt;). Insurance and Operational Risk are replete with examples of data that may well be generated IID (there would be no real way to know) but whose distributions have numerically infinite variance.&lt;/p&gt;
&lt;p&gt;In such interesting cases it could be a grave mistake to confuse sample statistics as an approximation for a particular parametric constant. For example, a Cauchy distribution is so heavy tailed that it has no finite mean. So the sample mean is not an approximation to any measure of central tendency of data generated by a Cauchy distribution, and would itself show enormous variation. In one experiment the sample mean may be 100. In the next experiment its value may very well be 100,000.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: There are no true tests for stationarity, path-independence, and IID-ness unless you make strong assumptions about the distributional properties of the data. In finance and economics, such assumptions are almost never justifiable. On the contrary, it’s very simple to give an argument for the non-stationarity, path-dependence, non-ARIMA structure of financial time series. Thus in finance and economics, in these cases, one is missing the one thing that makes data useful: limit theorems. No, the neural net you’re training will not solve the markets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Understanding the distributional properties of sample statistics is probably the most important part of the applied field of statistics. Mistaking sample statistics for constants throws away all of their distributional properties, such as their tendency to vary or to change with time, and assuming their distribution to follow from the central limit theorem may very well be incorrect.&lt;/p&gt;
</description>
      <content:encoded>


<p>To be honest, I use the clickbaity word “data” in the title when I really mean “sample statistics”. The point of this post is first illustrated using a sample mean, but applies to any estimate computed from data.</p>
<p>If you ask a student in statistics to write a formula for “the mean” they may write the expression:</p>
<p><span class="math display">\[
\bar{X} := \frac{1}{N}\sum_{i = 1}^N X_i
\]</span></p>
<p>There are at least two “problems” with this answer. The first problem is that the above expression is not the mean of a distribution, but it is a sample statistic from some sample data <span class="math inline">\(\{X_i\}\)</span>. It is not simply a constant number but a random variable in its own right. It has a distribution, a mean, a variance, quantiles, etc. If we had done a different “experiment” and had obtained different data the variable <span class="math inline">\(\bar{X}\)</span> may likely have come out with a different value. If the data were generated by a particularly misbehaved random process, then the distribution of <span class="math inline">\(\bar{X}\)</span> may have little to do with a measure of centrality and may tell us less little about the distribution of any future data element <span class="math inline">\(X_{N+1}\)</span>.</p>
<ul>
<li><strong>Lesson:</strong> Quantities estimated from data are usually random variables, and it’s necessary to understand their distributional properties before <em>inferring</em> anything from the observed value.</li>
</ul>
<p>The second problem however comes from the fact that in some cases the distribution of <span class="math inline">\(\bar{X}\)</span> makes it indeed a very good approximation (in a precise sense) to a mean. If the data <span class="math inline">\(X_i\)</span> are generated IID, with “small” variance, then the central limit theorem tells us that <span class="math inline">\(\bar{X}\)</span> has a distribution that is approximately normal with mean <span class="math inline">\(\mu = E[X_i]\)</span>, and variance <span class="math inline">\(\sigma^2 = \frac{Var[X_i]}{N}\)</span>. If the data is truly IID and generated from a distribution that is thin tailed then this limit is indeed a good approximation, even for modest data sizes. If this is the case and if we assume the limiting distribution is correct then the fact that normal distributions themselves have <em>very</em> thin tails means that we can construct short yet extremely strong confidence intervals for <span class="math inline">\(\mu\)</span> centered on <span class="math inline">\(\bar{X}\)</span>. So in this case, thin tails make it easy (<strong>far too easy</strong>) for a student to forget the difference between the sample statistic <span class="math inline">\(\bar{X}\)</span> and the distributional parameter <span class="math inline">\(\mu\)</span>.</p>
<p>Unfortunately, in many interesting situations, data is not generated IID with small variance. In the previous sentence I personally would replace the word “many” by “most”. Nassim Taleb, the author of Fooled by Randomness and the Black Swan, many very well replace it by “all”. In any case, dividing data into “IID” and “non-IID”, or “definitely thin tailed” and “possibly heavy tailed” is like dividing animals into “elephants” and “non-elephants”.</p>
<ul>
<li><strong>Lesson</strong>: It’s true that IID data with finite variance will have a sample mean that converges in distribution to a normal. But the rate of convergence may be very slow for data with large (or numerically infinite) variance, or for data that is not exactly IID. Do not assume asymptotic normality unless 1) you are <strong><em>absolutely</em></strong> sure, or 2) you are absolutely sure that the mistake you would make from such an assumption would not be fatal.</li>
</ul>
<p>Economics and finance abound with examples. Market data is almost never IID in any observable sense almost as a corollary to market participants seeking out arbitrage opportunities, causing previously stable relationships to degrade (a variation of the <a href="https://en.wikipedia.org/wiki/Lucas_critique">Lucas Critique</a>). Insurance and Operational Risk are replete with examples of data that may well be generated IID (there would be no real way to know) but whose distributions have numerically infinite variance.</p>
<p>In such interesting cases it could be a grave mistake to confuse sample statistics as an approximation for a particular parametric constant. For example, a Cauchy distribution is so heavy tailed that it has no finite mean. So the sample mean is not an approximation to any measure of central tendency of data generated by a Cauchy distribution, and would itself show enormous variation. In one experiment the sample mean may be 100. In the next experiment its value may very well be 100,000.</p>
<ul>
<li><strong>Lesson</strong>: There are no true tests for stationarity, path-independence, and IID-ness unless you make strong assumptions about the distributional properties of the data. In finance and economics, such assumptions are almost never justifiable. On the contrary, it’s very simple to give an argument for the non-stationarity, path-dependence, non-ARIMA structure of financial time series. Thus in finance and economics, in these cases, one is missing the one thing that makes data useful: limit theorems. No, the neural net you’re training will not solve the markets.</li>
</ul>
<p>Understanding the distributional properties of sample statistics is probably the most important part of the applied field of statistics. Mistaking sample statistics for constants throws away all of their distributional properties, such as their tendency to vary or to change with time, and assuming their distribution to follow from the central limit theorem may very well be incorrect.</p>
</content:encoded>	
    </item>
    
    <item>
      <title>Expectation Maximization, Part 2: Fitting Regularized Probit Regression using EM in C&#43;&#43;</title>
      <link>/post/005_em2_probit/main/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/post/005_em2_probit/main/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the &lt;a href=&#34;https://edsterjo.netlify.app/post/003_em1/main/&#34;&gt;first post&lt;/a&gt; in this series we discussed Expectation Maximization (EM) type algorithms. In the post &lt;a href=&#34;https://edsterjo.netlify.app/post/004_regularization/main/&#34;&gt;prior to this one&lt;/a&gt; we discussed regularization and showed how it leads to a bias-variance trade off in OLS models. Here we implement fitting for &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularized &lt;a href=&#34;https://en.wikipedia.org/wiki/Probit_model&#34;&gt;probit regression&lt;/a&gt; using EM. To make it more interesting we will code everything from scratch using the &lt;a href=&#34;http://eigen.tuxfamily.org/index.php?title=Main_Page&#34;&gt;Eigen&lt;/a&gt; linear algebra library, via &lt;a href=&#34;https://cran.r-project.org/web/packages/RcppEigen/index.html&#34;&gt;RcppEigen&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;probit-regression-as-a-censored-ols-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Probit Regression as a censored OLS model&lt;/h2&gt;
&lt;p&gt;In our first post on EM algorithms we emphasized that EM is particularly useful for models that have censored data. Suppose we have the following censored model. Suppose &lt;span class=&#34;math inline&#34;&gt;\(Y^*\)&lt;/span&gt; is a real valued random variable and &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; is a random vector with values in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^p\)&lt;/span&gt;. Suppose that we have the conditional relationship&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y^* \ \ | \ \ \vec{X} \sim \mathcal{N}(\ \langle \vec{X}  , \beta\rangle \ ,\ 1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(\mu, \sigma^2)\)&lt;/span&gt; denotes the univariate normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Here (and everywhere else) the symbol &lt;span class=&#34;math inline&#34;&gt;\(\langle v,w\rangle\)&lt;/span&gt; represents the Euclidean inner product (aka, &lt;a href=&#34;https://en.wikipedia.org/wiki/Dot_product&#34;&gt;dot product&lt;/a&gt;) of two vectors &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In this case we may write &lt;span class=&#34;math inline&#34;&gt;\(Y^* = \langle \vec{X} , \beta\rangle - \epsilon\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon \sim \mathcal{N}(0,1)\)&lt;/span&gt; is standard normal, which can be taken independent of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; or this distribution can be simply assumed conditional on &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Instead of observing &lt;span class=&#34;math inline&#34;&gt;\(Y^*\)&lt;/span&gt; in the data however, we observe the censored variable
&lt;span class=&#34;math display&#34;&gt;\[
Y := 
\begin{cases}
1, &amp;amp; \text{if} \ \ Y^* &amp;gt; 0 \\
0, &amp;amp; \text{otherwise}
\end{cases}
\]&lt;/span&gt;
Hence we have that &lt;span class=&#34;math inline&#34;&gt;\(Y \ |\vec{X} \sim \text{Bernoulli}(p)\)&lt;/span&gt; where
&lt;span class=&#34;math display&#34;&gt;\[
p = P(Y = 1 | \vec{X}) = P(Y^* &amp;gt; 0 | \vec{X}) = P\bigg(Y^* - \langle \vec{X},\beta\rangle &amp;gt; - \langle \vec{X},\beta\rangle\ \ \bigg|\ \ \vec{X}\bigg) = P(\epsilon &amp;lt; \langle \vec{X},\beta\rangle\ | \vec{X})
\]&lt;/span&gt;
Since &lt;span class=&#34;math inline&#34;&gt;\(\epsilon \sim \mathcal{N}(0,1)\)&lt;/span&gt; this last probability is equal to &lt;span class=&#34;math inline&#34;&gt;\(\Phi(\langle \vec{X},\beta\rangle)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; denotes the standard normal CDF. This derives the probit model.&lt;/p&gt;
&lt;p&gt;Before we proceed, notice 2 points which we won’t dwell on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the variance of &lt;span class=&#34;math inline&#34;&gt;\(Y^*\)&lt;/span&gt; had been &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 \ne 1\)&lt;/span&gt; then the value of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; would not be able to be estimated from data without knowing &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; since &lt;span class=&#34;math inline&#34;&gt;\(P(Y^* &amp;gt; 0 |\vec{X}) = P(Y^*/\sigma &amp;gt; 0 | \vec{X})\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;If the distributional relationship between &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; had been such that the error term &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; where a logistic random variable, instead of a normal one, then the censored problem would have become logistic regression.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the regression itself we assume that we have a data set &lt;span class=&#34;math inline&#34;&gt;\(\{ (y_i, \vec{x}_i)\}_{i = 1}^N\)&lt;/span&gt; consisting of samples generated independently of one another from a fixed multivariate distribution for &lt;span class=&#34;math inline&#34;&gt;\((Y, \vec{X})\)&lt;/span&gt; (i.e. we assume our data was sampled IID).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-probit-regression-via-em&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting Probit Regression via EM&lt;/h2&gt;
&lt;p&gt;Since probit regression arises from a censored normal OLS model, and since OLS is relatively easy to fit, probit regression is an excellent candidate for applying Expectation Maximization for fitting. A small difference will be that all of the probability densities involved will be conditional on the observed covariates &lt;span class=&#34;math inline&#34;&gt;\(\{\vec{x}_i\}_{i = 1}^N\)&lt;/span&gt; since regression is a conditional relationship.&lt;/p&gt;
&lt;p&gt;Let’s recall the standard EM algorithm for the case of the regression problem above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(m = 0\)&lt;/span&gt;. Make an initial estimate &lt;span class=&#34;math inline&#34;&gt;\(\beta_m\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Given the observed data &lt;span class=&#34;math inline&#34;&gt;\(\{(y_i, \vec{x}_i)\}_{i = 1}^N\)&lt;/span&gt; and pretending for the moment that our current guess &lt;span class=&#34;math inline&#34;&gt;\(\beta_m\)&lt;/span&gt; is correct, construct the conditional probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\{y^*_i\}\ \ |\ \ \{(y_i, \vec{x}_i)\},\ \beta_m)\)&lt;/span&gt; of the hidden data &lt;span class=&#34;math inline&#34;&gt;\(\{Y^*_i\}\)&lt;/span&gt; given all known information.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Using the distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\{y^*_i\}\ \ |\ \ \{(y_i, \vec{x}_i)\},\ \beta_m)\)&lt;/span&gt; construct the following estimator/approximation of the desired log-likelihood &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\}))\)&lt;/span&gt; for arbitrary values of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q(\beta | \beta_m) \ := \ \text{E}_{\{Y^*_i\}\ \ |\ \ \{(Y_i=y_i,\ \vec{X}_i=\vec{x}_i)\},\ \beta_m}  \big[ \log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\})) \big] 
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
= \int_{\mathcal{Y^*}} \log(p(\{y^*_i\} \ | \ \beta, \{\vec{x}_i\})) \ p(\{y^*_i\}\  |\  \{(y_i, \vec{x}_i)\}, \beta_m) \ dy_1^*...dy^*_N 
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Set &lt;span class=&#34;math inline&#34;&gt;\(\beta_{m+1}\)&lt;/span&gt; equal to a value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; that maximizes the current approximation &lt;span class=&#34;math inline&#34;&gt;\(Q(\beta|\beta_m)\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\}))\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Return to step 2 and repeat until some stopping criteria is met.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These formulas may seem difficult at the moment because they are in such a general form. As we specify things for our particular problem things will become more concrete. Now because &lt;span class=&#34;math inline&#34;&gt;\(Y^*|\vec{X}\sim \mathcal{N}(\ \langle \vec{X} , \beta\rangle \ ,\ 1)\)&lt;/span&gt; is a normal linear regression relationship we have
&lt;span class=&#34;math display&#34;&gt;\[
p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\}) = \frac{1}{(2\pi)^{N/2}}e^{\sum_{i=1}^N-\frac{1}{2}\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence
&lt;span class=&#34;math display&#34;&gt;\[
\log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\})) = const -\frac{1}{2} \sum_{i=1}^N\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and so the &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;-function is
&lt;span class=&#34;math display&#34;&gt;\[
Q(\beta | \beta_m) \ = \ \text{E}_{\{Y^*_i\}\ \ |\ \ \{(Y_i=y_i,\ \vec{X}_i=\vec{x}_i)\},\ \beta_m}  \bigg[ -\frac{1}{2} \sum_{i=1}^N\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2 \bigg] + const
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since the data samples are assumed IID we can apply the &lt;a href=&#34;https://edsterjo.netlify.app/post/003_em1/main/&#34;&gt;representation we derived in the first post in the series&lt;/a&gt; where instead of taking the expectation over all samples, we sum over the expectations of each individual sample:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q(\beta | \beta_m) \ = \ const - \frac{1}{2}\sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \bigg[\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2 \bigg]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that in the EM algorithm we do not actually need to evaluate this function. Instead in step 4 we simply want to find the value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; that maximizes it. In addition, as we discussed in the first post, if we wanted to incorporate a prior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\beta)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; for the purpose of regularization we would replace the problem of maximizing &lt;span class=&#34;math inline&#34;&gt;\(Q(\beta|\beta_m)\)&lt;/span&gt; by maximizing &lt;span class=&#34;math inline&#34;&gt;\(Q(\beta|\beta_m) + \log(p(\beta))\)&lt;/span&gt; instead. For the purpose of &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularization we could simply take &lt;span class=&#34;math inline&#34;&gt;\(\beta \sim \mathcal{N}(\vec{0}, \frac{1}{\lambda})\)&lt;/span&gt;. In that case
&lt;span class=&#34;math display&#34;&gt;\[
Q(\beta|\beta_m) + \log(p(\beta)) = \ const - \frac{1}{2}\sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \bigg[\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2 \bigg] - \frac{\lambda}{2}\langle \beta, \beta\rangle
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(the &lt;span class=&#34;math inline&#34;&gt;\(const\)&lt;/span&gt; may now depend on &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; which we always treat as a constant). We will focus on maximizing this regularized function, knowing that we can simply let &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 0\)&lt;/span&gt; to remove the regularization. At the maximizing point we need the gradient with respect to &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; to equal 0:
&lt;span class=&#34;math display&#34;&gt;\[
\nabla_{\beta} \ \big(Q(\beta|\beta_m) + \log(p(\beta))\big) = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(where the &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; represents the zero vector in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^p\)&lt;/span&gt;). Interchanging gradients first with the summation, then with the expectation (since all random variables have nice distributions) gives
&lt;span class=&#34;math display&#34;&gt;\[
0 = \sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \bigg[\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)\vec{x}_i \bigg] - \lambda \beta
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
= \sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \big[Y^*_i\big]\vec{x}_i - \sum_{i = 1}^N\langle \vec{x}_i  , \beta\rangle\vec{x}_i  - \lambda \beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X\in\mathbb{R}^{N\times p}\)&lt;/span&gt; be an &lt;span class=&#34;math inline&#34;&gt;\(N\times p\)&lt;/span&gt; matrix whose &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th row is &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}_i\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(Z\in \mathbb{R}^{N\times 1}\)&lt;/span&gt; be a vector with &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th component &lt;span class=&#34;math inline&#34;&gt;\(z_i = \text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m} \big[Y^*_i\big]\)&lt;/span&gt;. Then in matrix notation the above becomes
&lt;span class=&#34;math display&#34;&gt;\[
0 = X^TZ - X^TX\beta - \lambda\beta \\= X^TZ - (X^TX\beta + \lambda I)\beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This looks very familiar! It looks exactly like the normal equations of OLS if the target variable had been &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;. The value of &lt;span class=&#34;math inline&#34;&gt;\(Z = \big(\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m} \big[Y^*_i\big]\big)_{i=1}^N\)&lt;/span&gt; is just the value of &lt;span class=&#34;math inline&#34;&gt;\(Y^*\)&lt;/span&gt; we would guess given the values &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; of the censored variable and &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}_i\)&lt;/span&gt; of the covariates. Basically EM is telling us to impute a conditional average for the missing data &lt;span class=&#34;math inline&#34;&gt;\(Y^*\)&lt;/span&gt;, fit OLS, and repeat. Solving for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; gives
&lt;span class=&#34;math display&#34;&gt;\[
\beta_{m+1} = (X^TX + \lambda I)^{-1}X^TZ
\]&lt;/span&gt;
This can be implemented once we know the value of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(Z_i = \text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m} \big[Y^*_i\big]\)&lt;/span&gt; then this is just the mean of a truncated normal distribution:
&lt;span class=&#34;math display&#34;&gt;\[
\text{E}_{Y^*_i\ \ |\ \ Y_i=1,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \big[Y^*_i\big] = \langle \vec{x}_i  , \beta_m\rangle + \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{1 - \Phi(-\langle \vec{x}_i  , \beta_m\rangle)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{E}_{Y^*_i\ \ |\ \ Y_i=0,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \big[Y^*_i\big] = \langle \vec{x}_i  , \beta_m\rangle - \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{\Phi(-\langle \vec{x}_i  , \beta_m\rangle)}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; are the standard normal PDF and CDF respectively. Therefore, we can summarize the EM algorithm for Probit Regression as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(m = 0\)&lt;/span&gt;. Make an initial estimate &lt;span class=&#34;math inline&#34;&gt;\(\beta_m\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Impute the censored data according to&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
z_i =
\begin{cases}
\langle \vec{x}_i  , \beta_m\rangle + \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{1 - \Phi(-\langle \vec{x}_i  , \beta_m\rangle)}, &amp;amp; \text{if} \ \ y_i = 1 \\
\langle \vec{x}_i  , \beta_m\rangle - \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{\Phi(-\langle \vec{x}_i  , \beta_m\rangle)}, &amp;amp; \text{if} \ \ y_i = 0
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Solve the regularized OLS problem to update &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\beta_{m+1} = (X^TX + \lambda I)^{-1}X^TZ
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Return to step 2 and repeat until some stopping criteria is met.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;implementing-in-c-using-eigen&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implementing in C++ using Eigen&lt;/h2&gt;
&lt;p&gt;The algorithm above is easily implementable in R, Numpy, Matlab, etc., but for fun we’ll implement it in C++ using the Eigen linear algebra library. We do this in Rmarkdown using the &lt;a href=&#34;https://cran.r-project.org/web/packages/RcppEigen/index.html&#34;&gt;RcppEigen&lt;/a&gt; package in R.&lt;/p&gt;
&lt;p&gt;First we include the necessary header files. Here &lt;code&gt;RcppEigen.h&lt;/code&gt; includes the Eigen library itself, as well as all the necessary boilerplate code of Rcpp to integrate Eigen (and C++) with R.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;RcppEigen.h&amp;gt;
#include &amp;lt;algorithm&amp;gt;
#include &amp;lt;limits&amp;gt;

// [[Rcpp::depends(RcppEigen)]]

using namespace Eigen;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we define two functions that will be needed in computing the vector &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, whose computation requires the normal distribution’s PDF and CDF. We’ll use R’s own built-in functions &lt;code&gt;dnorm&lt;/code&gt; and &lt;code&gt;pnorm&lt;/code&gt;. These functions are written in C or Fortran (and hence can be called from any language with a C interface) and are very well tested. So instead of rolling our own versions we may as well use R’s.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// [[Rcpp::export]]
double positive(double mu) 
{
    double num = R::dnorm(mu, 0, 1, false);
    double den = 1 - R::pnorm(-mu, 0, 1, true, false);
    
    return mu + num/den;
}

// [[Rcpp::export]]
double negative(double mu)
{
    double num = R::dnorm(mu, 0, 1, false);
    double den = R::pnorm(-mu, 0, 1, true, false);
    
    return mu - num/den;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we want a function that actually computes the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; vector, given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_m\)&lt;/span&gt;. We could use some of Eigen’s nifty &lt;a href=&#34;https://eigen.tuxfamily.org/dox/TopicCustomizing_NullaryExpr.html&#34;&gt;nullary expressions&lt;/a&gt;, but a simple for-loop with a ternay conditional will do. We also create a wrapper function so that we can test from R:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;VectorXd impute(const MatrixXd&amp;amp; X,
                const VectorXd&amp;amp; beta,
                const VectorXi&amp;amp; Y)
{
    VectorXd Z = X * beta;
    
    // If Y(i) is non-zero use the function 
    // positive, else use negative
    for(int i = 0; i != Z.size(); ++i)
        Z(i) = (Y(i) == (int)1) ? (positive(Z(i))) : (negative(Z(i)));
    
    return Z;
}

// A wrapper function to test from R
// [[Rcpp::export]]
VectorXd impute_test(const Map&amp;lt;MatrixXd&amp;gt; X,
                     const Map&amp;lt;VectorXd&amp;gt; beta,
                     const Map&amp;lt;VectorXi&amp;gt; Y)
{
    return impute(X, beta, Y);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comparing to an R implementation is trivial:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X = matrix(rnorm(100*10), ncol = 10)
beta = matrix(rnorm(10), ncol = 1)
Y = matrix(sample(c(0L,1L), size = 100, replace = T), ncol = 1)

# Using the C++ implementation
Z.cpp = impute_test(X, beta, Y)

print(anyNA(Z.cpp))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Building one in R
imputeR = function(X, beta, Y)
{
    positiveR = function(Z){Z + dnorm(Z)/(1-pnorm(-Z))}
    negativeR = function(Z){Z - dnorm(Z)/pnorm(-Z)}
    
    Z = X %*% beta
    return(ifelse((Y == 1L), positiveR(Z), negativeR(Z)))
}

Z.r = imputeR(X, beta, Y)

# Checking range of values
print(summary(Z.cpp - Z.r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        V1            
##  Min.   :-7.105e-15  
##  1st Qu.:-1.110e-16  
##  Median : 0.000e+00  
##  Mean   : 1.577e-16  
##  3rd Qu.: 2.776e-17  
##  Max.   : 2.931e-14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not bad. Next we need a function to compute (regularized) least squares. For the solution we use the normal equations. According to the &lt;a href=&#34;https://eigen.tuxfamily.org/dox-devel/group__LeastSquares.html&#34;&gt;Eigen tutorial on the matter&lt;/a&gt; the normal equations are the fastest but least numerically stable option. For us this is good enough. Again we create a small wrapper to test from R:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;VectorXd RLS(const VectorXd&amp;amp; Z, 
             const MatrixXd&amp;amp; X, 
             const double lambda = 0.0)
{
    if(lambda &amp;gt;= std::numeric_limits&amp;lt;double&amp;gt;::min())
    {
        // Creating an identity matrix
        MatrixXd lambda_eye = lambda * MatrixXd::Identity(X.cols(), X.cols());
        
        // Using (regularized) normal equations
        return (X.transpose() * X + lambda_eye).ldlt().solve(X.transpose() * Z);
    }
    
    return (X.transpose() * X).ldlt().solve(X.transpose() * Z);
}

// [[Rcpp::export]]
VectorXd RLS_test(const Map&amp;lt;VectorXd&amp;gt; Z, 
                  const Map&amp;lt;MatrixXd&amp;gt; X,
                  double lambda = 0.0)
{
    return RLS(Z, X, lambda);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Testing in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
X = matrix(rnorm(100*10), ncol = 10)
beta = matrix(rnorm(10), ncol = 1)
Z = X %*% beta + matrix(rnorm(100), ncol = 1)

beta_hat.r = lm.fit(X, Z, method = &amp;quot;qr&amp;quot;)$coefficients
beta_hat.cpp = RLS_test(Z, X, 0)

print(data.frame(beta = beta, 
                 beta_hat.r = beta_hat.r, 
                 beta_hat.cpp = beta_hat.cpp))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           beta beta_hat.r beta_hat.cpp
## x1  -1.2053334 -1.2721727   -1.2721727
## x2   0.3014667  0.2774444    0.2774444
## x3  -1.5391452 -1.5961346   -1.5961346
## x4   0.6353707  0.5781086    0.5781086
## x5   0.7029518  0.8563414    0.8563414
## x6  -1.9058829 -2.0154512   -2.0154512
## x7   0.9389214  0.8108957    0.8108957
## x8  -0.2244921 -0.3825277   -0.3825277
## x9  -0.6738168 -0.7562696   -0.7562696
## x10  0.4457874  0.4209543    0.4209543&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(beta_hat.cpp - beta_hat.r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -8.882e-16 -6.800e-16 -5.551e-17  4.996e-17  3.053e-16  2.220e-15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not bad at all. Note that R’s very powerful &lt;code&gt;lm.fit&lt;/code&gt; function uses QR decomposition to solve the least squares problem. This method is a bit slower in principle than the &lt;span class=&#34;math inline&#34;&gt;\(LDL^T\)&lt;/span&gt; decomposition we used for the normal equations above, but it’s also higher quality numerically.&lt;/p&gt;
&lt;p&gt;As for a test with a non-trivial regularization constant:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lambda = 15
beta_hat_reg.r = solve(t(X) %*% X + lambda*diag(10), t(X) %*% Z)
beta_hat_reg.cpp = RLS_test(Z, X, lambda)

print(data.frame(beta = beta, 
                 beta_hat_reg.r = beta_hat_reg.r, 
                 beta_hat_reg.cpp = beta_hat_reg.cpp))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          beta beta_hat_reg.r beta_hat_reg.cpp
## 1  -1.2053334     -1.1372786       -1.1372786
## 2   0.3014667      0.2299293        0.2299293
## 3  -1.5391452     -1.3128364       -1.3128364
## 4   0.6353707      0.4366746        0.4366746
## 5   0.7029518      0.7691213        0.7691213
## 6  -1.9058829     -1.7122424       -1.7122424
## 7   0.9389214      0.7415632        0.7415632
## 8  -0.2244921     -0.2756274       -0.2756274
## 9  -0.6738168     -0.5224541       -0.5224541
## 10  0.4457874      0.4061769        0.4061769&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(summary(beta_hat_reg.cpp - beta_hat_reg.r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        V1            
##  Min.   :-7.772e-16  
##  1st Qu.:-1.110e-16  
##  Median : 0.000e+00  
##  Mean   :-2.776e-17  
##  3rd Qu.: 2.082e-16  
##  Max.   : 3.331e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Very good. Again, we see that the regularized least squares estimates are biased away from the true values, and towards the 0 vector.&lt;/p&gt;
&lt;p&gt;Now we bring it all together into one algorithm:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// [[Rcpp::export]]
VectorXd Probit(const Map&amp;lt;VectorXi&amp;gt; Y, 
                const Map&amp;lt;MatrixXd&amp;gt; X, 
                double lambda = 0.0,
                int num_iter = 100)
{

    // Making sure Lambda is non-negative;
    lambda = std::max(lambda, 0.0);
    
    // Making sure the number of rows of X is the 
    // same as the number of rows of Y
    assert(Y.size() == X.rows());
    
    // Initialize beta to 0 values
    VectorXd beta = VectorXd::Zero(X.cols());
    
    // Iteration
    for(int i = 0; i &amp;lt; num_iter; ++i)
    {
        // Impute the Z vector
        VectorXd Z = impute(X, beta, Y);
        
        // Solve (regularized) Least Squares 
        beta = RLS(Z, X, lambda);
    }
    
    return beta;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we carry out a comparison of the base R implementation of (unregularized) probit regression against our implementation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)

N = 10000
p = 4

set.seed(1234)
S = matrix(rnorm(p*p), ncol = p)
S = t(S) %*% S

X = MASS::mvrnorm(n = N, mu = rep(0.0, times = p), Sigma = S)

beta = matrix((1:p)/2, ncol = 1)

Z = X %*% beta + matrix(rnorm(N))
Y = as.integer(Z &amp;gt; 0)

system.time(probit.cpp100 &amp;lt;- Probit(Y, X, 0.0, 100))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##    0.21    0.00    0.20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(probit.cpp10000 &amp;lt;- Probit(Y, X, 0.0, 10000))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   21.33    0.00   21.33&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(probit.glm &amp;lt;- glm(Y ~ X - 1, family = binomial(link = &amp;quot;probit&amp;quot;))$coefficients)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##    0.07    0.00    0.06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(data.frame(beta = beta, 
                 Cpp_100_iter = probit.cpp100,
                 Cpp_10000_iter = probit.cpp10000,
                 R = probit.glm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    beta Cpp_100_iter Cpp_10000_iter         R
## X1  0.5    0.4128896      0.5002905 0.5002889
## X2  1.0    0.8632397      0.9845270 0.9845285
## X3  1.5    1.2932036      1.5056988 1.5056998
## X4  2.0    1.7173029      1.9814520 1.9814547&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see the base R converges much more quickly than our implementation of EM. Note, this is not due to weak compiler flags on our part. A local Makevars file in &lt;code&gt;Documents/.R&lt;/code&gt; overrides the default R build flags to use the optimizations &lt;code&gt;-O3 -march=native&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;No, instead &lt;a href=&#34;https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm&#34;&gt;&lt;code&gt;glm.fit&lt;/code&gt;&lt;/a&gt; uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares&#34;&gt;iteratively reweighted least squares (IRLS)&lt;/a&gt; to fit the model, not EM. This can be seen in the &lt;a href=&#34;https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/glm.R&#34;&gt;source code&lt;/a&gt;. So EM is not a very fast algorithm to fit probit models. At some point in the future we’ll probably implement IRLS in Eigen (or Fortran, or Julia).&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In the <a href="https://edsterjo.netlify.app/post/003_em1/main/">first post</a> in this series we discussed Expectation Maximization (EM) type algorithms. In the post <a href="https://edsterjo.netlify.app/post/004_regularization/main/">prior to this one</a> we discussed regularization and showed how it leads to a bias-variance trade off in OLS models. Here we implement fitting for <span class="math inline">\(L^2\)</span>-regularized <a href="https://en.wikipedia.org/wiki/Probit_model">probit regression</a> using EM. To make it more interesting we will code everything from scratch using the <a href="http://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a> linear algebra library, via <a href="https://cran.r-project.org/web/packages/RcppEigen/index.html">RcppEigen</a>.</p>
</div>
<div id="probit-regression-as-a-censored-ols-model" class="section level2">
<h2>Probit Regression as a censored OLS model</h2>
<p>In our first post on EM algorithms we emphasized that EM is particularly useful for models that have censored data. Suppose we have the following censored model. Suppose <span class="math inline">\(Y^*\)</span> is a real valued random variable and <span class="math inline">\(\vec{X}\)</span> is a random vector with values in <span class="math inline">\(\mathbb{R}^p\)</span>. Suppose that we have the conditional relationship</p>
<p><span class="math display">\[
Y^* \ \ | \ \ \vec{X} \sim \mathcal{N}(\ \langle \vec{X}  , \beta\rangle \ ,\ 1)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span> denotes the univariate normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Here (and everywhere else) the symbol <span class="math inline">\(\langle v,w\rangle\)</span> represents the Euclidean inner product (aka, <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a>) of two vectors <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span>.</p>
<p>In this case we may write <span class="math inline">\(Y^* = \langle \vec{X} , \beta\rangle - \epsilon\)</span>, where <span class="math inline">\(\epsilon \sim \mathcal{N}(0,1)\)</span> is standard normal, which can be taken independent of <span class="math inline">\(\vec{X}\)</span> or this distribution can be simply assumed conditional on <span class="math inline">\(\vec{X}\)</span>.</p>
<p>Instead of observing <span class="math inline">\(Y^*\)</span> in the data however, we observe the censored variable
<span class="math display">\[
Y := 
\begin{cases}
1, &amp; \text{if} \ \ Y^* &gt; 0 \\
0, &amp; \text{otherwise}
\end{cases}
\]</span>
Hence we have that <span class="math inline">\(Y \ |\vec{X} \sim \text{Bernoulli}(p)\)</span> where
<span class="math display">\[
p = P(Y = 1 | \vec{X}) = P(Y^* &gt; 0 | \vec{X}) = P\bigg(Y^* - \langle \vec{X},\beta\rangle &gt; - \langle \vec{X},\beta\rangle\ \ \bigg|\ \ \vec{X}\bigg) = P(\epsilon &lt; \langle \vec{X},\beta\rangle\ | \vec{X})
\]</span>
Since <span class="math inline">\(\epsilon \sim \mathcal{N}(0,1)\)</span> this last probability is equal to <span class="math inline">\(\Phi(\langle \vec{X},\beta\rangle)\)</span> where <span class="math inline">\(\Phi\)</span> denotes the standard normal CDF. This derives the probit model.</p>
<p>Before we proceed, notice 2 points which we won’t dwell on:</p>
<ul>
<li>If the variance of <span class="math inline">\(Y^*\)</span> had been <span class="math inline">\(\sigma^2 \ne 1\)</span> then the value of <span class="math inline">\(\sigma\)</span> would not be able to be estimated from data without knowing <span class="math inline">\(\beta\)</span> since <span class="math inline">\(P(Y^* &gt; 0 |\vec{X}) = P(Y^*/\sigma &gt; 0 | \vec{X})\)</span>.</li>
<li>If the distributional relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(\vec{X}\)</span> had been such that the error term <span class="math inline">\(\epsilon\)</span> where a logistic random variable, instead of a normal one, then the censored problem would have become logistic regression.</li>
</ul>
<p>For the regression itself we assume that we have a data set <span class="math inline">\(\{ (y_i, \vec{x}_i)\}_{i = 1}^N\)</span> consisting of samples generated independently of one another from a fixed multivariate distribution for <span class="math inline">\((Y, \vec{X})\)</span> (i.e. we assume our data was sampled IID).</p>
</div>
<div id="fitting-probit-regression-via-em" class="section level2">
<h2>Fitting Probit Regression via EM</h2>
<p>Since probit regression arises from a censored normal OLS model, and since OLS is relatively easy to fit, probit regression is an excellent candidate for applying Expectation Maximization for fitting. A small difference will be that all of the probability densities involved will be conditional on the observed covariates <span class="math inline">\(\{\vec{x}_i\}_{i = 1}^N\)</span> since regression is a conditional relationship.</p>
<p>Let’s recall the standard EM algorithm for the case of the regression problem above:</p>
<ul>
<li><p><strong>Step 1:</strong> Let <span class="math inline">\(m = 0\)</span>. Make an initial estimate <span class="math inline">\(\beta_m\)</span> for <span class="math inline">\(\beta\)</span>.</p></li>
<li><p><strong>Step 2:</strong> Given the observed data <span class="math inline">\(\{(y_i, \vec{x}_i)\}_{i = 1}^N\)</span> and pretending for the moment that our current guess <span class="math inline">\(\beta_m\)</span> is correct, construct the conditional probability distribution <span class="math inline">\(p(\{y^*_i\}\ \ |\ \ \{(y_i, \vec{x}_i)\},\ \beta_m)\)</span> of the hidden data <span class="math inline">\(\{Y^*_i\}\)</span> given all known information.</p></li>
<li><p><strong>Step 3:</strong> Using the distribution <span class="math inline">\(p(\{y^*_i\}\ \ |\ \ \{(y_i, \vec{x}_i)\},\ \beta_m)\)</span> construct the following estimator/approximation of the desired log-likelihood <span class="math inline">\(\log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\}))\)</span> for arbitrary values of <span class="math inline">\(\beta\)</span>:</p></li>
</ul>
<p><span class="math display">\[
Q(\beta | \beta_m) \ := \ \text{E}_{\{Y^*_i\}\ \ |\ \ \{(Y_i=y_i,\ \vec{X}_i=\vec{x}_i)\},\ \beta_m}  \big[ \log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\})) \big] 
\]</span>
<span class="math display">\[
= \int_{\mathcal{Y^*}} \log(p(\{y^*_i\} \ | \ \beta, \{\vec{x}_i\})) \ p(\{y^*_i\}\  |\  \{(y_i, \vec{x}_i)\}, \beta_m) \ dy_1^*...dy^*_N 
\]</span></p>
<ul>
<li><p><strong>Step 4:</strong> Set <span class="math inline">\(\beta_{m+1}\)</span> equal to a value of <span class="math inline">\(\beta\)</span> that maximizes the current approximation <span class="math inline">\(Q(\beta|\beta_m)\)</span> of <span class="math inline">\(\log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\}))\)</span>.</p></li>
<li><p><strong>Step 5:</strong> Return to step 2 and repeat until some stopping criteria is met.</p></li>
</ul>
<p>These formulas may seem difficult at the moment because they are in such a general form. As we specify things for our particular problem things will become more concrete. Now because <span class="math inline">\(Y^*|\vec{X}\sim \mathcal{N}(\ \langle \vec{X} , \beta\rangle \ ,\ 1)\)</span> is a normal linear regression relationship we have
<span class="math display">\[
p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\}) = \frac{1}{(2\pi)^{N/2}}e^{\sum_{i=1}^N-\frac{1}{2}\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2}
\]</span></p>
<p>Hence
<span class="math display">\[
\log(p(\{Y^*_i\} \ | \ \beta, \{\vec{x}_i\})) = const -\frac{1}{2} \sum_{i=1}^N\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2
\]</span></p>
<p>and so the <span class="math inline">\(Q\)</span>-function is
<span class="math display">\[
Q(\beta | \beta_m) \ = \ \text{E}_{\{Y^*_i\}\ \ |\ \ \{(Y_i=y_i,\ \vec{X}_i=\vec{x}_i)\},\ \beta_m}  \bigg[ -\frac{1}{2} \sum_{i=1}^N\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2 \bigg] + const
\]</span></p>
<p>Since the data samples are assumed IID we can apply the <a href="https://edsterjo.netlify.app/post/003_em1/main/">representation we derived in the first post in the series</a> where instead of taking the expectation over all samples, we sum over the expectations of each individual sample:</p>
<p><span class="math display">\[
Q(\beta | \beta_m) \ = \ const - \frac{1}{2}\sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \bigg[\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2 \bigg]
\]</span></p>
<p>Note that in the EM algorithm we do not actually need to evaluate this function. Instead in step 4 we simply want to find the value of <span class="math inline">\(\beta\)</span> that maximizes it. In addition, as we discussed in the first post, if we wanted to incorporate a prior distribution <span class="math inline">\(p(\beta)\)</span> on <span class="math inline">\(\beta\)</span> for the purpose of regularization we would replace the problem of maximizing <span class="math inline">\(Q(\beta|\beta_m)\)</span> by maximizing <span class="math inline">\(Q(\beta|\beta_m) + \log(p(\beta))\)</span> instead. For the purpose of <span class="math inline">\(L^2\)</span>-regularization we could simply take <span class="math inline">\(\beta \sim \mathcal{N}(\vec{0}, \frac{1}{\lambda})\)</span>. In that case
<span class="math display">\[
Q(\beta|\beta_m) + \log(p(\beta)) = \ const - \frac{1}{2}\sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \bigg[\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)^2 \bigg] - \frac{\lambda}{2}\langle \beta, \beta\rangle
\]</span></p>
<p>(the <span class="math inline">\(const\)</span> may now depend on <span class="math inline">\(\lambda\)</span> which we always treat as a constant). We will focus on maximizing this regularized function, knowing that we can simply let <span class="math inline">\(\lambda = 0\)</span> to remove the regularization. At the maximizing point we need the gradient with respect to <span class="math inline">\(\beta\)</span> to equal 0:
<span class="math display">\[
\nabla_{\beta} \ \big(Q(\beta|\beta_m) + \log(p(\beta))\big) = 0
\]</span></p>
<p>(where the <span class="math inline">\(0\)</span> represents the zero vector in <span class="math inline">\(\mathbb{R}^p\)</span>). Interchanging gradients first with the summation, then with the expectation (since all random variables have nice distributions) gives
<span class="math display">\[
0 = \sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \bigg[\big(Y^*_i - \langle \vec{x}_i  , \beta\rangle\big)\vec{x}_i \bigg] - \lambda \beta
\]</span>
<span class="math display">\[
= \sum_{i = 1}^N\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \big[Y^*_i\big]\vec{x}_i - \sum_{i = 1}^N\langle \vec{x}_i  , \beta\rangle\vec{x}_i  - \lambda \beta
\]</span></p>
<p>Let <span class="math inline">\(X\in\mathbb{R}^{N\times p}\)</span> be an <span class="math inline">\(N\times p\)</span> matrix whose <span class="math inline">\(i\)</span>-th row is <span class="math inline">\(\vec{x}_i\)</span>, and let <span class="math inline">\(Z\in \mathbb{R}^{N\times 1}\)</span> be a vector with <span class="math inline">\(i\)</span>-th component <span class="math inline">\(z_i = \text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m} \big[Y^*_i\big]\)</span>. Then in matrix notation the above becomes
<span class="math display">\[
0 = X^TZ - X^TX\beta - \lambda\beta \\= X^TZ - (X^TX\beta + \lambda I)\beta
\]</span></p>
<p>This looks very familiar! It looks exactly like the normal equations of OLS if the target variable had been <span class="math inline">\(Z\)</span>. The value of <span class="math inline">\(Z = \big(\text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m} \big[Y^*_i\big]\big)_{i=1}^N\)</span> is just the value of <span class="math inline">\(Y^*\)</span> we would guess given the values <span class="math inline">\(y_i\)</span> of the censored variable and <span class="math inline">\(\vec{x}_i\)</span> of the covariates. Basically EM is telling us to impute a conditional average for the missing data <span class="math inline">\(Y^*\)</span>, fit OLS, and repeat. Solving for <span class="math inline">\(\beta\)</span> gives
<span class="math display">\[
\beta_{m+1} = (X^TX + \lambda I)^{-1}X^TZ
\]</span>
This can be implemented once we know the value of <span class="math inline">\(Z\)</span>. Since <span class="math inline">\(Z_i = \text{E}_{Y^*_i\ \ |\ \ Y_i=y_i,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m} \big[Y^*_i\big]\)</span> then this is just the mean of a truncated normal distribution:
<span class="math display">\[
\text{E}_{Y^*_i\ \ |\ \ Y_i=1,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \big[Y^*_i\big] = \langle \vec{x}_i  , \beta_m\rangle + \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{1 - \Phi(-\langle \vec{x}_i  , \beta_m\rangle)}
\]</span></p>
<p><span class="math display">\[
\text{E}_{Y^*_i\ \ |\ \ Y_i=0,\ \ \vec{X}_i=\vec{x}_i,\ \ \beta_m}  \big[Y^*_i\big] = \langle \vec{x}_i  , \beta_m\rangle - \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{\Phi(-\langle \vec{x}_i  , \beta_m\rangle)}
\]</span>
where <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\Phi\)</span> are the standard normal PDF and CDF respectively. Therefore, we can summarize the EM algorithm for Probit Regression as:</p>
<ul>
<li><strong>Step 1:</strong> Let <span class="math inline">\(m = 0\)</span>. Make an initial estimate <span class="math inline">\(\beta_m\)</span> for <span class="math inline">\(\beta\)</span>.</li>
<li><strong>Step 2:</strong> Impute the censored data according to</li>
</ul>
<p><span class="math display">\[
z_i =
\begin{cases}
\langle \vec{x}_i  , \beta_m\rangle + \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{1 - \Phi(-\langle \vec{x}_i  , \beta_m\rangle)}, &amp; \text{if} \ \ y_i = 1 \\
\langle \vec{x}_i  , \beta_m\rangle - \frac{\phi(\langle \vec{x}_i  , \beta_m\rangle)}{\Phi(-\langle \vec{x}_i  , \beta_m\rangle)}, &amp; \text{if} \ \ y_i = 0
\end{cases}
\]</span></p>
<ul>
<li><strong>Step 3:</strong> Solve the regularized OLS problem to update <span class="math inline">\(\beta\)</span>:</li>
</ul>
<p><span class="math display">\[
\beta_{m+1} = (X^TX + \lambda I)^{-1}X^TZ
\]</span></p>
<ul>
<li><strong>Step 4:</strong> Return to step 2 and repeat until some stopping criteria is met.</li>
</ul>
</div>
<div id="implementing-in-c-using-eigen" class="section level2">
<h2>Implementing in C++ using Eigen</h2>
<p>The algorithm above is easily implementable in R, Numpy, Matlab, etc., but for fun we’ll implement it in C++ using the Eigen linear algebra library. We do this in Rmarkdown using the <a href="https://cran.r-project.org/web/packages/RcppEigen/index.html">RcppEigen</a> package in R.</p>
<p>First we include the necessary header files. Here <code>RcppEigen.h</code> includes the Eigen library itself, as well as all the necessary boilerplate code of Rcpp to integrate Eigen (and C++) with R.</p>
<pre class="cpp"><code>#include &lt;RcppEigen.h&gt;
#include &lt;algorithm&gt;
#include &lt;limits&gt;

// [[Rcpp::depends(RcppEigen)]]

using namespace Eigen;</code></pre>
<p>Next we define two functions that will be needed in computing the vector <span class="math inline">\(Z\)</span>, whose computation requires the normal distribution’s PDF and CDF. We’ll use R’s own built-in functions <code>dnorm</code> and <code>pnorm</code>. These functions are written in C or Fortran (and hence can be called from any language with a C interface) and are very well tested. So instead of rolling our own versions we may as well use R’s.</p>
<pre class="cpp"><code>// [[Rcpp::export]]
double positive(double mu) 
{
    double num = R::dnorm(mu, 0, 1, false);
    double den = 1 - R::pnorm(-mu, 0, 1, true, false);
    
    return mu + num/den;
}

// [[Rcpp::export]]
double negative(double mu)
{
    double num = R::dnorm(mu, 0, 1, false);
    double den = R::pnorm(-mu, 0, 1, true, false);
    
    return mu - num/den;
}</code></pre>
<p>Next we want a function that actually computes the <span class="math inline">\(Z\)</span> vector, given <span class="math inline">\(X\)</span> and <span class="math inline">\(\beta_m\)</span>. We could use some of Eigen’s nifty <a href="https://eigen.tuxfamily.org/dox/TopicCustomizing_NullaryExpr.html">nullary expressions</a>, but a simple for-loop with a ternay conditional will do. We also create a wrapper function so that we can test from R:</p>
<pre class="cpp"><code>VectorXd impute(const MatrixXd&amp; X,
                const VectorXd&amp; beta,
                const VectorXi&amp; Y)
{
    VectorXd Z = X * beta;
    
    // If Y(i) is non-zero use the function 
    // positive, else use negative
    for(int i = 0; i != Z.size(); ++i)
        Z(i) = (Y(i) == (int)1) ? (positive(Z(i))) : (negative(Z(i)));
    
    return Z;
}

// A wrapper function to test from R
// [[Rcpp::export]]
VectorXd impute_test(const Map&lt;MatrixXd&gt; X,
                     const Map&lt;VectorXd&gt; beta,
                     const Map&lt;VectorXi&gt; Y)
{
    return impute(X, beta, Y);
}</code></pre>
<p>Comparing to an R implementation is trivial:</p>
<pre class="r"><code>X = matrix(rnorm(100*10), ncol = 10)
beta = matrix(rnorm(10), ncol = 1)
Y = matrix(sample(c(0L,1L), size = 100, replace = T), ncol = 1)

# Using the C++ implementation
Z.cpp = impute_test(X, beta, Y)

print(anyNA(Z.cpp))</code></pre>
<pre><code>## [1] FALSE</code></pre>
<pre class="r"><code># Building one in R
imputeR = function(X, beta, Y)
{
    positiveR = function(Z){Z + dnorm(Z)/(1-pnorm(-Z))}
    negativeR = function(Z){Z - dnorm(Z)/pnorm(-Z)}
    
    Z = X %*% beta
    return(ifelse((Y == 1L), positiveR(Z), negativeR(Z)))
}

Z.r = imputeR(X, beta, Y)

# Checking range of values
print(summary(Z.cpp - Z.r))</code></pre>
<pre><code>##        V1            
##  Min.   :-7.105e-15  
##  1st Qu.:-1.110e-16  
##  Median : 0.000e+00  
##  Mean   : 1.577e-16  
##  3rd Qu.: 2.776e-17  
##  Max.   : 2.931e-14</code></pre>
<p>Not bad. Next we need a function to compute (regularized) least squares. For the solution we use the normal equations. According to the <a href="https://eigen.tuxfamily.org/dox-devel/group__LeastSquares.html">Eigen tutorial on the matter</a> the normal equations are the fastest but least numerically stable option. For us this is good enough. Again we create a small wrapper to test from R:</p>
<pre class="cpp"><code>VectorXd RLS(const VectorXd&amp; Z, 
             const MatrixXd&amp; X, 
             const double lambda = 0.0)
{
    if(lambda &gt;= std::numeric_limits&lt;double&gt;::min())
    {
        // Creating an identity matrix
        MatrixXd lambda_eye = lambda * MatrixXd::Identity(X.cols(), X.cols());
        
        // Using (regularized) normal equations
        return (X.transpose() * X + lambda_eye).ldlt().solve(X.transpose() * Z);
    }
    
    return (X.transpose() * X).ldlt().solve(X.transpose() * Z);
}

// [[Rcpp::export]]
VectorXd RLS_test(const Map&lt;VectorXd&gt; Z, 
                  const Map&lt;MatrixXd&gt; X,
                  double lambda = 0.0)
{
    return RLS(Z, X, lambda);
}</code></pre>
<p>Testing in R:</p>
<pre class="r"><code>set.seed(1234)
X = matrix(rnorm(100*10), ncol = 10)
beta = matrix(rnorm(10), ncol = 1)
Z = X %*% beta + matrix(rnorm(100), ncol = 1)

beta_hat.r = lm.fit(X, Z, method = &quot;qr&quot;)$coefficients
beta_hat.cpp = RLS_test(Z, X, 0)

print(data.frame(beta = beta, 
                 beta_hat.r = beta_hat.r, 
                 beta_hat.cpp = beta_hat.cpp))</code></pre>
<pre><code>##           beta beta_hat.r beta_hat.cpp
## x1  -1.2053334 -1.2721727   -1.2721727
## x2   0.3014667  0.2774444    0.2774444
## x3  -1.5391452 -1.5961346   -1.5961346
## x4   0.6353707  0.5781086    0.5781086
## x5   0.7029518  0.8563414    0.8563414
## x6  -1.9058829 -2.0154512   -2.0154512
## x7   0.9389214  0.8108957    0.8108957
## x8  -0.2244921 -0.3825277   -0.3825277
## x9  -0.6738168 -0.7562696   -0.7562696
## x10  0.4457874  0.4209543    0.4209543</code></pre>
<pre class="r"><code>print(summary(beta_hat.cpp - beta_hat.r))</code></pre>
<pre><code>##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -8.882e-16 -6.800e-16 -5.551e-17  4.996e-17  3.053e-16  2.220e-15</code></pre>
<p>Not bad at all. Note that R’s very powerful <code>lm.fit</code> function uses QR decomposition to solve the least squares problem. This method is a bit slower in principle than the <span class="math inline">\(LDL^T\)</span> decomposition we used for the normal equations above, but it’s also higher quality numerically.</p>
<p>As for a test with a non-trivial regularization constant:</p>
<pre class="r"><code>lambda = 15
beta_hat_reg.r = solve(t(X) %*% X + lambda*diag(10), t(X) %*% Z)
beta_hat_reg.cpp = RLS_test(Z, X, lambda)

print(data.frame(beta = beta, 
                 beta_hat_reg.r = beta_hat_reg.r, 
                 beta_hat_reg.cpp = beta_hat_reg.cpp))</code></pre>
<pre><code>##          beta beta_hat_reg.r beta_hat_reg.cpp
## 1  -1.2053334     -1.1372786       -1.1372786
## 2   0.3014667      0.2299293        0.2299293
## 3  -1.5391452     -1.3128364       -1.3128364
## 4   0.6353707      0.4366746        0.4366746
## 5   0.7029518      0.7691213        0.7691213
## 6  -1.9058829     -1.7122424       -1.7122424
## 7   0.9389214      0.7415632        0.7415632
## 8  -0.2244921     -0.2756274       -0.2756274
## 9  -0.6738168     -0.5224541       -0.5224541
## 10  0.4457874      0.4061769        0.4061769</code></pre>
<pre class="r"><code>print(summary(beta_hat_reg.cpp - beta_hat_reg.r))</code></pre>
<pre><code>##        V1            
##  Min.   :-7.772e-16  
##  1st Qu.:-1.110e-16  
##  Median : 0.000e+00  
##  Mean   :-2.776e-17  
##  3rd Qu.: 2.082e-16  
##  Max.   : 3.331e-16</code></pre>
<p>Very good. Again, we see that the regularized least squares estimates are biased away from the true values, and towards the 0 vector.</p>
<p>Now we bring it all together into one algorithm:</p>
<pre class="cpp"><code>// [[Rcpp::export]]
VectorXd Probit(const Map&lt;VectorXi&gt; Y, 
                const Map&lt;MatrixXd&gt; X, 
                double lambda = 0.0,
                int num_iter = 100)
{

    // Making sure Lambda is non-negative;
    lambda = std::max(lambda, 0.0);
    
    // Making sure the number of rows of X is the 
    // same as the number of rows of Y
    assert(Y.size() == X.rows());
    
    // Initialize beta to 0 values
    VectorXd beta = VectorXd::Zero(X.cols());
    
    // Iteration
    for(int i = 0; i &lt; num_iter; ++i)
    {
        // Impute the Z vector
        VectorXd Z = impute(X, beta, Y);
        
        // Solve (regularized) Least Squares 
        beta = RLS(Z, X, lambda);
    }
    
    return beta;
}</code></pre>
<p>Below we carry out a comparison of the base R implementation of (unregularized) probit regression against our implementation.</p>
<pre class="r"><code>library(MASS)

N = 10000
p = 4

set.seed(1234)
S = matrix(rnorm(p*p), ncol = p)
S = t(S) %*% S

X = MASS::mvrnorm(n = N, mu = rep(0.0, times = p), Sigma = S)

beta = matrix((1:p)/2, ncol = 1)

Z = X %*% beta + matrix(rnorm(N))
Y = as.integer(Z &gt; 0)

system.time(probit.cpp100 &lt;- Probit(Y, X, 0.0, 100))</code></pre>
<pre><code>##    user  system elapsed 
##    0.21    0.00    0.20</code></pre>
<pre class="r"><code>system.time(probit.cpp10000 &lt;- Probit(Y, X, 0.0, 10000))</code></pre>
<pre><code>##    user  system elapsed 
##   21.33    0.00   21.33</code></pre>
<pre class="r"><code>system.time(probit.glm &lt;- glm(Y ~ X - 1, family = binomial(link = &quot;probit&quot;))$coefficients)</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>##    user  system elapsed 
##    0.07    0.00    0.06</code></pre>
<pre class="r"><code>print(data.frame(beta = beta, 
                 Cpp_100_iter = probit.cpp100,
                 Cpp_10000_iter = probit.cpp10000,
                 R = probit.glm))</code></pre>
<pre><code>##    beta Cpp_100_iter Cpp_10000_iter         R
## X1  0.5    0.4128896      0.5002905 0.5002889
## X2  1.0    0.8632397      0.9845270 0.9845285
## X3  1.5    1.2932036      1.5056988 1.5056998
## X4  2.0    1.7173029      1.9814520 1.9814547</code></pre>
<p>We see the base R converges much more quickly than our implementation of EM. Note, this is not due to weak compiler flags on our part. A local Makevars file in <code>Documents/.R</code> overrides the default R build flags to use the optimizations <code>-O3 -march=native</code>.</p>
<p>No, instead <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm"><code>glm.fit</code></a> uses <a href="https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares">iteratively reweighted least squares (IRLS)</a> to fit the model, not EM. This can be seen in the <a href="https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/glm.R">source code</a>. So EM is not a very fast algorithm to fit probit models. At some point in the future we’ll probably implement IRLS in Eigen (or Fortran, or Julia).</p>
</div>
</content:encoded>	
    </item>
    
    <item>
      <title>In Machine Learning, why is Regularization called Regularization?</title>
      <link>/post/004_regularization/main/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/004_regularization/main/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Many newcomers to machine learning know about regularization, but they may not understand it yet. In particular, they may not know why regularization has that name. In this post we discuss the numerical and statistical significance of regularization methods in machine learning and more general statistical models. We’ll try to introduce why one may want to use regularization methods in the first place and how to interpret the fitted model from a statistical point of view.&lt;/p&gt;
&lt;p&gt;The post will be long because there are a lot of cute nooks and crannies, and we’ll assume you know your linear algebra. However, if you already know what an inner product is then we think this post will be worth your time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrices-and-linear-ill-posed-problems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matrices and Linear Ill-Posed Problems&lt;/h2&gt;
&lt;p&gt;Suppose that we have a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\in \mathbb{R}^{N \times p}\)&lt;/span&gt;, a vector &lt;span class=&#34;math inline&#34;&gt;\(y\in \mathbb{R}^N\)&lt;/span&gt;, and that we seek a vector &lt;span class=&#34;math inline&#34;&gt;\(\beta\in\mathbb{R}^p\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A\beta = y\)&lt;/span&gt;. How would one solve this problem? One answer might be to simply apply the inverse matrix to both sides of the equation: &lt;span class=&#34;math inline&#34;&gt;\(\beta = A^{-1}y\)&lt;/span&gt;. However, there are three problems with this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A matrix inverse &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; may not exist.&lt;/li&gt;
&lt;li&gt;Even if the matrix inverse exists it can be extremely expensive to calculate this inverse and apply the result to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Even if we are somehow able to calculate &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}y\)&lt;/span&gt; the solution may not be very stable. Small numerical changes in either &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; may lead to large changes in the solution &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Issue 2 above is not really a problem in the sense that &lt;a href=&#34;https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/&#34;&gt;one should never really need to find &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; to compute &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}y\)&lt;/span&gt;&lt;/a&gt;. Instead the most efficient numerical algorithms typically compute &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}y\)&lt;/span&gt; by using special factorizations of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/QR_decomposition&#34;&gt;QR decomposition&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The other two issues are very important and are inextricably linked to each other and to regularization in machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Issue 3 means just what is says: that the solution &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; may change a lot if the known data &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and/or &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; only change a little.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Issue 1 means that the matrix is not invertible. A matrix that is not invertible is called &lt;strong&gt;&lt;em&gt;singular&lt;/em&gt;&lt;/strong&gt;. A matrix that is invertible is usually called &lt;strong&gt;&lt;em&gt;nonsingular&lt;/em&gt;&lt;/strong&gt;, but a less common synonym is &lt;a href=&#34;https://mathworld.wolfram.com/RegularMatrix.html&#34;&gt;&lt;strong&gt;&lt;em&gt;regular&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; and this is where the name &lt;em&gt;regularization&lt;/em&gt; comes from. When a matrix is singular it means that the problem &lt;span class=&#34;math inline&#34;&gt;\(A\beta = y\)&lt;/span&gt; may have either no solution at all or have at least 2 distinct solutions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Any one of these issues being true means that the linear problem &lt;span class=&#34;math inline&#34;&gt;\(A\beta = y\)&lt;/span&gt; is ill-posed in the sense that it violates Hadamard’s conditions for a &lt;a href=&#34;https://en.wikipedia.org/wiki/Well-posed_problem&#34;&gt;well-posed problem&lt;/a&gt;. To be well-posed:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A problem must have a solution&lt;/li&gt;
&lt;li&gt;The solution must be unique&lt;/li&gt;
&lt;li&gt;The solution’s behavior must be stable/continuous with respect to the data&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These conditions are of extreme practical importance. They basically define what it means for a problem to be solvable &lt;strong&gt;&lt;em&gt;in practice&lt;/em&gt;&lt;/strong&gt;. In the case of the linear algebra problem above regularization means “&lt;em&gt;making the matrix regular&lt;/em&gt;” so that these conditions will hold true on the regularized problem. That’s where the name comes from.&lt;/p&gt;
&lt;p&gt;Ok, cool but why are these conditions important to ML or statistics? Consider the case of Maximum Likelihood Estimation (MLE) of a parametric model (although the lesson applies more generally):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In MLE we estimate a model’s unknown parameters by maximizing the log-likelihood. If no such maximizing values of the parameters can be found then the optimization problem does not have a solution and we can not obtain estimates for the unknown parameters to begin with! So, mirroring the first Hadamard condition, we require a maximizer to exist.&lt;/li&gt;
&lt;li&gt;Non-Bayesian statistical models naturally assume that a single fixed set of parameters exists that specifies the relevant distributions. If MLE gives multiple sets of parameters that maximize the likelihood (as happens in the presence of multiple local maxima) we may have no way to tell which maximizer is the one that estimates the actual parameters the best! So, mirroring the second Hadamard condition, we require the maximizer to be unique.&lt;/li&gt;
&lt;li&gt;Statistical models assume that data is in part random and so is subject to changes. If the estimated values of the parameters change a lot when the data changes a little then it’s impossible to tell when the MLE estimated parameters are in fact good estimates of the true parameters and when they are not! So, mirroring the third Hadamard condition, we require the maximizer to be stable with respect to changes in the data used for the fit.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Regularization is one way to change the problem so that these conditions are met.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some examples&lt;/h2&gt;
&lt;p&gt;Ok, let’s look at some examples!&lt;/p&gt;
&lt;div id=&#34;shifting-the-eigenvalues-of-a-symmetric-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Shifting the eigenvalues of a Symmetric Matrix&lt;/h3&gt;
&lt;p&gt;Suppose we are asked to solve the linear inverse problem from above &lt;span class=&#34;math inline&#34;&gt;\(A\beta = y\)&lt;/span&gt;, but that this time &lt;span class=&#34;math inline&#34;&gt;\(A\in \mathbb{R}^{N\times N}\)&lt;/span&gt; is a symmetric matrix. The &lt;a href=&#34;https://pages.mtu.edu/~msgocken/ma5630spring2003/lectures/spectral/spectral/node2.html&#34;&gt;spectral theorem for symmetric matrices&lt;/a&gt; tells us that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be represented as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
A = QDQ^T
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is an &lt;a href=&#34;https://en.wikipedia.org/wiki/Orthogonal_matrix&#34;&gt;orthogonal matrix&lt;/a&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is a diagonal matrix. Moreover, the columns of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; are the eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, and the diagonal elements of &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; are the corresponding (real) eigenvalues. &lt;a href=&#34;https://mast.queensu.ca/~br66/419/spectraltheoremproof.pdf&#34;&gt;Here’s&lt;/a&gt; a proof if you care for it.&lt;/p&gt;
&lt;p&gt;This representation shows us exactly what the theoretical difficulty is in the inverse problem. Inverting &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is easy. Indeed orthogonal matrices are always invertible, with the inverse given by the transpose: &lt;span class=&#34;math inline&#34;&gt;\(Q^{-1} = Q^T\)&lt;/span&gt;. The geometric significance of orthogonal matrices comes from the fact (basically their definition) that they preserve the inner product of vectors: If we denote the inner product by &lt;span class=&#34;math inline&#34;&gt;\(\langle \cdot,\cdot\rangle\)&lt;/span&gt; then for any &lt;span class=&#34;math inline&#34;&gt;\(x,y \in R^N\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\langle x, y \rangle = \langle Qx, Qy \rangle
\]&lt;/span&gt;
Thus (using the definition of the transpose) &lt;span class=&#34;math inline&#34;&gt;\(\langle x, y \rangle = \langle Q^TQx, y \rangle\)&lt;/span&gt;. Since this holds for any &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(x = Q^TQx\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Since this holds for all &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(Q^TQ = I\)&lt;/span&gt; and so &lt;span class=&#34;math inline&#34;&gt;\(Q^T\)&lt;/span&gt; is the inverse of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;No, the difficulty is simply in inverting the diagonal matrix &lt;span class=&#34;math inline&#34;&gt;\(D = \text{diag}(\sigma_1, \sigma_2, ..., \sigma_N)\)&lt;/span&gt;. If none of the eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(D^{-1} = \text{diag}(\sigma_1^{-1}, \sigma_2^{-1}, ..., \sigma_N^{-1})\)&lt;/span&gt;. In this case there isn’t any direction that &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; (and hence &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;) squashes into &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. However, if some of the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt;’s are &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; then we can not invert &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; and the problem will fail to satisfy at least one of the first 2 Hadamard conditions. Even if none of the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt;’s are exactly &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, some may be numerically very close to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; in comparison to the others. In that case the value of their reciprocals may be enormously large and may lead to numerical instability in the problem, violating the 3rd Hadamard condition. This would be a big problem in practice because computers &lt;a href=&#34;https://floating-point-gui.de/errors/propagation/&#34;&gt;hate mixing floating point numbers that are drastically different in size&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To address this issue we note that we can shift the eigenvalues of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; by adding a multiple of an identity matrix:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
A \to  A + \lambda I
\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is an eigenvalue of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(\sigma+\lambda\)&lt;/span&gt; is an eigenvalue of &lt;span class=&#34;math inline&#34;&gt;\(A+\lambda I\)&lt;/span&gt;. This is because every vector is an eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;: If &lt;span class=&#34;math inline&#34;&gt;\(Av = \sigma v\)&lt;/span&gt; then trivially &lt;span class=&#34;math inline&#34;&gt;\(\lambda Iv = \lambda v\)&lt;/span&gt;, so adding gives &lt;span class=&#34;math inline&#34;&gt;\((A+\lambda I)v = (\sigma + \lambda)v\)&lt;/span&gt;. This can also be seen form the representation above:
&lt;span class=&#34;math display&#34;&gt;\[
A + \lambda I = QDQ^T + \lambda QQ^T = Q(D + \lambda I)Q^T = Q\tilde{D}Q^T
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\tilde{D}= \text{diag}(\sigma_1 + \lambda, \sigma_2 + \lambda, ..., \sigma_N + \lambda)\)&lt;/span&gt;. Therefore, if we choose &lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;gt; \min\{\sigma_1, \sigma_2, ..., \sigma_N\}+ \delta\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\delta &amp;gt; 0\)&lt;/span&gt;, then all the shifted eigenvalues satisfy &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i + \lambda &amp;gt; \delta &amp;gt; 0\)&lt;/span&gt; and the new shifted problem&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(A+\lambda I)x = y
\]&lt;/span&gt;
will be solvable with solution given by
&lt;span class=&#34;math display&#34;&gt;\[
x = Q(D + \lambda I)^{-1}Q^Ty
\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is sufficiently large this inverse will exist and will be numerically stable (all of the eigenvalues will have been shifted away from 0).&lt;/p&gt;
&lt;p&gt;Making the change &lt;span class=&#34;math inline&#34;&gt;\(A \to A + \lambda I\)&lt;/span&gt; regularized the problem into one that satisfied Hadamard’s conditions, which is fundamentally the point of regularization. The change we made was essentially to replace the &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; with the approximation &lt;span class=&#34;math inline&#34;&gt;\((A+\lambda I)^{-1}\)&lt;/span&gt;, but we could have used other approximations as well, for example partial sums of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Invertible_matrix#By_Neumann_series&#34;&gt;Neumann Series Expansion of the &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt;&lt;/a&gt;. Regardless, the general principle illustrated above is basically to replace one problem by an approximate problem that does not suffer the same existence/stability issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regularization-as-a-perturbation-of-an-invertible-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regularization as a perturbation of an invertible matrix&lt;/h3&gt;
&lt;p&gt;Above we regularized the ill-posed problem &lt;span class=&#34;math inline&#34;&gt;\(A\beta = y\)&lt;/span&gt; by replacing it with the problem &lt;span class=&#34;math inline&#34;&gt;\((A+\lambda I)x = y\)&lt;/span&gt;. Let’s go a bit deeper with this process.&lt;strong&gt;You may skip this section on your first read.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dividing by &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, the problem &lt;span class=&#34;math inline&#34;&gt;\((A+\lambda I)x = y\)&lt;/span&gt; is equivalent to the problem&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\epsilon A+I)x = \epsilon y
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon := \frac{1}{\lambda}\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is large &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is small, and vice versa. Hence for large &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; the matrix &lt;span class=&#34;math inline&#34;&gt;\(\epsilon A+I\)&lt;/span&gt; can be seen as a small perturbation from the identity matrix &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now because &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; is invertible then for a small enough &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; (and hence for a large enough &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;) the preturbed matrix &lt;span class=&#34;math inline&#34;&gt;\(\epsilon A+I\)&lt;/span&gt; is also invertible! Why? That’s a great question! The previous section gave one proof, but there are some much nicer ways to see why:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Consider the function
&lt;span class=&#34;math display&#34;&gt;\[
\det:\mathbb{R}^{N\times N} \to \mathbb{R}
\]&lt;/span&gt;
that maps a matrix to it’s &lt;a href=&#34;https://en.wikipedia.org/wiki/Determinant&#34;&gt;determinant&lt;/a&gt;. Because the space &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{N\times N}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(N\times N\)&lt;/span&gt; matrices is just the Euclidean inner product space &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{N^2}\)&lt;/span&gt; with some extra algebraic structure, and because &lt;span class=&#34;math inline&#34;&gt;\(\det(A)\)&lt;/span&gt; is a polynomial function of the elements of a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\det\)&lt;/span&gt; is a continuous function on &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{N\times N}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;By &lt;a href=&#34;https://en.wikipedia.org/wiki/Cramer%27s_rule&#34;&gt;Cramer’s Rule&lt;/a&gt;, a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is invertible if and only if &lt;span class=&#34;math inline&#34;&gt;\(\det(A) \ne 0\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(\det\)&lt;/span&gt; is a continuous function on &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{N\times N}\)&lt;/span&gt; then the set of invertible matrices is an open subset of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{N\times N}\)&lt;/span&gt;! Hence for every invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(L\in\mathbb{R}^{N\times N}\)&lt;/span&gt; and every arbitrary matrix &lt;span class=&#34;math inline&#34;&gt;\(A\in\mathbb{R}^{N\times N}\)&lt;/span&gt; there exists an &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_0 &amp;gt; 0\)&lt;/span&gt; such that for all &lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;lt; \epsilon_0\)&lt;/span&gt; the matrix &lt;span class=&#34;math inline&#34;&gt;\(\epsilon A+L\)&lt;/span&gt; is invertible. &lt;strong&gt;QED&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There’s a version of the theorem in Banach spaces, but we don’t need it.&lt;/p&gt;
&lt;p&gt;Noticed that the only thing we needed about the matrix &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; in the above proof was that it was invertible. Therefore, we never needed to restrict attention to just the identity matrix &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;, but could have used any invertible matrix to regularize the problem:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(A+\lambda L)x = y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is any convenient invertible matrix. Below, where we regularize OLS, we are not restricted to using the identity matrix &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; to regularize, but can use any invertible symmetric matrix (preferably one that is positive definite so that a minimizer continues to exist).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;l2-regularization-of-ols&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularization of OLS&lt;/h3&gt;
&lt;p&gt;With this example we begin moving towards the statistical part of the post. One of the most widely known examples of regularization is what is often called &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularization, or &lt;a href=&#34;https://en.wikipedia.org/wiki/Tikhonov_regularization&#34;&gt;Tikhonov&lt;/a&gt; regularization of Ordinary Least Squares.&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is a real valued random variable and &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; is a random vector with values in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^p\)&lt;/span&gt;. Suppose that we have the conditional relationship&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y \ \ | \ \ \vec{X} \sim \mathcal{N}(\ \langle \vec{X}  , \beta\rangle \ ,\ \sigma^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(\mu, \sigma^2)\)&lt;/span&gt; denotes the univariate normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Here (and everywhere else) the symbol &lt;span class=&#34;math inline&#34;&gt;\(\langle v,w\rangle\)&lt;/span&gt; represents the inner product of two vectors &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. This is the most natural probability model that leads to linear regression. In practice the parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; that specify this conditional distribution are unknown and it is desired that they be estimated from data.&lt;/p&gt;
&lt;p&gt;In this canonical situation we assume that we have a data set &lt;span class=&#34;math inline&#34;&gt;\(\{ (Y_i, \vec{X}_i)\}_{i = 1}^N\)&lt;/span&gt; consisting of samples generated independently of one another from a fixed multivariate distribution for &lt;span class=&#34;math inline&#34;&gt;\((Y, \vec{X})\)&lt;/span&gt; (i.e. we assume our data was sampled IID). To fit the unknown parameters we use MLE. We may choose to use the conditional density &lt;span class=&#34;math inline&#34;&gt;\(p(Y|\vec{X})\)&lt;/span&gt; in the likelihood and this would make it &lt;strong&gt;&lt;em&gt;conditional&lt;/em&gt;&lt;/strong&gt; MLE. Or we may choose the unconditional multivariate density &lt;span class=&#34;math inline&#34;&gt;\(p(Y, \vec{X})\)&lt;/span&gt;. However, if we assume that the marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, (i.e. &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{X})\)&lt;/span&gt;), does not depend on either &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; then because &lt;span class=&#34;math inline&#34;&gt;\(p(Y,\vec{X}) = p(Y|\vec{X})p(\vec{X})\)&lt;/span&gt; building the likelihood using either &lt;span class=&#34;math inline&#34;&gt;\(p(Y|\vec{X})\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(p(Y,\vec{X})\)&lt;/span&gt; will lead to the same maximization problem because they differ by a constant factor (constant in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; that is). So we will use the conditional density &lt;span class=&#34;math inline&#34;&gt;\(p(Y|\vec{X})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Because the data is assumed to be generated IID then the full likelihood of the data is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L}(\beta, \sigma\ |\ \{ (Y_i, \vec{X}_i)\}) = \prod_{i = 1}^N p(Y_i|\vec{X}_i) = \prod_{i=1}^N\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\big(\frac{Y_i - \langle \vec{X}_i  , \beta\rangle}{\sigma}\big)^2} = \frac{1}{(\sigma^22\pi)^{N/2}}e^{\sum_{i=1}^N-\frac{1}{2}\big(\frac{Y_i - \langle \vec{X}_i  , \beta\rangle}{\sigma}\big)^2}
\]&lt;/span&gt;
Because the function &lt;span class=&#34;math inline&#34;&gt;\(f(x) := -\log(x)\)&lt;/span&gt; is decreasing we may instead minimize the negative of the log of this expression:
&lt;span class=&#34;math display&#34;&gt;\[
-\log(\mathcal{L}(\beta,\sigma\ |\ \{ (Y_i, \vec{X}_i)\})) = \frac{N}{2}\log(\sigma^22\pi) + \frac{1}{2\sigma^2}\sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i  , \beta\rangle\bigg)^2
\]&lt;/span&gt;
We first minimize with respect to &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; as this is necessary to do first before finding the minimizing value of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. To do this we need to minimize the only term that depends on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, namely the sum of squares &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta) := \sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)&lt;/span&gt; (hence Least Squares regression).&lt;/p&gt;
&lt;div id=&#34;a-geometric-interlude&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;A geometric interlude&lt;/h4&gt;
&lt;p&gt;Before we do that, let’s think about what the expression &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta)\)&lt;/span&gt; is. The term &lt;span class=&#34;math inline&#34;&gt;\(\langle \vec{X}_i , \beta\rangle\)&lt;/span&gt; is linear in the unknowns &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, and hence so is &lt;span class=&#34;math inline&#34;&gt;\(Y_i - \langle \vec{X}_i , \beta\rangle\)&lt;/span&gt;. Therefore, the square &lt;span class=&#34;math inline&#34;&gt;\(\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)&lt;/span&gt; is quadratic in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Thus since the full expression &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)&lt;/span&gt; is a sum of quadratic functions in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; it too is a quadratic function in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Since all terms in the sum are squares, the full sum is never negative and its graph in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Paraboloid&#34;&gt;non-hyperbolic paraboloid&lt;/a&gt;. Usually such shapes look like bowls. However, some can degenerate so that they become flat in one or more directions. Here’re some examples in R:&lt;/p&gt;
&lt;p&gt;Non-degenarte paraboloids look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nice.paraboloid = function(x,y)
{
    return(x^2+0.5*y^2)
}
x = y = seq(from = -4, to = 4, by = 0.2)
z = outer(x, y, nice.paraboloid)

persp(x, y, z,
      main=&amp;quot;Plot of a Non-degenerate 2D Paraboloid&amp;quot;,
      theta = 30, phi = 15,
      col = &amp;quot;springgreen&amp;quot;, shade = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/004_Regularization/main_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see such a paraboloid is bowl shaped. More technically it’s strictly convex, with a clear unique minimum point. However, paraboloids can degenerate so that they flatten out in some directions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;degenerate.paraboloid = function(x,y)
{
    return(x^2) #Does not change value as y changes
}
x = y = seq(from = -4, to = 4, by = 0.2)
z = outer(x, y, degenerate.paraboloid)

persp(x, y, z,
      main=&amp;quot;Plot of a Degenerate 2D Paraboloid&amp;quot;,
      theta = 30, phi = 15,
      col = &amp;quot;springgreen&amp;quot;, shade = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/004_Regularization/main_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example we changed the coefficient of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; from 0.5 to 0. The result is that in the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-direction the paraboloid flattened out and it no longer looks bowl shaped. Instead there are infinitely many minimum points all on the axis &lt;span class=&#34;math inline&#34;&gt;\(\{(x,y): x = 0\}\)&lt;/span&gt;. Note that if instead of making the coefficient of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; equal to 0 we had made it a positive number very close to zero then the mimima would become unique but would become hard to distinguish from nearby points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tricky.paraboloid = function(x,y)
{
    return(x^2+ 0.05*y^2) #Notice the coefficient of y is quite small
}
x = y = seq(from = -4, to = 4, by = 0.2)
z = outer(x, y, tricky.paraboloid)

persp(x, y, z,
      main=&amp;quot;Plot of a Nearly-Degenerate 2D Paraboloid&amp;quot;,
      theta = 30, phi = 15,
      col = &amp;quot;springgreen&amp;quot;, shade = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/004_Regularization/main_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These pictures show what can go wrong with the minima of quadratic functions like &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta)\)&lt;/span&gt; and why regularization may be needed. Now to get back to the minimizing the sum of squares &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta) = \sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)&lt;/span&gt;. If you’re reading this article I’m going to assume you’ve seen this derivation before so I’ll move a bit fast.&lt;/p&gt;
&lt;p&gt;First we define &lt;span class=&#34;math inline&#34;&gt;\(Y\in \mathbb{R}^{N\times 1}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th component equal to &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt;. (Note an abuse of notation we are making: earlier &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; denoted a real valued random variable, but now we are using the same symbol to denote the vector of the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; realizations of this random variable.) In addition, let &lt;span class=&#34;math inline&#34;&gt;\(X\in \mathbb{R}^{N\times p}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th row equal to &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}_i\)&lt;/span&gt;. Then in matrix notation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i  , \beta\rangle\bigg)^2 = (Y - X\beta)^T(Y - X\beta)
\]&lt;/span&gt;
Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} = \text{argmax}_{\beta} \ \ (Y - X\beta)^T(Y - X\beta)\)&lt;/span&gt; be the sought after minimizer. Since &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; is a minizer in the interior of the domain of &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta)\)&lt;/span&gt;, the gradient of &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta)\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; must be 0:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-2X^TY + 2X^TX\hat{\beta} = 0
\]&lt;/span&gt;
therefore we obtain the normal equations&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
X^TX\hat{\beta} = X^TY
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is basically the same linear algebra problem as before: If the inverse &lt;span class=&#34;math inline&#34;&gt;\((X^TX)^{-1}\)&lt;/span&gt; existed and was numerically nice then we can solve for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} = (X^TX)^{-1}X^TY\)&lt;/span&gt;. However, if this matrix inverse does not exist (as can happen when we do not have enough rows/samples for the given number of columns/unknowns) then this formula is not useful.&lt;/p&gt;
&lt;p&gt;But as before we can simply regularize by replacing the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(X^TX + \lambda I\)&lt;/span&gt; for some sufficiently large &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Actually since &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is non-negative definite&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and symmetric all of it’s eigenvalues are non-negative. So any &lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt; would be sufficient to shift the eigenvalues into positive numbers. Now the regularized problem becomes &lt;span class=&#34;math inline&#34;&gt;\((X^TX + \lambda I)\hat{\beta} = X^TY\)&lt;/span&gt;. Therefore we get the regularized MLE solution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_{reg} := (X^TX + \lambda I)^{-1}X^TY
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Does this regularized problem correspond to its own minimization problem? Yes! Working backwards, this new problem is equivalent to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-2X^TY + 2X^TX\hat{\beta} + 2\lambda\hat{\beta}= 0
\]&lt;/span&gt;
The left had side is the gradient of &lt;span class=&#34;math inline&#34;&gt;\((Y - X\beta)^T(Y - X\beta) + \lambda\sum_{j=1}^p\beta_j^2 = (Y - X\beta)^T(Y - X\beta) + \lambda \langle \beta, \beta\rangle\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\beta = \hat{\beta}\)&lt;/span&gt; as can be checked. So the regularized problem corresponds to trying to minimize the expression &lt;span class=&#34;math inline&#34;&gt;\((Y - X\beta)^T(Y - X\beta) + \lambda\sum_{j=1}^p\beta_j^2 = (Y - X\beta)^T(Y - X\beta) + \lambda \langle \beta, \beta\rangle\)&lt;/span&gt;. This of course is &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularization. Thus we have derived &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularization for OLS simply by seeking to transform the inverse problem that arises in OLS so that it may satisfy the Hadamard conditions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;an-illustrative-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;An Illustrative Example&lt;/h3&gt;
&lt;p&gt;Below we can see geometrically what regularization does. The sum of squares expression &lt;span class=&#34;math inline&#34;&gt;\((Y - X\beta)^T(Y - X\beta)\)&lt;/span&gt; is quadratic in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, but may have a graph that is a degenerate paraboloid. This is what causes it to have multiple minimizers in OLS and what makes the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; singular (more on this point in the next section). However the expression &lt;span class=&#34;math inline&#34;&gt;\(\lambda\langle \beta, \beta\rangle = \lambda\sum_{j = 1}^p\beta_j^2\)&lt;/span&gt; is a strictly positive-definite quadratic form. Its graph is a non-degenerate bowl shaped paraboloid.&lt;/p&gt;
&lt;p&gt;Adding a non-degenerate paraboloid to something that is not bowl shaped makes the second graph more bowl shaped! Moreover it shifts the minimum of the 2nd graph towards the mimimum of the bowl. As an illustration, let’s take a look at an example where this is easy to see.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())
bumpy.function = function(x,y)
{
    return(sin(x)+sin(y))
}

nice.paraboloid = function(x,y)
{
    return(0.15*(x^2 + y^2)) #lambda = 0.15 is used
}

x = y = seq(from = -4, to = 4, by = 0.2)
bumpy = outer(x, y, bumpy.function)
paraboloid = outer(x, y, nice.paraboloid)
bumpy.plus.paraboloid = bumpy + paraboloid

persp(x, y, bumpy,
      main=&amp;quot;Graph of a bumpy function with multiple minima&amp;quot;,
      theta = 30, phi = 15,
      col = &amp;quot;springgreen&amp;quot;, shade = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/004_Regularization/main_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;persp(x, y, paraboloid,
      main=&amp;quot;Graph of a nice paraboloid&amp;quot;,
      theta = 30, phi = 15,
      col = &amp;quot;springgreen&amp;quot;, shade = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/004_Regularization/main_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;persp(x, y, bumpy.plus.paraboloid,
      main=&amp;quot;Graph of a regularized bumpy function = bumpy function + paraboloid&amp;quot;,
      theta = 30, phi = 15,
      col = &amp;quot;springgreen&amp;quot;, shade = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/004_Regularization/main_files/figure-html/unnamed-chunk-4-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we see that the most important geometric aspect of the regularizing term &lt;span class=&#34;math inline&#34;&gt;\(\lambda\langle\beta, \beta\rangle\)&lt;/span&gt; is the fact that it is &lt;a href=&#34;https://en.wikipedia.org/wiki/Convex_function&#34;&gt;strictly convex&lt;/a&gt;!! &lt;strong&gt;&lt;em&gt;Although we will not dwell on it, it is impossible to overstate the theoretical importance of the previous sentence.&lt;/em&gt;&lt;/strong&gt; As a matter of fact, geometrically speaking it’s clear that had we added &lt;strong&gt;any&lt;/strong&gt; strictly convex function to the bumpy function we would have gotten something more bowl shaped. We will not go further into it here but you should know that &lt;a href=&#34;https://en.wikipedia.org/wiki/Convex_analysis&#34;&gt;convexity is one of those properties in mathematics out of which entire fields are created&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-hessian-matrix-and-more-complex-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Hessian matrix and more complex models&lt;/h3&gt;
&lt;p&gt;In the OLS problem above the Maximum Likelihood estimator turned out to be the one that minimized the sum of squares expression &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta) := (Y - X\beta)^T(Y - X\beta)\)&lt;/span&gt;. This expression can be expanded in matrix notation as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y^TY - Y^TX\beta - \beta^TX^TY + \beta^TX^TX\beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We see that there is a quadratic term (&lt;span class=&#34;math inline&#34;&gt;\(\beta^TX^TX\beta\)&lt;/span&gt;) and the rest are terms with powers of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; less than 2. The matrix of second derivatives of this expression (known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Hessian_matrix&#34;&gt;Hessian matrix&lt;/a&gt;) is therefore just the matrix of coefficients of this quadratic term: &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;. This Hessian is exactly what was the star of the show in the OLS problem!&lt;/p&gt;
&lt;p&gt;The Hessian of a function at a point tells us the convexity of the function at the point. &lt;a href=&#34;https://en.wikipedia.org/wiki/Second_partial_derivative_test&#34;&gt;If the Hessian is positive definite, then near the minimizing point the function is bowl shaped. If the Hessian is negative definite then near a maximizing point the function is shaped like an upside down bowl.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Moreover, The Hessian is always a symmetric matrix by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives#Formal_expressions_of_symmetry&#34;&gt;equality of cross-derivatives&lt;/a&gt;. So the previous point is really a statement about Hessian’s eigenvalues. In short, if the Hessian has all positive eigenvalues then it is positive definite and the function is bowl shaped near its minimizer.&lt;/p&gt;
&lt;p&gt;The OLS Hessian matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is symmetric and non-negative definite, so the graph of the sum of squares &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta) := (Y - X\beta)^T(Y - X\beta)\)&lt;/span&gt; can only fail to be bowl shaped near the minimizer &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; if the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; has eigenvalues that are equal 0 (or positive but close to 0 in the case numerical instability). In which case, the graph of &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta)\)&lt;/span&gt; is a degenerate non-hyperbolic paraboloid and there are multiple minimizing solutions &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thus regularizing the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is just regularizing the Hessian matrix of the function &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta)\)&lt;/span&gt; we want to minimize!!&lt;/strong&gt; This fact is what allows us to take the idea beyond OLS.&lt;/p&gt;
&lt;p&gt;Indeed if &lt;span class=&#34;math inline&#34;&gt;\(f(\beta)\)&lt;/span&gt; is any 2nd order differentiable cost function in any machine learning model then by the linearity of the derivative&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Hessian}(f + \lambda \langle \beta, \beta\rangle) = \text{Hessian}(f) + \lambda\text{Hessian}(\langle \beta, \beta\rangle) = \text{Hessian}(f) + \lambda I
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There we used the fact that &lt;span class=&#34;math inline&#34;&gt;\(\text{Hessian}(\langle \beta, \beta\rangle) = \text{Hessian}(\sum_i^p \beta_i^2) = I\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is almost arbitrary we see that we can apply &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularization to a very large family of problems, with the goal being to regularize the Hessian of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. As an example let’s look at some other kinds of regression problems. For OLS we assumed the conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(Y \ \ | \ \ \vec{X} \sim \mathcal{N}(\ \langle \vec{X} , \beta\rangle \ ,\ \sigma^2)\)&lt;/span&gt;, but we may have chosen a different conditional distribution.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; takes on only values in the set &lt;span class=&#34;math inline&#34;&gt;\(\{0,1\}\)&lt;/span&gt; then it is a Bernoulli random variable. This is the case in logistic regression where the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; is
&lt;span class=&#34;math display&#34;&gt;\[
Y \ \ | \ \ \vec{X} \sim \mathcal{B}(\ p = \phi(\langle \vec{X}  , \beta\rangle) \ )
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B(p)}\)&lt;/span&gt; is a Bernoulli distribution with probability of a positive event equal to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p = \text{E}[Y|\vec{X}] = \phi(\langle \vec{X} , \beta\rangle)\)&lt;/span&gt; is the probability of a positive event given &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\phi(t) = \frac{e^t}{1+e^t}\)&lt;/span&gt; is the standard logit. In this case the conditional density &lt;span class=&#34;math inline&#34;&gt;\(p(Y|\vec{X})\)&lt;/span&gt; can be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(Y|\vec{X}) = p^Y\big(1 - p)\big)^{1-Y} =\phi(\langle \vec{X}  , \beta\rangle)^Y\big(1 - \phi(\langle \vec{X}  , \beta\rangle)\big)^{1-Y}
\]&lt;/span&gt;
So the negative log-likelihood is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-\mathcal{l}(\beta) = -\sum_{i=1}^N Y_i\log(\phi(\langle \vec{X}_i  , \beta\rangle)) + (1-Y_i)\log(1-\phi(\langle \vec{X}_i  , \beta\rangle))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This function may or may not look bowl shaped (i.e. strictly convex) near the &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; that minimizes it. In case it doesn’t we can make it so by adding &lt;span class=&#34;math inline&#34;&gt;\(\lambda \langle \beta, \beta\rangle = \lambda\sum_{j = 1}^p\beta_j^2\)&lt;/span&gt; for some sufficiently large &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and minimizing this new problem. The same applies to generalized linear models, neural networks, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bias-variance-trade-offs-and-regularization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bias-Variance trade offs and Regularization&lt;/h2&gt;
&lt;p&gt;Above we used regularization methods to make a problem “nicer” in the numerical sense (i.e. satisfying Hadamard’s conditions). But what does “nicer” mean in the statistical context? That is a multifaceted question. The first step is to recognize that what might be viewed as instability from the numerical point of view, can be understood as high variance from the statistical point of view.&lt;/p&gt;
&lt;p&gt;We illustrate with the OLS estimator. Suppose that the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is indeed invertible. The standard OLS estimator is the random vector given by the normal equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta} = (X^TX)^{-1}X^TY
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We see that this is an unbiased estimator of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{E}[\hat{\beta}] = \text{E}\big[\text{E}[\hat{\beta}|X]\big] = \text{E}\bigg[(X^TX)^{-1}X^T\text{E}[Y|X]\bigg] = \text{E}\bigg[(X^TX)^{-1}X^TX\beta\bigg] = \text{E}[\beta] = \beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Moreover, it’s easy enough to compute the conditional covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}[\hat{\beta}|X] = (X^TX)^{-1}X^T\cdot \text{Var}[Y|X] \cdot X(X^TX)^{-1} = (X^TX)^{-1}X^T\cdot \sigma^2 I \cdot X(X^TX)^{-1} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \sigma^2 (X^TX)^{-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_total_variance&#34;&gt;unconditional covariance matrix can be computed as&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}[\hat{\beta}] = \text{E}[\text{Var}[\hat{\beta}|X]] + \text{Var}[\text{E}[\hat{\beta}|X]] 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \sigma^2\text{E}[(X^TX)^{-1}] + \text{Var}[\beta] = \sigma^2\text{E}[(X^TX)^{-1}]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is harder to compute because it depends on the distribution of the random matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Regardless, we can see that what controls the variance of the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; (whether conditional or not) is the inverse matrix &lt;span class=&#34;math inline&#34;&gt;\((X^TX)^{-1}\)&lt;/span&gt;. This is interesting because it shows that the matrix we identified as the Hessian of the OLS cost function (&lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;) is also the matrix that controls the covariance of the OLS estimator.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If any of the eigenvalues of the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; were “close” to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; then the eigenvalues of the inverse will be very large, causing the variance of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; to be very large. If you’re familiar with VIFs, this is what causes large &lt;a href=&#34;https://en.wikipedia.org/wiki/Variance_inflation_factor&#34;&gt;variance inflation factors&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Regularization is used to reduce the variance in this estimator. If we denote the regularized estimator by:
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_{reg} = (X^TX + \lambda I)^{-1}X^TY
\]&lt;/span&gt;
Then this estimator is biased away from &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. To see this we first compute the conditional mean:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{E}[\hat{\beta}_{reg}|X] = (X^TX + \lambda I)^{-1}X^TX\beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= (X^TX + \lambda I)^{-1}(X^TX + \lambda I)\beta - \lambda (X^TX + \lambda I)^{-1}\beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \beta - \lambda (X^TX + \lambda I)^{-1}\beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence
&lt;span class=&#34;math display&#34;&gt;\[
\text{E}[\hat{\beta}_{reg}] = \beta - \lambda\text{E}\big[(X^TX + \lambda I)^{-1}\big]\beta
\]&lt;/span&gt;
which is “&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; minus something” and hence not equal to &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. However the effect on the variance is better:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}[\hat{\beta}_{reg}|X] = (X^TX + \lambda I)^{-1}X^T\cdot\text{Var}[Y|X]\cdot X(X^TX + \lambda I)^{-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \sigma^2(X^TX + \lambda I)^{-1}X^TX(X^TX + \lambda I)^{-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \sigma^2(X^TX + \lambda I)^{-1}(X^TX+\lambda I)(X^TX + \lambda I)^{-1} - \sigma^2\lambda(X^TX + \lambda I)^{-2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
=  \sigma^2(X^TX + \lambda I)^{-1} - \sigma^2\lambda(X^TX + \lambda I)^{-2}
\]&lt;/span&gt;
This variance formula may look messy but the gist is that instead of inverting &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; we are inverting &lt;span class=&#34;math inline&#34;&gt;\(X^TX + \lambda I\)&lt;/span&gt;. The matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX + \lambda I\)&lt;/span&gt; has larger eigenvalues than the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;. Therefore &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}[\hat{\beta}_{reg}|X] = \sigma^2(X^TX + \lambda I)^{-1} - \sigma^2\lambda(X^TX + \lambda I)^{-2}\)&lt;/span&gt; is smaller than &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}[\hat{\beta}|X] = \sigma^2 (X^TX)^{-1}\)&lt;/span&gt; in the sense that it has smaller eigenvalues. &lt;strong&gt;Thus regularization has increased bias, but reduced variance.&lt;/strong&gt; Similar effects hold for more complex models than OLS, but instead of chasing formulas the read should try cooking up some numerical examples via Monte Carlo.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-the-bayesian-view-point&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What about the Bayesian view point?&lt;/h2&gt;
&lt;p&gt;A very natural perspective on regularization can be found in Bayesian modeling, where regularization terms amount to simply specifying prior distributions. However, this is standard Bayesian theory and this post is already long enough :P&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Indeed suppose &lt;span class=&#34;math inline&#34;&gt;\(v\in \mathbb{R}^p\)&lt;/span&gt; is any vector. Then &lt;span class=&#34;math inline&#34;&gt;\(\langle v, X^TXv\rangle = \langle Xv,Xv\rangle = ||Xv||^2 \ge 0\)&lt;/span&gt;. Hence &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is always non-negative definite.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;This is a general feature of Maximum Likelihood estimators called “&lt;em&gt;asymptotic efficiency&lt;/em&gt;”, where the covariance matrix of the MLE estimator approaches a “best possible” covariance matrix as the sample size increases. Essentially the best possible covariance matrix that an unbiased estimator of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; can have is given given by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound&#34;&gt;Cramer-Rao Bound&lt;/a&gt; and is determined by the inverse of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fisher_information&#34;&gt;Fisher Information matrix&lt;/a&gt;, whose &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt;-component is &lt;span class=&#34;math inline&#34;&gt;\(-\text{E}[\partial^2\log(p(Y, \vec{X}| \beta))/\partial \beta_i\partial \beta_j]\)&lt;/span&gt;. The beauty is that the Hessian of the negative loglikelihood is the sample estimator of this Fisher Information (where expectation is replaced by average over samples). This is why the Hessian is showing up as the determining factor in estimator covariance.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      <content:encoded>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Many newcomers to machine learning know about regularization, but they may not understand it yet. In particular, they may not know why regularization has that name. In this post we discuss the numerical and statistical significance of regularization methods in machine learning and more general statistical models. We’ll try to introduce why one may want to use regularization methods in the first place and how to interpret the fitted model from a statistical point of view.</p>
<p>The post will be long because there are a lot of cute nooks and crannies, and we’ll assume you know your linear algebra. However, if you already know what an inner product is then we think this post will be worth your time.</p>
</div>
<div id="matrices-and-linear-ill-posed-problems" class="section level2">
<h2>Matrices and Linear Ill-Posed Problems</h2>
<p>Suppose that we have a matrix <span class="math inline">\(A\in \mathbb{R}^{N \times p}\)</span>, a vector <span class="math inline">\(y\in \mathbb{R}^N\)</span>, and that we seek a vector <span class="math inline">\(\beta\in\mathbb{R}^p\)</span> such that <span class="math inline">\(A\beta = y\)</span>. How would one solve this problem? One answer might be to simply apply the inverse matrix to both sides of the equation: <span class="math inline">\(\beta = A^{-1}y\)</span>. However, there are three problems with this:</p>
<ol style="list-style-type: decimal">
<li>A matrix inverse <span class="math inline">\(A^{-1}\)</span> may not exist.</li>
<li>Even if the matrix inverse exists it can be extremely expensive to calculate this inverse and apply the result to <span class="math inline">\(y\)</span>.</li>
<li>Even if we are somehow able to calculate <span class="math inline">\(A^{-1}y\)</span> the solution may not be very stable. Small numerical changes in either <span class="math inline">\(A\)</span> or <span class="math inline">\(y\)</span> may lead to large changes in the solution <span class="math inline">\(\beta\)</span>.</li>
</ol>
<p>Issue 2 above is not really a problem in the sense that <a href="https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/">one should never really need to find <span class="math inline">\(A^{-1}\)</span> to compute <span class="math inline">\(A^{-1}y\)</span></a>. Instead the most efficient numerical algorithms typically compute <span class="math inline">\(A^{-1}y\)</span> by using special factorizations of <span class="math inline">\(A\)</span>, such as <a href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a>.</p>
<p>The other two issues are very important and are inextricably linked to each other and to regularization in machine learning.</p>
<ul>
<li><p>Issue 3 means just what is says: that the solution <span class="math inline">\(\beta\)</span> may change a lot if the known data <span class="math inline">\(A\)</span> and/or <span class="math inline">\(y\)</span> only change a little.</p></li>
<li><p>Issue 1 means that the matrix is not invertible. A matrix that is not invertible is called <strong><em>singular</em></strong>. A matrix that is invertible is usually called <strong><em>nonsingular</em></strong>, but a less common synonym is <a href="https://mathworld.wolfram.com/RegularMatrix.html"><strong><em>regular</em></strong></a> and this is where the name <em>regularization</em> comes from. When a matrix is singular it means that the problem <span class="math inline">\(A\beta = y\)</span> may have either no solution at all or have at least 2 distinct solutions.</p></li>
</ul>
<p>Any one of these issues being true means that the linear problem <span class="math inline">\(A\beta = y\)</span> is ill-posed in the sense that it violates Hadamard’s conditions for a <a href="https://en.wikipedia.org/wiki/Well-posed_problem">well-posed problem</a>. To be well-posed:</p>
<ol style="list-style-type: decimal">
<li>A problem must have a solution</li>
<li>The solution must be unique</li>
<li>The solution’s behavior must be stable/continuous with respect to the data</li>
</ol>
<p>These conditions are of extreme practical importance. They basically define what it means for a problem to be solvable <strong><em>in practice</em></strong>. In the case of the linear algebra problem above regularization means “<em>making the matrix regular</em>” so that these conditions will hold true on the regularized problem. That’s where the name comes from.</p>
<p>Ok, cool but why are these conditions important to ML or statistics? Consider the case of Maximum Likelihood Estimation (MLE) of a parametric model (although the lesson applies more generally):</p>
<ol style="list-style-type: decimal">
<li>In MLE we estimate a model’s unknown parameters by maximizing the log-likelihood. If no such maximizing values of the parameters can be found then the optimization problem does not have a solution and we can not obtain estimates for the unknown parameters to begin with! So, mirroring the first Hadamard condition, we require a maximizer to exist.</li>
<li>Non-Bayesian statistical models naturally assume that a single fixed set of parameters exists that specifies the relevant distributions. If MLE gives multiple sets of parameters that maximize the likelihood (as happens in the presence of multiple local maxima) we may have no way to tell which maximizer is the one that estimates the actual parameters the best! So, mirroring the second Hadamard condition, we require the maximizer to be unique.</li>
<li>Statistical models assume that data is in part random and so is subject to changes. If the estimated values of the parameters change a lot when the data changes a little then it’s impossible to tell when the MLE estimated parameters are in fact good estimates of the true parameters and when they are not! So, mirroring the third Hadamard condition, we require the maximizer to be stable with respect to changes in the data used for the fit.</li>
</ol>
<p>Regularization is one way to change the problem so that these conditions are met.</p>
</div>
<div id="some-examples" class="section level2">
<h2>Some examples</h2>
<p>Ok, let’s look at some examples!</p>
<div id="shifting-the-eigenvalues-of-a-symmetric-matrix" class="section level3">
<h3>Shifting the eigenvalues of a Symmetric Matrix</h3>
<p>Suppose we are asked to solve the linear inverse problem from above <span class="math inline">\(A\beta = y\)</span>, but that this time <span class="math inline">\(A\in \mathbb{R}^{N\times N}\)</span> is a symmetric matrix. The <a href="https://pages.mtu.edu/~msgocken/ma5630spring2003/lectures/spectral/spectral/node2.html">spectral theorem for symmetric matrices</a> tells us that <span class="math inline">\(A\)</span> can be represented as</p>
<p><span class="math display">\[
A = QDQ^T
\]</span>
where <span class="math inline">\(Q\)</span> is an <a href="https://en.wikipedia.org/wiki/Orthogonal_matrix">orthogonal matrix</a> and <span class="math inline">\(D\)</span> is a diagonal matrix. Moreover, the columns of <span class="math inline">\(Q\)</span> are the eigenvectors of <span class="math inline">\(A\)</span>, and the diagonal elements of <span class="math inline">\(D\)</span> are the corresponding (real) eigenvalues. <a href="https://mast.queensu.ca/~br66/419/spectraltheoremproof.pdf">Here’s</a> a proof if you care for it.</p>
<p>This representation shows us exactly what the theoretical difficulty is in the inverse problem. Inverting <span class="math inline">\(Q\)</span> is easy. Indeed orthogonal matrices are always invertible, with the inverse given by the transpose: <span class="math inline">\(Q^{-1} = Q^T\)</span>. The geometric significance of orthogonal matrices comes from the fact (basically their definition) that they preserve the inner product of vectors: If we denote the inner product by <span class="math inline">\(\langle \cdot,\cdot\rangle\)</span> then for any <span class="math inline">\(x,y \in R^N\)</span></p>
<p><span class="math display">\[
\langle x, y \rangle = \langle Qx, Qy \rangle
\]</span>
Thus (using the definition of the transpose) <span class="math inline">\(\langle x, y \rangle = \langle Q^TQx, y \rangle\)</span>. Since this holds for any <span class="math inline">\(y\)</span>, then <span class="math inline">\(x = Q^TQx\)</span> for all <span class="math inline">\(x\)</span>. Since this holds for all <span class="math inline">\(x\)</span> then <span class="math inline">\(Q^TQ = I\)</span> and so <span class="math inline">\(Q^T\)</span> is the inverse of <span class="math inline">\(Q\)</span>.</p>
<p>No, the difficulty is simply in inverting the diagonal matrix <span class="math inline">\(D = \text{diag}(\sigma_1, \sigma_2, ..., \sigma_N)\)</span>. If none of the eigenvalues <span class="math inline">\(\sigma_i\)</span> are <span class="math inline">\(0\)</span> then <span class="math inline">\(D^{-1} = \text{diag}(\sigma_1^{-1}, \sigma_2^{-1}, ..., \sigma_N^{-1})\)</span>. In this case there isn’t any direction that <span class="math inline">\(D\)</span> (and hence <span class="math inline">\(A\)</span>) squashes into <span class="math inline">\(0\)</span>. However, if some of the <span class="math inline">\(\sigma_i\)</span>’s are <span class="math inline">\(0\)</span> then we can not invert <span class="math inline">\(D\)</span> and the problem will fail to satisfy at least one of the first 2 Hadamard conditions. Even if none of the <span class="math inline">\(\sigma_i\)</span>’s are exactly <span class="math inline">\(0\)</span>, some may be numerically very close to <span class="math inline">\(0\)</span> in comparison to the others. In that case the value of their reciprocals may be enormously large and may lead to numerical instability in the problem, violating the 3rd Hadamard condition. This would be a big problem in practice because computers <a href="https://floating-point-gui.de/errors/propagation/">hate mixing floating point numbers that are drastically different in size</a>.</p>
<p>To address this issue we note that we can shift the eigenvalues of <span class="math inline">\(A\)</span> by adding a multiple of an identity matrix:</p>
<p><span class="math display">\[
A \to  A + \lambda I
\]</span>
If <span class="math inline">\(\sigma\)</span> is an eigenvalue of <span class="math inline">\(A\)</span> then <span class="math inline">\(\sigma+\lambda\)</span> is an eigenvalue of <span class="math inline">\(A+\lambda I\)</span>. This is because every vector is an eigenvector of <span class="math inline">\(I\)</span>: If <span class="math inline">\(Av = \sigma v\)</span> then trivially <span class="math inline">\(\lambda Iv = \lambda v\)</span>, so adding gives <span class="math inline">\((A+\lambda I)v = (\sigma + \lambda)v\)</span>. This can also be seen form the representation above:
<span class="math display">\[
A + \lambda I = QDQ^T + \lambda QQ^T = Q(D + \lambda I)Q^T = Q\tilde{D}Q^T
\]</span>
where <span class="math inline">\(\tilde{D}= \text{diag}(\sigma_1 + \lambda, \sigma_2 + \lambda, ..., \sigma_N + \lambda)\)</span>. Therefore, if we choose <span class="math inline">\(\lambda &gt; \min\{\sigma_1, \sigma_2, ..., \sigma_N\}+ \delta\)</span> for some <span class="math inline">\(\delta &gt; 0\)</span>, then all the shifted eigenvalues satisfy <span class="math inline">\(\sigma_i + \lambda &gt; \delta &gt; 0\)</span> and the new shifted problem</p>
<p><span class="math display">\[
(A+\lambda I)x = y
\]</span>
will be solvable with solution given by
<span class="math display">\[
x = Q(D + \lambda I)^{-1}Q^Ty
\]</span>
If <span class="math inline">\(\lambda\)</span> is sufficiently large this inverse will exist and will be numerically stable (all of the eigenvalues will have been shifted away from 0).</p>
<p>Making the change <span class="math inline">\(A \to A + \lambda I\)</span> regularized the problem into one that satisfied Hadamard’s conditions, which is fundamentally the point of regularization. The change we made was essentially to replace the <span class="math inline">\(A^{-1}\)</span> with the approximation <span class="math inline">\((A+\lambda I)^{-1}\)</span>, but we could have used other approximations as well, for example partial sums of the <a href="https://en.wikipedia.org/wiki/Invertible_matrix#By_Neumann_series">Neumann Series Expansion of the <span class="math inline">\(A^{-1}\)</span></a>. Regardless, the general principle illustrated above is basically to replace one problem by an approximate problem that does not suffer the same existence/stability issues.</p>
</div>
<div id="regularization-as-a-perturbation-of-an-invertible-matrix" class="section level3">
<h3>Regularization as a perturbation of an invertible matrix</h3>
<p>Above we regularized the ill-posed problem <span class="math inline">\(A\beta = y\)</span> by replacing it with the problem <span class="math inline">\((A+\lambda I)x = y\)</span>. Let’s go a bit deeper with this process.<strong>You may skip this section on your first read.</strong></p>
<p>Dividing by <span class="math inline">\(\lambda\)</span>, the problem <span class="math inline">\((A+\lambda I)x = y\)</span> is equivalent to the problem</p>
<p><span class="math display">\[
(\epsilon A+I)x = \epsilon y
\]</span>
where <span class="math inline">\(\epsilon := \frac{1}{\lambda}\)</span>. When <span class="math inline">\(\lambda\)</span> is large <span class="math inline">\(\epsilon\)</span> is small, and vice versa. Hence for large <span class="math inline">\(\lambda\)</span> the matrix <span class="math inline">\(\epsilon A+I\)</span> can be seen as a small perturbation from the identity matrix <span class="math inline">\(I\)</span>.</p>
<p>Now because <span class="math inline">\(I\)</span> is invertible then for a small enough <span class="math inline">\(\epsilon\)</span> (and hence for a large enough <span class="math inline">\(\lambda\)</span>) the preturbed matrix <span class="math inline">\(\epsilon A+I\)</span> is also invertible! Why? That’s a great question! The previous section gave one proof, but there are some much nicer ways to see why:</p>
<p><strong>Proof:</strong> Consider the function
<span class="math display">\[
\det:\mathbb{R}^{N\times N} \to \mathbb{R}
\]</span>
that maps a matrix to it’s <a href="https://en.wikipedia.org/wiki/Determinant">determinant</a>. Because the space <span class="math inline">\(\mathbb{R}^{N\times N}\)</span> of <span class="math inline">\(N\times N\)</span> matrices is just the Euclidean inner product space <span class="math inline">\(\mathbb{R}^{N^2}\)</span> with some extra algebraic structure, and because <span class="math inline">\(\det(A)\)</span> is a polynomial function of the elements of a matrix <span class="math inline">\(A\)</span>, <span class="math inline">\(\det\)</span> is a continuous function on <span class="math inline">\(\mathbb{R}^{N\times N}\)</span>.</p>
<p>By <a href="https://en.wikipedia.org/wiki/Cramer%27s_rule">Cramer’s Rule</a>, a matrix <span class="math inline">\(A\)</span> is invertible if and only if <span class="math inline">\(\det(A) \ne 0\)</span>. Because <span class="math inline">\(\det\)</span> is a continuous function on <span class="math inline">\(\mathbb{R}^{N\times N}\)</span> then the set of invertible matrices is an open subset of <span class="math inline">\(\mathbb{R}^{N\times N}\)</span>! Hence for every invertible matrix <span class="math inline">\(L\in\mathbb{R}^{N\times N}\)</span> and every arbitrary matrix <span class="math inline">\(A\in\mathbb{R}^{N\times N}\)</span> there exists an <span class="math inline">\(\epsilon_0 &gt; 0\)</span> such that for all <span class="math inline">\(\epsilon &lt; \epsilon_0\)</span> the matrix <span class="math inline">\(\epsilon A+L\)</span> is invertible. <strong>QED</strong></p>
<p>There’s a version of the theorem in Banach spaces, but we don’t need it.</p>
<p>Noticed that the only thing we needed about the matrix <span class="math inline">\(I\)</span> in the above proof was that it was invertible. Therefore, we never needed to restrict attention to just the identity matrix <span class="math inline">\(I\)</span>, but could have used any invertible matrix to regularize the problem:</p>
<p><span class="math display">\[
(A+\lambda L)x = y
\]</span></p>
<p>where <span class="math inline">\(L\)</span> is any convenient invertible matrix. Below, where we regularize OLS, we are not restricted to using the identity matrix <span class="math inline">\(I\)</span> to regularize, but can use any invertible symmetric matrix (preferably one that is positive definite so that a minimizer continues to exist).</p>
</div>
<div id="l2-regularization-of-ols" class="section level3">
<h3><span class="math inline">\(L^2\)</span>-regularization of OLS</h3>
<p>With this example we begin moving towards the statistical part of the post. One of the most widely known examples of regularization is what is often called <span class="math inline">\(L^2\)</span>-regularization, or <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov</a> regularization of Ordinary Least Squares.</p>
<p>Suppose <span class="math inline">\(Y\)</span> is a real valued random variable and <span class="math inline">\(\vec{X}\)</span> is a random vector with values in <span class="math inline">\(\mathbb{R}^p\)</span>. Suppose that we have the conditional relationship</p>
<p><span class="math display">\[
Y \ \ | \ \ \vec{X} \sim \mathcal{N}(\ \langle \vec{X}  , \beta\rangle \ ,\ \sigma^2)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span> denotes the univariate normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Here (and everywhere else) the symbol <span class="math inline">\(\langle v,w\rangle\)</span> represents the inner product of two vectors <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span>. This is the most natural probability model that leads to linear regression. In practice the parameters <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span> that specify this conditional distribution are unknown and it is desired that they be estimated from data.</p>
<p>In this canonical situation we assume that we have a data set <span class="math inline">\(\{ (Y_i, \vec{X}_i)\}_{i = 1}^N\)</span> consisting of samples generated independently of one another from a fixed multivariate distribution for <span class="math inline">\((Y, \vec{X})\)</span> (i.e. we assume our data was sampled IID). To fit the unknown parameters we use MLE. We may choose to use the conditional density <span class="math inline">\(p(Y|\vec{X})\)</span> in the likelihood and this would make it <strong><em>conditional</em></strong> MLE. Or we may choose the unconditional multivariate density <span class="math inline">\(p(Y, \vec{X})\)</span>. However, if we assume that the marginal distribution of <span class="math inline">\(\vec{X}\)</span>, (i.e. <span class="math inline">\(p(\vec{X})\)</span>), does not depend on either <span class="math inline">\(\beta\)</span> or <span class="math inline">\(\sigma\)</span> then because <span class="math inline">\(p(Y,\vec{X}) = p(Y|\vec{X})p(\vec{X})\)</span> building the likelihood using either <span class="math inline">\(p(Y|\vec{X})\)</span> or <span class="math inline">\(p(Y,\vec{X})\)</span> will lead to the same maximization problem because they differ by a constant factor (constant in <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span> that is). So we will use the conditional density <span class="math inline">\(p(Y|\vec{X})\)</span>.</p>
<p>Because the data is assumed to be generated IID then the full likelihood of the data is</p>
<p><span class="math display">\[
\mathcal{L}(\beta, \sigma\ |\ \{ (Y_i, \vec{X}_i)\}) = \prod_{i = 1}^N p(Y_i|\vec{X}_i) = \prod_{i=1}^N\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\big(\frac{Y_i - \langle \vec{X}_i  , \beta\rangle}{\sigma}\big)^2} = \frac{1}{(\sigma^22\pi)^{N/2}}e^{\sum_{i=1}^N-\frac{1}{2}\big(\frac{Y_i - \langle \vec{X}_i  , \beta\rangle}{\sigma}\big)^2}
\]</span>
Because the function <span class="math inline">\(f(x) := -\log(x)\)</span> is decreasing we may instead minimize the negative of the log of this expression:
<span class="math display">\[
-\log(\mathcal{L}(\beta,\sigma\ |\ \{ (Y_i, \vec{X}_i)\})) = \frac{N}{2}\log(\sigma^22\pi) + \frac{1}{2\sigma^2}\sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i  , \beta\rangle\bigg)^2
\]</span>
We first minimize with respect to <span class="math inline">\(\beta\)</span> as this is necessary to do first before finding the minimizing value of <span class="math inline">\(\sigma\)</span>. To do this we need to minimize the only term that depends on <span class="math inline">\(\beta\)</span>, namely the sum of squares <span class="math inline">\(SSE(\beta) := \sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)</span> (hence Least Squares regression).</p>
<div id="a-geometric-interlude" class="section level4">
<h4>A geometric interlude</h4>
<p>Before we do that, let’s think about what the expression <span class="math inline">\(SSE(\beta)\)</span> is. The term <span class="math inline">\(\langle \vec{X}_i , \beta\rangle\)</span> is linear in the unknowns <span class="math inline">\(\beta\)</span>, and hence so is <span class="math inline">\(Y_i - \langle \vec{X}_i , \beta\rangle\)</span>. Therefore, the square <span class="math inline">\(\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)</span> is quadratic in <span class="math inline">\(\beta\)</span>. Thus since the full expression <span class="math inline">\(\sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)</span> is a sum of quadratic functions in <span class="math inline">\(\beta\)</span> it too is a quadratic function in <span class="math inline">\(\beta\)</span>. Since all terms in the sum are squares, the full sum is never negative and its graph in <span class="math inline">\(\beta\)</span> is a <a href="https://en.wikipedia.org/wiki/Paraboloid">non-hyperbolic paraboloid</a>. Usually such shapes look like bowls. However, some can degenerate so that they become flat in one or more directions. Here’re some examples in R:</p>
<p>Non-degenarte paraboloids look like this:</p>
<pre class="r"><code>nice.paraboloid = function(x,y)
{
    return(x^2+0.5*y^2)
}
x = y = seq(from = -4, to = 4, by = 0.2)
z = outer(x, y, nice.paraboloid)

persp(x, y, z,
      main=&quot;Plot of a Non-degenerate 2D Paraboloid&quot;,
      theta = 30, phi = 15,
      col = &quot;springgreen&quot;, shade = 0.5)</code></pre>
<p><img src="/post/004_Regularization/main_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>We can see such a paraboloid is bowl shaped. More technically it’s strictly convex, with a clear unique minimum point. However, paraboloids can degenerate so that they flatten out in some directions:</p>
<pre class="r"><code>degenerate.paraboloid = function(x,y)
{
    return(x^2) #Does not change value as y changes
}
x = y = seq(from = -4, to = 4, by = 0.2)
z = outer(x, y, degenerate.paraboloid)

persp(x, y, z,
      main=&quot;Plot of a Degenerate 2D Paraboloid&quot;,
      theta = 30, phi = 15,
      col = &quot;springgreen&quot;, shade = 0.5)</code></pre>
<p><img src="/post/004_Regularization/main_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>In this example we changed the coefficient of <span class="math inline">\(y\)</span> from 0.5 to 0. The result is that in the <span class="math inline">\(y\)</span>-direction the paraboloid flattened out and it no longer looks bowl shaped. Instead there are infinitely many minimum points all on the axis <span class="math inline">\(\{(x,y): x = 0\}\)</span>. Note that if instead of making the coefficient of <span class="math inline">\(y\)</span> equal to 0 we had made it a positive number very close to zero then the mimima would become unique but would become hard to distinguish from nearby points:</p>
<pre class="r"><code>tricky.paraboloid = function(x,y)
{
    return(x^2+ 0.05*y^2) #Notice the coefficient of y is quite small
}
x = y = seq(from = -4, to = 4, by = 0.2)
z = outer(x, y, tricky.paraboloid)

persp(x, y, z,
      main=&quot;Plot of a Nearly-Degenerate 2D Paraboloid&quot;,
      theta = 30, phi = 15,
      col = &quot;springgreen&quot;, shade = 0.5)</code></pre>
<p><img src="/post/004_Regularization/main_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>These pictures show what can go wrong with the minima of quadratic functions like <span class="math inline">\(SSE(\beta)\)</span> and why regularization may be needed. Now to get back to the minimizing the sum of squares <span class="math inline">\(SSE(\beta) = \sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)</span>. If you’re reading this article I’m going to assume you’ve seen this derivation before so I’ll move a bit fast.</p>
<p>First we define <span class="math inline">\(Y\in \mathbb{R}^{N\times 1}\)</span> with <span class="math inline">\(i\)</span>-th component equal to <span class="math inline">\(Y_i\)</span>. (Note an abuse of notation we are making: earlier <span class="math inline">\(Y\)</span> denoted a real valued random variable, but now we are using the same symbol to denote the vector of the <span class="math inline">\(N\)</span> realizations of this random variable.) In addition, let <span class="math inline">\(X\in \mathbb{R}^{N\times p}\)</span> with <span class="math inline">\(i\)</span>-th row equal to <span class="math inline">\(\vec{X}_i\)</span>. Then in matrix notation</p>
<p><span class="math display">\[
\sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i  , \beta\rangle\bigg)^2 = (Y - X\beta)^T(Y - X\beta)
\]</span>
Let <span class="math inline">\(\hat{\beta} = \text{argmax}_{\beta} \ \ (Y - X\beta)^T(Y - X\beta)\)</span> be the sought after minimizer. Since <span class="math inline">\(\hat{\beta}\)</span> is a minizer in the interior of the domain of <span class="math inline">\(SSE(\beta)\)</span>, the gradient of <span class="math inline">\(SSE(\beta)\)</span> at <span class="math inline">\(\hat{\beta}\)</span> must be 0:</p>
<p><span class="math display">\[
-2X^TY + 2X^TX\hat{\beta} = 0
\]</span>
therefore we obtain the normal equations</p>
<p><span class="math display">\[
X^TX\hat{\beta} = X^TY
\]</span></p>
<p>This is basically the same linear algebra problem as before: If the inverse <span class="math inline">\((X^TX)^{-1}\)</span> existed and was numerically nice then we can solve for <span class="math inline">\(\hat{\beta} = (X^TX)^{-1}X^TY\)</span>. However, if this matrix inverse does not exist (as can happen when we do not have enough rows/samples for the given number of columns/unknowns) then this formula is not useful.</p>
<p>But as before we can simply regularize by replacing the matrix <span class="math inline">\(X^TX\)</span> by <span class="math inline">\(X^TX + \lambda I\)</span> for some sufficiently large <span class="math inline">\(\lambda\)</span>. Actually since <span class="math inline">\(X^TX\)</span> is non-negative definite<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> and symmetric all of it’s eigenvalues are non-negative. So any <span class="math inline">\(\lambda &gt; 0\)</span> would be sufficient to shift the eigenvalues into positive numbers. Now the regularized problem becomes <span class="math inline">\((X^TX + \lambda I)\hat{\beta} = X^TY\)</span>. Therefore we get the regularized MLE solution:</p>
<p><span class="math display">\[
\hat{\beta}_{reg} := (X^TX + \lambda I)^{-1}X^TY
\]</span></p>
<p>Does this regularized problem correspond to its own minimization problem? Yes! Working backwards, this new problem is equivalent to</p>
<p><span class="math display">\[
-2X^TY + 2X^TX\hat{\beta} + 2\lambda\hat{\beta}= 0
\]</span>
The left had side is the gradient of <span class="math inline">\((Y - X\beta)^T(Y - X\beta) + \lambda\sum_{j=1}^p\beta_j^2 = (Y - X\beta)^T(Y - X\beta) + \lambda \langle \beta, \beta\rangle\)</span> at <span class="math inline">\(\beta = \hat{\beta}\)</span> as can be checked. So the regularized problem corresponds to trying to minimize the expression <span class="math inline">\((Y - X\beta)^T(Y - X\beta) + \lambda\sum_{j=1}^p\beta_j^2 = (Y - X\beta)^T(Y - X\beta) + \lambda \langle \beta, \beta\rangle\)</span>. This of course is <span class="math inline">\(L^2\)</span>-regularization. Thus we have derived <span class="math inline">\(L^2\)</span>-regularization for OLS simply by seeking to transform the inverse problem that arises in OLS so that it may satisfy the Hadamard conditions.</p>
</div>
</div>
<div id="an-illustrative-example" class="section level3">
<h3>An Illustrative Example</h3>
<p>Below we can see geometrically what regularization does. The sum of squares expression <span class="math inline">\((Y - X\beta)^T(Y - X\beta)\)</span> is quadratic in <span class="math inline">\(\beta\)</span>, but may have a graph that is a degenerate paraboloid. This is what causes it to have multiple minimizers in OLS and what makes the matrix <span class="math inline">\(X^TX\)</span> singular (more on this point in the next section). However the expression <span class="math inline">\(\lambda\langle \beta, \beta\rangle = \lambda\sum_{j = 1}^p\beta_j^2\)</span> is a strictly positive-definite quadratic form. Its graph is a non-degenerate bowl shaped paraboloid.</p>
<p>Adding a non-degenerate paraboloid to something that is not bowl shaped makes the second graph more bowl shaped! Moreover it shifts the minimum of the 2nd graph towards the mimimum of the bowl. As an illustration, let’s take a look at an example where this is easy to see.</p>
<pre class="r"><code>rm(list = ls())
bumpy.function = function(x,y)
{
    return(sin(x)+sin(y))
}

nice.paraboloid = function(x,y)
{
    return(0.15*(x^2 + y^2)) #lambda = 0.15 is used
}

x = y = seq(from = -4, to = 4, by = 0.2)
bumpy = outer(x, y, bumpy.function)
paraboloid = outer(x, y, nice.paraboloid)
bumpy.plus.paraboloid = bumpy + paraboloid

persp(x, y, bumpy,
      main=&quot;Graph of a bumpy function with multiple minima&quot;,
      theta = 30, phi = 15,
      col = &quot;springgreen&quot;, shade = 0.5)</code></pre>
<p><img src="/post/004_Regularization/main_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>persp(x, y, paraboloid,
      main=&quot;Graph of a nice paraboloid&quot;,
      theta = 30, phi = 15,
      col = &quot;springgreen&quot;, shade = 0.5)</code></pre>
<p><img src="/post/004_Regularization/main_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<pre class="r"><code>persp(x, y, bumpy.plus.paraboloid,
      main=&quot;Graph of a regularized bumpy function = bumpy function + paraboloid&quot;,
      theta = 30, phi = 15,
      col = &quot;springgreen&quot;, shade = 0.5)</code></pre>
<p><img src="/post/004_Regularization/main_files/figure-html/unnamed-chunk-4-3.png" width="672" /></p>
<p>Here we see that the most important geometric aspect of the regularizing term <span class="math inline">\(\lambda\langle\beta, \beta\rangle\)</span> is the fact that it is <a href="https://en.wikipedia.org/wiki/Convex_function">strictly convex</a>!! <strong><em>Although we will not dwell on it, it is impossible to overstate the theoretical importance of the previous sentence.</em></strong> As a matter of fact, geometrically speaking it’s clear that had we added <strong>any</strong> strictly convex function to the bumpy function we would have gotten something more bowl shaped. We will not go further into it here but you should know that <a href="https://en.wikipedia.org/wiki/Convex_analysis">convexity is one of those properties in mathematics out of which entire fields are created</a>.</p>
</div>
<div id="the-hessian-matrix-and-more-complex-models" class="section level3">
<h3>The Hessian matrix and more complex models</h3>
<p>In the OLS problem above the Maximum Likelihood estimator turned out to be the one that minimized the sum of squares expression <span class="math inline">\(SSE(\beta) := (Y - X\beta)^T(Y - X\beta)\)</span>. This expression can be expanded in matrix notation as</p>
<p><span class="math display">\[
Y^TY - Y^TX\beta - \beta^TX^TY + \beta^TX^TX\beta
\]</span></p>
<p>We see that there is a quadratic term (<span class="math inline">\(\beta^TX^TX\beta\)</span>) and the rest are terms with powers of <span class="math inline">\(\beta\)</span> less than 2. The matrix of second derivatives of this expression (known as the <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian matrix</a>) is therefore just the matrix of coefficients of this quadratic term: <span class="math inline">\(X^TX\)</span>. This Hessian is exactly what was the star of the show in the OLS problem!</p>
<p>The Hessian of a function at a point tells us the convexity of the function at the point. <a href="https://en.wikipedia.org/wiki/Second_partial_derivative_test">If the Hessian is positive definite, then near the minimizing point the function is bowl shaped. If the Hessian is negative definite then near a maximizing point the function is shaped like an upside down bowl.</a></p>
<p>Moreover, The Hessian is always a symmetric matrix by the <a href="https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives#Formal_expressions_of_symmetry">equality of cross-derivatives</a>. So the previous point is really a statement about Hessian’s eigenvalues. In short, if the Hessian has all positive eigenvalues then it is positive definite and the function is bowl shaped near its minimizer.</p>
<p>The OLS Hessian matrix <span class="math inline">\(X^TX\)</span> is symmetric and non-negative definite, so the graph of the sum of squares <span class="math inline">\(SSE(\beta) := (Y - X\beta)^T(Y - X\beta)\)</span> can only fail to be bowl shaped near the minimizer <span class="math inline">\(\hat{\beta}\)</span> if the matrix <span class="math inline">\(X^TX\)</span> has eigenvalues that are equal 0 (or positive but close to 0 in the case numerical instability). In which case, the graph of <span class="math inline">\(SSE(\beta)\)</span> is a degenerate non-hyperbolic paraboloid and there are multiple minimizing solutions <span class="math inline">\(\beta\)</span>.</p>
<p><strong>Thus regularizing the matrix <span class="math inline">\(X^TX\)</span> is just regularizing the Hessian matrix of the function <span class="math inline">\(SSE(\beta)\)</span> we want to minimize!!</strong> This fact is what allows us to take the idea beyond OLS.</p>
<p>Indeed if <span class="math inline">\(f(\beta)\)</span> is any 2nd order differentiable cost function in any machine learning model then by the linearity of the derivative</p>
<p><span class="math display">\[
\text{Hessian}(f + \lambda \langle \beta, \beta\rangle) = \text{Hessian}(f) + \lambda\text{Hessian}(\langle \beta, \beta\rangle) = \text{Hessian}(f) + \lambda I
\]</span></p>
<p>There we used the fact that <span class="math inline">\(\text{Hessian}(\langle \beta, \beta\rangle) = \text{Hessian}(\sum_i^p \beta_i^2) = I\)</span>. Because <span class="math inline">\(f\)</span> is almost arbitrary we see that we can apply <span class="math inline">\(L^2\)</span>-regularization to a very large family of problems, with the goal being to regularize the Hessian of <span class="math inline">\(f\)</span>. As an example let’s look at some other kinds of regression problems. For OLS we assumed the conditional distribution <span class="math inline">\(Y \ \ | \ \ \vec{X} \sim \mathcal{N}(\ \langle \vec{X} , \beta\rangle \ ,\ \sigma^2)\)</span>, but we may have chosen a different conditional distribution.</p>
<p>If <span class="math inline">\(Y\)</span> takes on only values in the set <span class="math inline">\(\{0,1\}\)</span> then it is a Bernoulli random variable. This is the case in logistic regression where the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\vec{X}\)</span> is
<span class="math display">\[
Y \ \ | \ \ \vec{X} \sim \mathcal{B}(\ p = \phi(\langle \vec{X}  , \beta\rangle) \ )
\]</span></p>
<p>where <span class="math inline">\(\mathcal{B(p)}\)</span> is a Bernoulli distribution with probability of a positive event equal to <span class="math inline">\(p\)</span>, <span class="math inline">\(p = \text{E}[Y|\vec{X}] = \phi(\langle \vec{X} , \beta\rangle)\)</span> is the probability of a positive event given <span class="math inline">\(\vec{X}\)</span>, and <span class="math inline">\(\phi(t) = \frac{e^t}{1+e^t}\)</span> is the standard logit. In this case the conditional density <span class="math inline">\(p(Y|\vec{X})\)</span> can be written as</p>
<p><span class="math display">\[
p(Y|\vec{X}) = p^Y\big(1 - p)\big)^{1-Y} =\phi(\langle \vec{X}  , \beta\rangle)^Y\big(1 - \phi(\langle \vec{X}  , \beta\rangle)\big)^{1-Y}
\]</span>
So the negative log-likelihood is given by</p>
<p><span class="math display">\[
-\mathcal{l}(\beta) = -\sum_{i=1}^N Y_i\log(\phi(\langle \vec{X}_i  , \beta\rangle)) + (1-Y_i)\log(1-\phi(\langle \vec{X}_i  , \beta\rangle))
\]</span></p>
<p>This function may or may not look bowl shaped (i.e. strictly convex) near the <span class="math inline">\(\hat{\beta}\)</span> that minimizes it. In case it doesn’t we can make it so by adding <span class="math inline">\(\lambda \langle \beta, \beta\rangle = \lambda\sum_{j = 1}^p\beta_j^2\)</span> for some sufficiently large <span class="math inline">\(\lambda\)</span> and minimizing this new problem. The same applies to generalized linear models, neural networks, etc.</p>
</div>
</div>
<div id="bias-variance-trade-offs-and-regularization" class="section level2">
<h2>Bias-Variance trade offs and Regularization</h2>
<p>Above we used regularization methods to make a problem “nicer” in the numerical sense (i.e. satisfying Hadamard’s conditions). But what does “nicer” mean in the statistical context? That is a multifaceted question. The first step is to recognize that what might be viewed as instability from the numerical point of view, can be understood as high variance from the statistical point of view.</p>
<p>We illustrate with the OLS estimator. Suppose that the matrix <span class="math inline">\(X^TX\)</span> is indeed invertible. The standard OLS estimator is the random vector given by the normal equations:</p>
<p><span class="math display">\[
\hat{\beta} = (X^TX)^{-1}X^TY
\]</span></p>
<p>We see that this is an unbiased estimator of <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\text{E}[\hat{\beta}] = \text{E}\big[\text{E}[\hat{\beta}|X]\big] = \text{E}\bigg[(X^TX)^{-1}X^T\text{E}[Y|X]\bigg] = \text{E}\bigg[(X^TX)^{-1}X^TX\beta\bigg] = \text{E}[\beta] = \beta
\]</span></p>
<p>Moreover, it’s easy enough to compute the conditional covariance matrix of <span class="math inline">\(\hat{\beta}\)</span>:
<span class="math display">\[
\text{Var}[\hat{\beta}|X] = (X^TX)^{-1}X^T\cdot \text{Var}[Y|X] \cdot X(X^TX)^{-1} = (X^TX)^{-1}X^T\cdot \sigma^2 I \cdot X(X^TX)^{-1} 
\]</span></p>
<p><span class="math display">\[
= \sigma^2 (X^TX)^{-1}
\]</span></p>
<p>The <a href="https://en.wikipedia.org/wiki/Law_of_total_variance">unconditional covariance matrix can be computed as</a></p>
<p><span class="math display">\[
\text{Var}[\hat{\beta}] = \text{E}[\text{Var}[\hat{\beta}|X]] + \text{Var}[\text{E}[\hat{\beta}|X]] 
\]</span></p>
<p><span class="math display">\[
= \sigma^2\text{E}[(X^TX)^{-1}] + \text{Var}[\beta] = \sigma^2\text{E}[(X^TX)^{-1}]
\]</span></p>
<p>This is harder to compute because it depends on the distribution of the random matrix <span class="math inline">\(X\)</span>. Regardless, we can see that what controls the variance of the estimator <span class="math inline">\(\hat{\beta}\)</span> (whether conditional or not) is the inverse matrix <span class="math inline">\((X^TX)^{-1}\)</span>. This is interesting because it shows that the matrix we identified as the Hessian of the OLS cost function (<span class="math inline">\(X^TX\)</span>) is also the matrix that controls the covariance of the OLS estimator.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>If any of the eigenvalues of the matrix <span class="math inline">\(X^TX\)</span> were “close” to <span class="math inline">\(0\)</span> then the eigenvalues of the inverse will be very large, causing the variance of <span class="math inline">\(\hat{\beta}\)</span> to be very large. If you’re familiar with VIFs, this is what causes large <a href="https://en.wikipedia.org/wiki/Variance_inflation_factor">variance inflation factors</a>.</p>
<p>Regularization is used to reduce the variance in this estimator. If we denote the regularized estimator by:
<span class="math display">\[
\hat{\beta}_{reg} = (X^TX + \lambda I)^{-1}X^TY
\]</span>
Then this estimator is biased away from <span class="math inline">\(\beta\)</span>. To see this we first compute the conditional mean:</p>
<p><span class="math display">\[
\text{E}[\hat{\beta}_{reg}|X] = (X^TX + \lambda I)^{-1}X^TX\beta
\]</span></p>
<p><span class="math display">\[
= (X^TX + \lambda I)^{-1}(X^TX + \lambda I)\beta - \lambda (X^TX + \lambda I)^{-1}\beta
\]</span></p>
<p><span class="math display">\[
= \beta - \lambda (X^TX + \lambda I)^{-1}\beta
\]</span></p>
<p>Hence
<span class="math display">\[
\text{E}[\hat{\beta}_{reg}] = \beta - \lambda\text{E}\big[(X^TX + \lambda I)^{-1}\big]\beta
\]</span>
which is “<span class="math inline">\(\beta\)</span> minus something” and hence not equal to <span class="math inline">\(\beta\)</span>. However the effect on the variance is better:</p>
<p><span class="math display">\[
\text{Var}[\hat{\beta}_{reg}|X] = (X^TX + \lambda I)^{-1}X^T\cdot\text{Var}[Y|X]\cdot X(X^TX + \lambda I)^{-1}
\]</span></p>
<p><span class="math display">\[
= \sigma^2(X^TX + \lambda I)^{-1}X^TX(X^TX + \lambda I)^{-1}
\]</span></p>
<p><span class="math display">\[
= \sigma^2(X^TX + \lambda I)^{-1}(X^TX+\lambda I)(X^TX + \lambda I)^{-1} - \sigma^2\lambda(X^TX + \lambda I)^{-2}
\]</span></p>
<p><span class="math display">\[
=  \sigma^2(X^TX + \lambda I)^{-1} - \sigma^2\lambda(X^TX + \lambda I)^{-2}
\]</span>
This variance formula may look messy but the gist is that instead of inverting <span class="math inline">\(X^TX\)</span> we are inverting <span class="math inline">\(X^TX + \lambda I\)</span>. The matrix <span class="math inline">\(X^TX + \lambda I\)</span> has larger eigenvalues than the matrix <span class="math inline">\(X^TX\)</span>. Therefore <span class="math inline">\(\text{Var}[\hat{\beta}_{reg}|X] = \sigma^2(X^TX + \lambda I)^{-1} - \sigma^2\lambda(X^TX + \lambda I)^{-2}\)</span> is smaller than <span class="math inline">\(\text{Var}[\hat{\beta}|X] = \sigma^2 (X^TX)^{-1}\)</span> in the sense that it has smaller eigenvalues. <strong>Thus regularization has increased bias, but reduced variance.</strong> Similar effects hold for more complex models than OLS, but instead of chasing formulas the read should try cooking up some numerical examples via Monte Carlo.</p>
</div>
<div id="what-about-the-bayesian-view-point" class="section level2">
<h2>What about the Bayesian view point?</h2>
<p>A very natural perspective on regularization can be found in Bayesian modeling, where regularization terms amount to simply specifying prior distributions. However, this is standard Bayesian theory and this post is already long enough :P</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Indeed suppose <span class="math inline">\(v\in \mathbb{R}^p\)</span> is any vector. Then <span class="math inline">\(\langle v, X^TXv\rangle = \langle Xv,Xv\rangle = ||Xv||^2 \ge 0\)</span>. Hence <span class="math inline">\(X^TX\)</span> is always non-negative definite.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>This is a general feature of Maximum Likelihood estimators called “<em>asymptotic efficiency</em>”, where the covariance matrix of the MLE estimator approaches a “best possible” covariance matrix as the sample size increases. Essentially the best possible covariance matrix that an unbiased estimator of <span class="math inline">\(\beta\)</span> can have is given given by the <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">Cramer-Rao Bound</a> and is determined by the inverse of the <a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher Information matrix</a>, whose <span class="math inline">\(ij\)</span>-component is <span class="math inline">\(-\text{E}[\partial^2\log(p(Y, \vec{X}| \beta))/\partial \beta_i\partial \beta_j]\)</span>. The beauty is that the Hessian of the negative loglikelihood is the sample estimator of this Fisher Information (where expectation is replaced by average over samples). This is why the Hessian is showing up as the determining factor in estimator covariance.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
</content:encoded>	
    </item>
    
    <item>
      <title>Passing expressions and data from R to C&#43;&#43; before compile-time in Rmarkdown</title>
      <link>/post/002_compile_time_data_r_to_cpp/main/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/002_compile_time_data_r_to_cpp/main/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post we give a simple illustrative example of how data generated by R code can be used by compiled languages such as C++ at compile time, instead of run-time, inside Rmarkdown.&lt;/p&gt;
&lt;p&gt;This is an example of inter-language code generation. Metaprogramming/code generation is an extremely powerful technique but it’s also one that is very easy to overdo. This is just a fun example to learn from. Thorough testing is very important for any production code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-other-languages-in-rmarkdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using other languages in Rmarkdown&lt;/h2&gt;
&lt;p&gt;Out of the box Rmarkdown can work with the following languages assuming a proper back-end is available:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(knitr::knit_engines$get())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;awk&amp;quot;         &amp;quot;bash&amp;quot;        &amp;quot;coffee&amp;quot;      &amp;quot;gawk&amp;quot;        &amp;quot;groovy&amp;quot;     
##  [6] &amp;quot;haskell&amp;quot;     &amp;quot;lein&amp;quot;        &amp;quot;mysql&amp;quot;       &amp;quot;node&amp;quot;        &amp;quot;octave&amp;quot;     
## [11] &amp;quot;perl&amp;quot;        &amp;quot;psql&amp;quot;        &amp;quot;Rscript&amp;quot;     &amp;quot;ruby&amp;quot;        &amp;quot;sas&amp;quot;        
## [16] &amp;quot;scala&amp;quot;       &amp;quot;sed&amp;quot;         &amp;quot;sh&amp;quot;          &amp;quot;stata&amp;quot;       &amp;quot;zsh&amp;quot;        
## [21] &amp;quot;highlight&amp;quot;   &amp;quot;Rcpp&amp;quot;        &amp;quot;tikz&amp;quot;        &amp;quot;dot&amp;quot;         &amp;quot;c&amp;quot;          
## [26] &amp;quot;fortran&amp;quot;     &amp;quot;fortran95&amp;quot;   &amp;quot;asy&amp;quot;         &amp;quot;cat&amp;quot;         &amp;quot;asis&amp;quot;       
## [31] &amp;quot;stan&amp;quot;        &amp;quot;block&amp;quot;       &amp;quot;block2&amp;quot;      &amp;quot;js&amp;quot;          &amp;quot;css&amp;quot;        
## [36] &amp;quot;sql&amp;quot;         &amp;quot;go&amp;quot;          &amp;quot;python&amp;quot;      &amp;quot;julia&amp;quot;       &amp;quot;sass&amp;quot;       
## [41] &amp;quot;scss&amp;quot;        &amp;quot;theorem&amp;quot;     &amp;quot;lemma&amp;quot;       &amp;quot;corollary&amp;quot;   &amp;quot;proposition&amp;quot;
## [46] &amp;quot;conjecture&amp;quot;  &amp;quot;definition&amp;quot;  &amp;quot;example&amp;quot;     &amp;quot;exercise&amp;quot;    &amp;quot;proof&amp;quot;      
## [51] &amp;quot;remark&amp;quot;      &amp;quot;solution&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although we can use R’s native foreign function interface to call compiled code, for C++ a higher level alternative is to use &lt;a href=&#34;https://cran.r-project.org/web/packages/Rcpp/index.html&#34;&gt;Rcpp&lt;/a&gt;. In Rmarkdown we can &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/language-engines.html#rcpp&#34;&gt;compile C++ code chunks using Rcpp and export the compiled functions to be available for use in R&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As a common example, we can compile the following code&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector timesTwo(NumericVector x) 
{
    return x * 2;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and use the exported function in R&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;timesTwo(1:10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  2  4  6  8 10 12 14 16 18 20&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;registering-a-user-defined-language-engine-in-knitr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Registering a user-defined language engine in Knitr&lt;/h2&gt;
&lt;p&gt;We can create &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/custom-engine.html&#34;&gt;user-defined engines&lt;/a&gt; to control exactly how the code chunk is sourced, or even modify existing engines. To get an idea we can look at the default Rcpp engine used by knitr:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::knit_engines$get()$Rcpp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (options) 
## {
##     sourceCpp = getFromNamespace(&amp;quot;sourceCpp&amp;quot;, &amp;quot;Rcpp&amp;quot;)
##     code = one_string(options$code)
##     opts = options$engine.opts
##     cache = options$cache &amp;amp;&amp;amp; (&amp;quot;cacheDir&amp;quot; %in% names(formals(sourceCpp)))
##     if (cache) {
##         opts$cacheDir = paste(valid_path(options$cache.path, 
##             options$label), &amp;quot;sourceCpp&amp;quot;, sep = &amp;quot;_&amp;quot;)
##         opts$cleanupCacheDir = TRUE
##     }
##     if (!is.environment(opts$env)) 
##         opts$env = knit_global()
##     if (options$eval) {
##         message(&amp;quot;Building shared library for Rcpp code chunk...&amp;quot;)
##         do.call(sourceCpp, c(list(code = code), opts))
##     }
##     options$engine = &amp;quot;cpp&amp;quot;
##     engine_output(options, code, &amp;quot;&amp;quot;)
## }
## &amp;lt;environment: namespace:knitr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the default engine above as a template we can define a new knitr engine for compiling C++. One that can read and make use of more dynamic R data in C++ before compilation (or even dynamically create &lt;code&gt;Makevars&lt;/code&gt; files to control compilation flags). First let’s include the knitr package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(knitr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next let’s take a crack at defining a new engine to compile C++ code. In this example we will modify the current Rcpp engine to take in an &lt;code&gt;extra&lt;/code&gt; field (but otherwise behave the same).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knit_engines$set(RcppFoo = function(options) {
    
    extra = options$extra
    
    sourceCpp = getFromNamespace(&amp;quot;sourceCpp&amp;quot;, &amp;quot;Rcpp&amp;quot;)
    
    ## Code is read as a list of strings, one list element per line
    ## Here we append extra code that may be defined in R to the 
    ## code written in the chunk
    code = c(extra, options$code)
    code = paste(code, collapse = &amp;#39;\n&amp;#39;)
    opts = options$engine.opts
    
    if (!is.environment(opts$env)) 
        opts$env = knit_global()

    if (options$eval) {    
        message(&amp;quot;Building shared library for Rcpp code chunk...&amp;quot;)
        do.call(sourceCpp, c(list(code = code), opts))
    }
    options$engine = &amp;quot;cpp&amp;quot;
    engine_output(options, 
                  options$code, 
                  paste(&amp;quot;Added the lines:\n&amp;quot;, 
                      paste(extra, collapse = &amp;#39;\n&amp;#39;), 
                      sep = &amp;#39;\n&amp;#39;))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we test by creating some data in R and using that as a compile time constant in C++. Here we pass values of pi and e as static const doubles to C++ (a much cleaner API is possible of course).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;constants = list(
    paste(&amp;#39;static const double Pi =&amp;#39;, pi, &amp;#39;;&amp;#39;),
    paste(&amp;#39;static const double Euler =&amp;#39;, exp(1),&amp;#39;;&amp;#39;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This already highlights a danger as we have not considered exactly how R might convert these double precision floating point numbers to strings. Regardless, we proceed. To use the new engine we run the engine as &lt;code&gt;{RcppFoo test_chunk, extra = constants}&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector timesFoo(NumericVector x) 
{
    return x * Pi + Euler;
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Added the lines:
## 
## static const double Pi = 3.14159265358979 ;
## static const double Euler = 2.71828182845905 ;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = timesFoo(1:10)
print(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  5.859874  9.001467 12.143060 15.284652 18.426245 21.567838 24.709430
##  [8] 27.851023 30.992616 34.134208&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get &lt;strong&gt;almost&lt;/strong&gt; the same result as in R&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y = pi*(1:10)+exp(1)
print(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  5.859874  9.001467 12.143060 15.284652 18.426245 21.567838 24.709430
##  [8] 27.851023 30.992616 34.134208&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But metaprogramming can be dangerous when mixed with floating point arithmetic. In this case some loss of precision occurred with the doubles when converting to strings:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x - y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1.776357e-15  0.000000e+00 -3.552714e-15 -7.105427e-15 -1.065814e-14
##  [6] -1.421085e-14 -1.776357e-14 -1.776357e-14 -2.131628e-14 -2.842171e-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.double(as.character(pi))*(1:10) + as.double(as.character(exp(1))) - x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
##  [6] 3.552714e-15 3.552714e-15 0.000000e+00 0.000000e+00 0.000000e+00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Anyway this was just a small example. There are many many directions one can choose to take with metaprogramming. Even creating new preprocessing directives such as unrolling loops, defining constexprs, etc.&lt;/p&gt;
&lt;/div&gt;
</description>
      <content:encoded>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In this post we give a simple illustrative example of how data generated by R code can be used by compiled languages such as C++ at compile time, instead of run-time, inside Rmarkdown.</p>
<p>This is an example of inter-language code generation. Metaprogramming/code generation is an extremely powerful technique but it’s also one that is very easy to overdo. This is just a fun example to learn from. Thorough testing is very important for any production code.</p>
</div>
<div id="using-other-languages-in-rmarkdown" class="section level2">
<h2>Using other languages in Rmarkdown</h2>
<p>Out of the box Rmarkdown can work with the following languages assuming a proper back-end is available:</p>
<pre class="r"><code>names(knitr::knit_engines$get())</code></pre>
<pre><code>##  [1] &quot;awk&quot;         &quot;bash&quot;        &quot;coffee&quot;      &quot;gawk&quot;        &quot;groovy&quot;     
##  [6] &quot;haskell&quot;     &quot;lein&quot;        &quot;mysql&quot;       &quot;node&quot;        &quot;octave&quot;     
## [11] &quot;perl&quot;        &quot;psql&quot;        &quot;Rscript&quot;     &quot;ruby&quot;        &quot;sas&quot;        
## [16] &quot;scala&quot;       &quot;sed&quot;         &quot;sh&quot;          &quot;stata&quot;       &quot;zsh&quot;        
## [21] &quot;highlight&quot;   &quot;Rcpp&quot;        &quot;tikz&quot;        &quot;dot&quot;         &quot;c&quot;          
## [26] &quot;fortran&quot;     &quot;fortran95&quot;   &quot;asy&quot;         &quot;cat&quot;         &quot;asis&quot;       
## [31] &quot;stan&quot;        &quot;block&quot;       &quot;block2&quot;      &quot;js&quot;          &quot;css&quot;        
## [36] &quot;sql&quot;         &quot;go&quot;          &quot;python&quot;      &quot;julia&quot;       &quot;sass&quot;       
## [41] &quot;scss&quot;        &quot;theorem&quot;     &quot;lemma&quot;       &quot;corollary&quot;   &quot;proposition&quot;
## [46] &quot;conjecture&quot;  &quot;definition&quot;  &quot;example&quot;     &quot;exercise&quot;    &quot;proof&quot;      
## [51] &quot;remark&quot;      &quot;solution&quot;</code></pre>
<p>Although we can use R’s native foreign function interface to call compiled code, for C++ a higher level alternative is to use <a href="https://cran.r-project.org/web/packages/Rcpp/index.html">Rcpp</a>. In Rmarkdown we can <a href="https://bookdown.org/yihui/rmarkdown/language-engines.html#rcpp">compile C++ code chunks using Rcpp and export the compiled functions to be available for use in R</a>.</p>
<p>As a common example, we can compile the following code</p>
<pre class="cpp"><code>#include &lt;Rcpp.h&gt;
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector timesTwo(NumericVector x) 
{
    return x * 2;
}</code></pre>
<p>and use the exported function in R</p>
<pre class="r"><code>timesTwo(1:10)</code></pre>
<pre><code>##  [1]  2  4  6  8 10 12 14 16 18 20</code></pre>
</div>
<div id="registering-a-user-defined-language-engine-in-knitr" class="section level2">
<h2>Registering a user-defined language engine in Knitr</h2>
<p>We can create <a href="https://bookdown.org/yihui/rmarkdown-cookbook/custom-engine.html">user-defined engines</a> to control exactly how the code chunk is sourced, or even modify existing engines. To get an idea we can look at the default Rcpp engine used by knitr:</p>
<pre class="r"><code>knitr::knit_engines$get()$Rcpp</code></pre>
<pre><code>## function (options) 
## {
##     sourceCpp = getFromNamespace(&quot;sourceCpp&quot;, &quot;Rcpp&quot;)
##     code = one_string(options$code)
##     opts = options$engine.opts
##     cache = options$cache &amp;&amp; (&quot;cacheDir&quot; %in% names(formals(sourceCpp)))
##     if (cache) {
##         opts$cacheDir = paste(valid_path(options$cache.path, 
##             options$label), &quot;sourceCpp&quot;, sep = &quot;_&quot;)
##         opts$cleanupCacheDir = TRUE
##     }
##     if (!is.environment(opts$env)) 
##         opts$env = knit_global()
##     if (options$eval) {
##         message(&quot;Building shared library for Rcpp code chunk...&quot;)
##         do.call(sourceCpp, c(list(code = code), opts))
##     }
##     options$engine = &quot;cpp&quot;
##     engine_output(options, code, &quot;&quot;)
## }
## &lt;environment: namespace:knitr&gt;</code></pre>
<p>Using the default engine above as a template we can define a new knitr engine for compiling C++. One that can read and make use of more dynamic R data in C++ before compilation (or even dynamically create <code>Makevars</code> files to control compilation flags). First let’s include the knitr package:</p>
<pre class="r"><code>library(knitr)</code></pre>
<p>Next let’s take a crack at defining a new engine to compile C++ code. In this example we will modify the current Rcpp engine to take in an <code>extra</code> field (but otherwise behave the same).</p>
<pre class="r"><code>knit_engines$set(RcppFoo = function(options) {
    
    extra = options$extra
    
    sourceCpp = getFromNamespace(&quot;sourceCpp&quot;, &quot;Rcpp&quot;)
    
    ## Code is read as a list of strings, one list element per line
    ## Here we append extra code that may be defined in R to the 
    ## code written in the chunk
    code = c(extra, options$code)
    code = paste(code, collapse = &#39;\n&#39;)
    opts = options$engine.opts
    
    if (!is.environment(opts$env)) 
        opts$env = knit_global()

    if (options$eval) {    
        message(&quot;Building shared library for Rcpp code chunk...&quot;)
        do.call(sourceCpp, c(list(code = code), opts))
    }
    options$engine = &quot;cpp&quot;
    engine_output(options, 
                  options$code, 
                  paste(&quot;Added the lines:\n&quot;, 
                      paste(extra, collapse = &#39;\n&#39;), 
                      sep = &#39;\n&#39;))
})</code></pre>
<p>Next we test by creating some data in R and using that as a compile time constant in C++. Here we pass values of pi and e as static const doubles to C++ (a much cleaner API is possible of course).</p>
<pre class="r"><code>constants = list(
    paste(&#39;static const double Pi =&#39;, pi, &#39;;&#39;),
    paste(&#39;static const double Euler =&#39;, exp(1),&#39;;&#39;)
)</code></pre>
<p>This already highlights a danger as we have not considered exactly how R might convert these double precision floating point numbers to strings. Regardless, we proceed. To use the new engine we run the engine as <code>{RcppFoo test_chunk, extra = constants}</code></p>
<pre class="cpp"><code>#include &lt;Rcpp.h&gt;
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector timesFoo(NumericVector x) 
{
    return x * Pi + Euler;
}</code></pre>
<pre><code>## Added the lines:
## 
## static const double Pi = 3.14159265358979 ;
## static const double Euler = 2.71828182845905 ;</code></pre>
<pre class="r"><code>x = timesFoo(1:10)
print(x)</code></pre>
<pre><code>##  [1]  5.859874  9.001467 12.143060 15.284652 18.426245 21.567838 24.709430
##  [8] 27.851023 30.992616 34.134208</code></pre>
<p>We get <strong>almost</strong> the same result as in R</p>
<pre class="r"><code>y = pi*(1:10)+exp(1)
print(y)</code></pre>
<pre><code>##  [1]  5.859874  9.001467 12.143060 15.284652 18.426245 21.567838 24.709430
##  [8] 27.851023 30.992616 34.134208</code></pre>
<p>But metaprogramming can be dangerous when mixed with floating point arithmetic. In this case some loss of precision occurred with the doubles when converting to strings:</p>
<pre class="r"><code>x - y</code></pre>
<pre><code>##  [1]  1.776357e-15  0.000000e+00 -3.552714e-15 -7.105427e-15 -1.065814e-14
##  [6] -1.421085e-14 -1.776357e-14 -1.776357e-14 -2.131628e-14 -2.842171e-14</code></pre>
<pre class="r"><code>as.double(as.character(pi))*(1:10) + as.double(as.character(exp(1))) - x</code></pre>
<pre><code>##  [1] 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
##  [6] 3.552714e-15 3.552714e-15 0.000000e+00 0.000000e+00 0.000000e+00</code></pre>
<p>Anyway this was just a small example. There are many many directions one can choose to take with metaprogramming. Even creating new preprocessing directives such as unrolling loops, defining constexprs, etc.</p>
</div>
</content:encoded>	
    </item>
    
    <item>
      <title>Deriving Principal Component Analysis and implementing in C&#43;&#43; using Eigen</title>
      <link>/post/001_deriving_pca/main/</link>
      <pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/001_deriving_pca/main/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;Principal component analysis&lt;/a&gt; is one of the most commonly used techniques in statistical modeling and machine learning. In typical applications it serves as a (linear) dimensionality reduction, allowing one to project high dimensional data onto a lower dimensional subspace. This can help make a problem that was previously computationally intractable easier, or can help transform feature variables into something more useful. However, most presentations fail to give a sense of “why” and students are left without an understanding of exactly what PCA is and what assumptions it makes. This can lead to model risk issues and prevent users from being able to modify the technique when different assumptions hold. The purpose of this post is to rectify this with a derivation for those that want to know why, which should be everyone. For fun we implement what we learn at the end in a few lines of C++.&lt;/p&gt;
&lt;div id=&#34;a-note-on-difficulty&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A note on difficulty&lt;/h3&gt;
&lt;p&gt;To understand what follows you need to understand linear algebra and undergraduate probability. &lt;strong&gt;The proof that follows is as clear, honest, and self-contained as I think is possible, but most will not find it easy&lt;/strong&gt;. In my opinion if a truly easy and theoretically honest proof were possible you would have already seen it.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;deriving-pca&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deriving PCA&lt;/h2&gt;
&lt;p&gt;As scientists our data is often times multidimensional because it involves measurements of many features of the world. Equally often, our data may have some “randomness” in it that we can not capture (so that if the experiment that was run to obtain the data were rerun the results may not be exactly the same).&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\vec{X} = [X^1, X^2, ..., X^d]\)&lt;/span&gt; be a &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-dimensional random vector &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; that represents the measured values of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; feature variables.&lt;/p&gt;
&lt;p&gt;We want to capture the “shape” of the randomness of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;. For example, in what directions does &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; vary the most? In what directions does it vary the least? This is important because if, for example, &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; had a lot of randomness in its first coordinate &lt;span class=&#34;math inline&#34;&gt;\(X^1\)&lt;/span&gt;, but had very little randomness in the other coordinates, then independent measurements of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; would differ a lot in the first coordinate, but not much in the others. The other coordinates would all give roughly the same values and hence roughly the same information. The other coordinates would in a sense be redundant: replacing &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(X^1\)&lt;/span&gt; would not lose a lot of information but would have the benefit of having to deal with only 1 feature as opposed to &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; features (i.e. a dimensionality reduction).&lt;/p&gt;
&lt;p&gt;To proceed we need to define some measure of variation or randomness. A good one is variance. Our goal is to decompose &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; into vectors along which &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; has the most variance. Directions are represented by unit vectors (i.e. vectors of length 1). If &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; is a non-random unit vector, then the component of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; along &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle\ \vec{\omega}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle\)&lt;/span&gt; denotes the inner product in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^d\)&lt;/span&gt; (aka, dot product). Since &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; is not random, the randomness of &lt;span class=&#34;math inline&#34;&gt;\(\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle\ \vec{\omega}\)&lt;/span&gt; is controlled entirely by the coefficient &lt;span class=&#34;math inline&#34;&gt;\(\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle\)&lt;/span&gt;. To find the direction of maximal variance is to simply find &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; that maximizes the variance of this inner product. In other words we want&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{\omega}_1 := \text{argmax} \ \ \text{Var}( \langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle ) 
\]&lt;/span&gt;
where the argmax is taken over all &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(||\vec{\omega}|| = 1\)&lt;/span&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. We begin:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \begin{aligned}

\text{Var}( \langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle )  &amp;amp;= 
\text{E}\bigg[\bigg(\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle - \text{E}[\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle]\bigg)^2\bigg] \\


    &amp;amp;= \text{E}[\langle\ \vec{\omega}\ , \ \vec{X} - \text{E}[\vec{X}] \ \rangle^2] \\
    
    
    &amp;amp;= \text{E}\bigg[\ \bigg(\sum_i\omega_i(X^i - \text{E}[X^i])\bigg)^2\bigg] \\
    
    
    &amp;amp;= \text{E}\bigg[ \sum_{i,j}\omega_i\omega_j(X^i - \text{E}[X^i])(X^j - \text{E}[X^j]) \bigg] \\
    
    
    &amp;amp;= \sum_{i,j}\omega_i\omega_j \ \text{E}\bigg[ \ (X^i - \text{E}[X^i])(X^j - \text{E}[X^j]) \ \bigg] \\
    
    
    &amp;amp;= \sum_{i,j}\omega_i\omega_j \ \text{Cov}(X^i, X^j) \\
    
    
    &amp;amp;= \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle
  \end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\vec{X})\)&lt;/span&gt; is the covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;. So&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{\omega}_1 := \text{argmax} \ \ \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle 
\]&lt;/span&gt;
again the argmax is taken over all &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(||\vec{\omega}|| = 1\)&lt;/span&gt;. This problem is called a “variational problem”, but why so is not important at the moment.&lt;/p&gt;
&lt;p&gt;This &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; must be the first eigenvector of the matrix &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\vec{X})\)&lt;/span&gt;. Why? &lt;strong&gt;This is the hard part. If you can understand what follows you’re golden&lt;/strong&gt;. There are multiple ways to see why this is the case:&lt;/p&gt;
&lt;p&gt;One is by Lagrange multipliers. If we write &lt;span class=&#34;math inline&#34;&gt;\(f(\vec{\omega}) := \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle\)&lt;/span&gt; then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\begin{aligned}

f(\vec{\omega} + \vec{h}) - f(\vec{\omega}) &amp;amp;=  \langle \ \vec{\omega} + \vec{h} \ , \ \text{Cov}(\vec{X})(\vec{\omega}+\vec{h}) \ \rangle - \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle \\

    &amp;amp;= \langle \ \vec{h} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle + \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{h}\rangle \ + \langle \ \vec{h} \ , \ \text{Cov}(\vec{X})\vec{h} \rangle \\
    
    &amp;amp;= 2\langle \ \vec{h} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle + \langle \ \vec{h} \ , \ \text{Cov}(\vec{X})\vec{h} \rangle

\end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we first expanded the first term using the bilinearity of the inner product, canceled like terms, and lastly used the symmetry of the covariance matrix to combine two terms. In the above expression the first order term in &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}\)&lt;/span&gt; is given by &lt;span class=&#34;math inline&#34;&gt;\(\langle \ \vec{h} \ , \ 2\text{Cov}(\vec{X})\vec{\omega} \ \rangle\)&lt;/span&gt;. The other term is quadratic in &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}\)&lt;/span&gt;. By definition the differential of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; is this linear term:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
df_{\vec{\omega}} \ (\vec{h}) = \langle \ \vec{h} \ , \ 2\text{Cov}(\vec{X})\vec{\omega} \ \rangle
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By definition&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; is just the vector in the above expression which the inner product with &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}\)&lt;/span&gt; is being taken:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nabla_{\vec{\omega}} f = 2 \ \text{Cov}(\vec{X}) \ \vec{\omega}
\]&lt;/span&gt;
Because our variational problem is to maximize &lt;span class=&#34;math inline&#34;&gt;\(f(\vec{\omega})\)&lt;/span&gt; on the unit sphere where &lt;span class=&#34;math inline&#34;&gt;\(||\vec{\omega}|| = 1\)&lt;/span&gt;, then the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at the maximizing point &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; must be orthogonal (i.e. perpendicular, i.e. normal) to the surface of the unit sphere at that point&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. The direction (i.e. unit vector) perpendicular to the unit sphere at &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; itself with its starting point translated to the surface!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/Spherical_unit_vectors.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Thus the gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; must be collinear with (and hence a multiple of) &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nabla_{\vec{\omega}_1}f  = \lambda&amp;#39; \ \vec{\omega}_1
\]&lt;/span&gt;
for some number &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;#39;\)&lt;/span&gt;. Thus&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Cov}(\vec{X}) \ \vec{\omega}_1 = \frac{\lambda&amp;#39;}{2} \ \vec{\omega}_1 =: \lambda \ \vec{\omega}_1
\]&lt;/span&gt;
Hence &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; is an eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\vec{X})\)&lt;/span&gt;. We note that the eigenvalue &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is just the variance we wanted to maximize:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\langle \ \vec{\omega}_1 \ , \ \text{Cov}(\vec{X}) \ \vec{\omega}_1 \ \rangle = \langle \ \vec{\omega}_1 \ , \lambda \vec{\omega}_1 \ \rangle = \lambda\langle \ \vec{\omega}_1 \ , \vec{\omega}_1 \ \rangle = \lambda ||\vec{\omega}||^2 = \lambda
\]&lt;/span&gt;
Thus we see that eigenvectors capture directions of maximal variance and eigenvalues capture the value of the variance in that maximal direction! We can also see why the variance is a nice measure of variation/randomness. Because it’s &lt;strong&gt;quadratic&lt;/strong&gt; in its arguments, derivatives of it become &lt;strong&gt;linear&lt;/strong&gt;, leading to &lt;strong&gt;linear&lt;/strong&gt; eigenvalue problems, which are very well understood by mathematicians.&lt;/p&gt;
&lt;p&gt;We proceed as follows. Let &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}_{new} = \vec{X} - \langle\vec{\omega}_1,\vec{X}\rangle\vec{\omega}_1\)&lt;/span&gt;. This &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}_{new}\)&lt;/span&gt; is just the component of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; orthogonal to &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt;. Intuitively it’s the part of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; that &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; can not explain.&lt;/p&gt;
&lt;p&gt;Just as before we want to capture the direction of maximal variance of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}_{new}\)&lt;/span&gt;. I.e. we want a vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_2\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(||\vec{\omega}_2||=1\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\langle \vec{\omega}_2, \vec{X}_{new}\rangle)\)&lt;/span&gt; is maximal.&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}_{new} \perp \vec{\omega}_1\)&lt;/span&gt;&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; then &lt;span class=&#34;math inline&#34;&gt;\(\langle \vec{\omega}, \vec{X}_{new}\rangle = \langle \vec{\omega} - \alpha\vec{\omega}_1, \vec{X}_{new}\rangle\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in \mathbb{R}\)&lt;/span&gt;. Therefore by replacing &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega} - \langle\vec{\omega}_1,\vec{\omega}\rangle\vec{\omega}_1\)&lt;/span&gt; we may restrict our maximization problem to maximizing &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\langle \vec{\omega}, \vec{X}_{new}\rangle)\)&lt;/span&gt; over all &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(||\vec{\omega}|| = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega} \perp \vec{\omega}_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We transform this expression as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\begin{aligned}

\text{Var}(\langle \vec{\omega}, \vec{X}_{new}\rangle) &amp;amp;= \text{Var}(\langle \vec{\omega}, \vec{X} - \langle\vec{\omega}_1,\vec{X}\rangle\vec{\omega}_1\rangle) \\

    &amp;amp;= \text{Var}(\langle \vec{\omega}, \vec{X}\rangle) \qquad \text{Since }\vec{\omega}\perp\vec{\omega}_1 \\
    
    &amp;amp;= \langle \vec{\omega}, \text{Cov}(\vec{X})\vec{\omega}\rangle \qquad \text{By the earlier computation}

\end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So the vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_2\)&lt;/span&gt; is given by the new variational problem&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{\omega}_2 = \text{argmax} \ \langle \vec{\omega}, \text{Cov}(\vec{X})\vec{\omega}\rangle
\]&lt;/span&gt;
where the argmax is taken over all &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(||\vec{\omega}|| = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega} \perp \vec{\omega}_1\)&lt;/span&gt;. Now &lt;span class=&#34;math inline&#34;&gt;\(\langle \vec{\omega}_2, \vec{X}_{new}\rangle = \langle \vec{\omega}_2, \vec{X}\rangle\)&lt;/span&gt; is of maximal variance in a direction perpendicular to &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Notice that this is the same maximization problem as before, but now restricted to a lower dimensional subspace (the subspace that is prependicular to &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt;). The same Lagrange multiplier calculation as before can be applied again in this subspace. This shows that &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_2\)&lt;/span&gt; is an eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\vec{X})\)&lt;/span&gt; with eigenvalue &lt;span class=&#34;math inline&#34;&gt;\(\langle \vec{\omega}_2, \text{Cov}(\vec{X})\vec{\omega}_2\rangle\)&lt;/span&gt;. This eigenvalue must be less than or equal to the eigenvalue of &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; because the maximum of the same expression is being taken over a smaller set for &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can continue this process until all eigenvectors are exhausted. By decomposing &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; into linear combinations of the eigenvectors &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_i\)&lt;/span&gt; we may choose to capture as much or as little of the variance of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; as we please. For example, by projecting onto the first k eigenvectors we may capture the k-dimensional variance of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{X}_k := \sum_{i = 1}^k\langle\vec{\omega}_i,\vec{X}\rangle\vec{\omega}_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sample-estimators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sample estimators&lt;/h2&gt;
&lt;p&gt;In practice we do not know the matrix &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\vec{X})\)&lt;/span&gt;, but instead have a data matrix &lt;span class=&#34;math inline&#34;&gt;\(\{ \vec{X}_j \}_{j=1}^N\)&lt;/span&gt; of row vectors representing realizations of the random vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Statistics is often concerned with constructing sample estimators of quantities. If our data rows are sampled IID from the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; then in lieu of &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(X^i,X^j)\)&lt;/span&gt; we construct the sample covariances:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S^2_{i,j} := \frac{1}{N-1}\sum_{n=1}^N\bigg(X^i_n - \bar{X}^i\bigg)\bigg(X^j_n - \bar{X}^j\bigg)
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}^i\)&lt;/span&gt; is the mean of the &lt;span class=&#34;math inline&#34;&gt;\(i^{\text{th}}\)&lt;/span&gt; feature column. This estimator is a statistic constructed for its favorable distributional properties under IID assumptions as &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; becomes large. In particular, it converges to &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(X^i,X^j)\)&lt;/span&gt; in some sense.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementing-in-eigen&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implementing in Eigen&lt;/h2&gt;
&lt;p&gt;The derivation above gives us one formula to carry out PCA: simply compute the sample covariance matrix of the data and extract its eigenvectors and eigenvalues. This may or may not be the most numerically efficient/stable algorithm to use (I haven’t checked), but this is easy enough to implement in most numerical computing languages. Here we implement it in C++ using the &lt;a href=&#34;http://eigen.tuxfamily.org/&#34;&gt;Eigen&lt;/a&gt; library. To make it more interactive we use the &lt;a href=&#34;https://cran.r-project.org/web/packages/RcppEigen/index.html&#34;&gt;RcppEigen&lt;/a&gt; package in R to allow using the function in R sessions:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;RcppEigen.h&amp;gt;

// [[Rcpp::depends(RcppEigen)]]

using namespace Eigen;


// [[Rcpp::export]]
Rcpp::List EigenDecomp(const Map&amp;lt;MatrixXd&amp;gt; M) 
{
    //Constructing sample covariance matrix 
    MatrixXd centered = M.rowwise() - M.colwise().mean();
    MatrixXd cov = centered.adjoint() * centered/(M.rows()-1);
    
    //Using Eigen&amp;#39;s eigensolver (with default settings)
    SelfAdjointEigenSolver&amp;lt;MatrixXd&amp;gt; eig(cov);
    
    VectorXd values = eig.eigenvalues();
    MatrixXd vectors = eig.eigenvectors();
    
    //Returning results as a R-list
    return Rcpp::List::create(Rcpp::Named(&amp;quot;Cov&amp;quot;) = cov,
                           Rcpp::Named(&amp;quot;values&amp;quot;) = values,
                           Rcpp::Named(&amp;quot;vectors&amp;quot;) = vectors);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note on compilation: I’m using a laptop with an i7-8750h CPU running Windows 10. The compiler is the version of &lt;a href=&#34;http://mingw-w64.org/doku.php&#34;&gt;mingw-w64&lt;/a&gt; that comes with &lt;a href=&#34;https://cran.r-project.org/bin/windows/Rtools/&#34;&gt;Rtools40&lt;/a&gt; (i.e. the Windows port of GCC 8.3.0). By creating a Makevars.win file in an &lt;code&gt;./Documents/.R&lt;/code&gt; folder file I altered R’s default flags for g++ to use:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CXXFLAGS = -march=native -O3 -Wno-ignored-attributes $(DEBUGFLAG)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Eigen is a template expression library that relies heavily on the compiler using the best options for the machine at hand. Here we’ve used &lt;code&gt;-march=native&lt;/code&gt; which enables all instruction subsets supported by my local machine. For more info running &lt;code&gt;g++ -march=native -Q --help=target&lt;/code&gt; in the command line will show you what compiler flags this turns on. For example mine enables flags targeting AVX2, as well as a variety of others. The &lt;code&gt;-Wno-ignored-attributes&lt;/code&gt; suppresses the large number of ignored attributes warnings that an expression template library like Eigen can produce. Let’s compare with R’s built in PCA function &lt;a href=&#34;https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp&#34;&gt;prcomp&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
X = matrix(rnorm(10000*4), 10000, 4)

R = prcomp(X)

Cpp = EigenDecomp(X)

print(R$sdev^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.036884 1.021022 1.013685 1.001778&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(Cpp$values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.001778 1.013685 1.021022 1.036884&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The eigenvalues are exactly the same, just in opposite order. Next time we might link an optimized BLAS library such as Intel’s MKL, but I suspect the plain Eigen version is quite competitive.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The exact definition of “random variable” or “random vector” is unimportant. For mathematicians this means that there is a probability space &lt;span class=&#34;math inline&#34;&gt;\((\Omega, \mathcal{M}, \mathbf{P})\)&lt;/span&gt; and that &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}:\Omega \mapsto \mathbb{R}^d\)&lt;/span&gt; is a Borel-measurable map.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Note on existence. A vector that attains the maximum must exist because the expression being maximized is continuous (in fact quadratic) in &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}\)&lt;/span&gt; and the unit sphere in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^d\)&lt;/span&gt; is compact.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;And this is indeed the true definition of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient#Differential_or_(exterior)_derivative&#34;&gt;gradient of a function&lt;/a&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;This is the method of &lt;a href=&#34;https://en.wikipedia.org/wiki/Lagrange_multiplier#Modern_formulation_via_differentiable_manifolds&#34;&gt;Lagrange multipliers&lt;/a&gt;. It can be proven easily as follows. Let &lt;span class=&#34;math inline&#34;&gt;\(\vec{v}\)&lt;/span&gt; be any vector tangent to the sphere at the maximizing point &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(\gamma(t)\)&lt;/span&gt; be a smooth curve on the sphere going through &lt;span class=&#34;math inline&#34;&gt;\(\vec{\omega}_1\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\gamma&amp;#39;(t) = \vec{v}\)&lt;/span&gt;. Then the function &lt;span class=&#34;math inline&#34;&gt;\(f(\gamma(t))\)&lt;/span&gt; achieves a maximum at the value of &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; at which &lt;span class=&#34;math inline&#34;&gt;\(\gamma(t) = \vec{\omega}_1\)&lt;/span&gt; so it’s derivative must be 0 there. Thus &lt;span class=&#34;math inline&#34;&gt;\(0 = d/dt(f(\gamma(t))) = df_{\vec{\omega}_1} \ (\gamma&amp;#39;(t)) = \langle \gamma&amp;#39;(t),\nabla_{\vec{\omega}_1}f\rangle = \langle\vec{v},\nabla_{\vec{\omega}_1}f\rangle\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(\vec{v}\)&lt;/span&gt; was an arbitrary tangent vector this shows that &lt;span class=&#34;math inline&#34;&gt;\(\nabla_{\vec{\omega}_1}f\)&lt;/span&gt; is orthogonal to every tangent vector and hence is a normal vector.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;The expression &lt;span class=&#34;math inline&#34;&gt;\(A\perp B\)&lt;/span&gt; denotes “&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is perpendicular to &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;”.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      <content:encoded>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p><a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal component analysis</a> is one of the most commonly used techniques in statistical modeling and machine learning. In typical applications it serves as a (linear) dimensionality reduction, allowing one to project high dimensional data onto a lower dimensional subspace. This can help make a problem that was previously computationally intractable easier, or can help transform feature variables into something more useful. However, most presentations fail to give a sense of “why” and students are left without an understanding of exactly what PCA is and what assumptions it makes. This can lead to model risk issues and prevent users from being able to modify the technique when different assumptions hold. The purpose of this post is to rectify this with a derivation for those that want to know why, which should be everyone. For fun we implement what we learn at the end in a few lines of C++.</p>
<div id="a-note-on-difficulty" class="section level3">
<h3>A note on difficulty</h3>
<p>To understand what follows you need to understand linear algebra and undergraduate probability. <strong>The proof that follows is as clear, honest, and self-contained as I think is possible, but most will not find it easy</strong>. In my opinion if a truly easy and theoretically honest proof were possible you would have already seen it.</p>
</div>
</div>
<div id="deriving-pca" class="section level2">
<h2>Deriving PCA</h2>
<p>As scientists our data is often times multidimensional because it involves measurements of many features of the world. Equally often, our data may have some “randomness” in it that we can not capture (so that if the experiment that was run to obtain the data were rerun the results may not be exactly the same).</p>
<p>Let <span class="math inline">\(\vec{X} = [X^1, X^2, ..., X^d]\)</span> be a <span class="math inline">\(d\)</span>-dimensional random vector <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> that represents the measured values of <span class="math inline">\(d\)</span> feature variables.</p>
<p>We want to capture the “shape” of the randomness of <span class="math inline">\(\vec{X}\)</span>. For example, in what directions does <span class="math inline">\(\vec{X}\)</span> vary the most? In what directions does it vary the least? This is important because if, for example, <span class="math inline">\(\vec{X}\)</span> had a lot of randomness in its first coordinate <span class="math inline">\(X^1\)</span>, but had very little randomness in the other coordinates, then independent measurements of <span class="math inline">\(\vec{X}\)</span> would differ a lot in the first coordinate, but not much in the others. The other coordinates would all give roughly the same values and hence roughly the same information. The other coordinates would in a sense be redundant: replacing <span class="math inline">\(\vec{X}\)</span> by <span class="math inline">\(X^1\)</span> would not lose a lot of information but would have the benefit of having to deal with only 1 feature as opposed to <span class="math inline">\(d\)</span> features (i.e. a dimensionality reduction).</p>
<p>To proceed we need to define some measure of variation or randomness. A good one is variance. Our goal is to decompose <span class="math inline">\(\vec{X}\)</span> into vectors along which <span class="math inline">\(\vec{X}\)</span> has the most variance. Directions are represented by unit vectors (i.e. vectors of length 1). If <span class="math inline">\(\vec{\omega}\)</span> is a non-random unit vector, then the component of <span class="math inline">\(\vec{X}\)</span> along <span class="math inline">\(\vec{\omega}\)</span> is given by</p>
<p><span class="math display">\[
\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle\ \vec{\omega}
\]</span>
where <span class="math inline">\(\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle\)</span> denotes the inner product in <span class="math inline">\(\mathbb{R}^d\)</span> (aka, dot product). Since <span class="math inline">\(\vec{\omega}\)</span> is not random, the randomness of <span class="math inline">\(\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle\ \vec{\omega}\)</span> is controlled entirely by the coefficient <span class="math inline">\(\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle\)</span>. To find the direction of maximal variance is to simply find <span class="math inline">\(\vec{\omega}\)</span> that maximizes the variance of this inner product. In other words we want</p>
<p><span class="math display">\[
\vec{\omega}_1 := \text{argmax} \ \ \text{Var}( \langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle ) 
\]</span>
where the argmax is taken over all <span class="math inline">\(\vec{\omega}\)</span> with <span class="math inline">\(||\vec{\omega}|| = 1\)</span><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. We begin:</p>
<p><span class="math display">\[\begin{equation}
  \begin{aligned}

\text{Var}( \langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle )  &amp;= 
\text{E}\bigg[\bigg(\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle - \text{E}[\langle\ \vec{\omega}\ ,\ \vec{X}\ \rangle]\bigg)^2\bigg] \\


    &amp;= \text{E}[\langle\ \vec{\omega}\ , \ \vec{X} - \text{E}[\vec{X}] \ \rangle^2] \\
    
    
    &amp;= \text{E}\bigg[\ \bigg(\sum_i\omega_i(X^i - \text{E}[X^i])\bigg)^2\bigg] \\
    
    
    &amp;= \text{E}\bigg[ \sum_{i,j}\omega_i\omega_j(X^i - \text{E}[X^i])(X^j - \text{E}[X^j]) \bigg] \\
    
    
    &amp;= \sum_{i,j}\omega_i\omega_j \ \text{E}\bigg[ \ (X^i - \text{E}[X^i])(X^j - \text{E}[X^j]) \ \bigg] \\
    
    
    &amp;= \sum_{i,j}\omega_i\omega_j \ \text{Cov}(X^i, X^j) \\
    
    
    &amp;= \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle
  \end{aligned}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\text{Cov}(\vec{X})\)</span> is the covariance matrix of <span class="math inline">\(\vec{X}\)</span>. So</p>
<p><span class="math display">\[
\vec{\omega}_1 := \text{argmax} \ \ \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle 
\]</span>
again the argmax is taken over all <span class="math inline">\(\vec{\omega}\)</span> with <span class="math inline">\(||\vec{\omega}|| = 1\)</span>. This problem is called a “variational problem”, but why so is not important at the moment.</p>
<p>This <span class="math inline">\(\vec{\omega}_1\)</span> must be the first eigenvector of the matrix <span class="math inline">\(\text{Cov}(\vec{X})\)</span>. Why? <strong>This is the hard part. If you can understand what follows you’re golden</strong>. There are multiple ways to see why this is the case:</p>
<p>One is by Lagrange multipliers. If we write <span class="math inline">\(f(\vec{\omega}) := \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle\)</span> then</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}

f(\vec{\omega} + \vec{h}) - f(\vec{\omega}) &amp;=  \langle \ \vec{\omega} + \vec{h} \ , \ \text{Cov}(\vec{X})(\vec{\omega}+\vec{h}) \ \rangle - \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle \\

    &amp;= \langle \ \vec{h} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle + \langle \ \vec{\omega} \ , \ \text{Cov}(\vec{X})\vec{h}\rangle \ + \langle \ \vec{h} \ , \ \text{Cov}(\vec{X})\vec{h} \rangle \\
    
    &amp;= 2\langle \ \vec{h} \ , \ \text{Cov}(\vec{X})\vec{\omega} \ \rangle + \langle \ \vec{h} \ , \ \text{Cov}(\vec{X})\vec{h} \rangle

\end{aligned}
\end{equation}\]</span></p>
<p>where we first expanded the first term using the bilinearity of the inner product, canceled like terms, and lastly used the symmetry of the covariance matrix to combine two terms. In the above expression the first order term in <span class="math inline">\(\vec{h}\)</span> is given by <span class="math inline">\(\langle \ \vec{h} \ , \ 2\text{Cov}(\vec{X})\vec{\omega} \ \rangle\)</span>. The other term is quadratic in <span class="math inline">\(\vec{h}\)</span>. By definition the differential of <span class="math inline">\(f\)</span> at <span class="math inline">\(\vec{\omega}\)</span> is this linear term:</p>
<p><span class="math display">\[
df_{\vec{\omega}} \ (\vec{h}) = \langle \ \vec{h} \ , \ 2\text{Cov}(\vec{X})\vec{\omega} \ \rangle
\]</span></p>
<p>By definition<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> the gradient of <span class="math inline">\(f\)</span> at <span class="math inline">\(\vec{\omega}\)</span> is just the vector in the above expression which the inner product with <span class="math inline">\(\vec{h}\)</span> is being taken:</p>
<p><span class="math display">\[
\nabla_{\vec{\omega}} f = 2 \ \text{Cov}(\vec{X}) \ \vec{\omega}
\]</span>
Because our variational problem is to maximize <span class="math inline">\(f(\vec{\omega})\)</span> on the unit sphere where <span class="math inline">\(||\vec{\omega}|| = 1\)</span>, then the gradient of <span class="math inline">\(f\)</span> at the maximizing point <span class="math inline">\(\vec{\omega}_1\)</span> must be orthogonal (i.e. perpendicular, i.e. normal) to the surface of the unit sphere at that point<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. The direction (i.e. unit vector) perpendicular to the unit sphere at <span class="math inline">\(\vec{\omega}_1\)</span> is <span class="math inline">\(\vec{\omega}_1\)</span> itself with its starting point translated to the surface!</p>
<p><img src="/img/Spherical_unit_vectors.png" /></p>
<p>Thus the gradient of <span class="math inline">\(f\)</span> at <span class="math inline">\(\vec{\omega}_1\)</span> must be collinear with (and hence a multiple of) <span class="math inline">\(\vec{\omega}_1\)</span>:</p>
<p><span class="math display">\[
\nabla_{\vec{\omega}_1}f  = \lambda&#39; \ \vec{\omega}_1
\]</span>
for some number <span class="math inline">\(\lambda&#39;\)</span>. Thus</p>
<p><span class="math display">\[
\text{Cov}(\vec{X}) \ \vec{\omega}_1 = \frac{\lambda&#39;}{2} \ \vec{\omega}_1 =: \lambda \ \vec{\omega}_1
\]</span>
Hence <span class="math inline">\(\vec{\omega}_1\)</span> is an eigenvector of <span class="math inline">\(\text{Cov}(\vec{X})\)</span>. We note that the eigenvalue <span class="math inline">\(\lambda\)</span> is just the variance we wanted to maximize:</p>
<p><span class="math display">\[
\langle \ \vec{\omega}_1 \ , \ \text{Cov}(\vec{X}) \ \vec{\omega}_1 \ \rangle = \langle \ \vec{\omega}_1 \ , \lambda \vec{\omega}_1 \ \rangle = \lambda\langle \ \vec{\omega}_1 \ , \vec{\omega}_1 \ \rangle = \lambda ||\vec{\omega}||^2 = \lambda
\]</span>
Thus we see that eigenvectors capture directions of maximal variance and eigenvalues capture the value of the variance in that maximal direction! We can also see why the variance is a nice measure of variation/randomness. Because it’s <strong>quadratic</strong> in its arguments, derivatives of it become <strong>linear</strong>, leading to <strong>linear</strong> eigenvalue problems, which are very well understood by mathematicians.</p>
<p>We proceed as follows. Let <span class="math inline">\(\vec{X}_{new} = \vec{X} - \langle\vec{\omega}_1,\vec{X}\rangle\vec{\omega}_1\)</span>. This <span class="math inline">\(\vec{X}_{new}\)</span> is just the component of <span class="math inline">\(\vec{X}\)</span> orthogonal to <span class="math inline">\(\vec{\omega}_1\)</span>. Intuitively it’s the part of <span class="math inline">\(\vec{X}\)</span> that <span class="math inline">\(\vec{\omega}_1\)</span> can not explain.</p>
<p>Just as before we want to capture the direction of maximal variance of <span class="math inline">\(\vec{X}_{new}\)</span>. I.e. we want a vector <span class="math inline">\(\vec{\omega}_2\)</span> with <span class="math inline">\(||\vec{\omega}_2||=1\)</span> such that <span class="math inline">\(\text{Var}(\langle \vec{\omega}_2, \vec{X}_{new}\rangle)\)</span> is maximal.</p>
<p>Since <span class="math inline">\(\vec{X}_{new} \perp \vec{\omega}_1\)</span><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> then <span class="math inline">\(\langle \vec{\omega}, \vec{X}_{new}\rangle = \langle \vec{\omega} - \alpha\vec{\omega}_1, \vec{X}_{new}\rangle\)</span> for any <span class="math inline">\(\alpha \in \mathbb{R}\)</span>. Therefore by replacing <span class="math inline">\(\vec{\omega}\)</span> with <span class="math inline">\(\vec{\omega} - \langle\vec{\omega}_1,\vec{\omega}\rangle\vec{\omega}_1\)</span> we may restrict our maximization problem to maximizing <span class="math inline">\(\text{Var}(\langle \vec{\omega}, \vec{X}_{new}\rangle)\)</span> over all <span class="math inline">\(\vec{\omega}\)</span> with <span class="math inline">\(||\vec{\omega}|| = 1\)</span> and <span class="math inline">\(\vec{\omega} \perp \vec{\omega}_1\)</span>.</p>
<p>We transform this expression as follows:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}

\text{Var}(\langle \vec{\omega}, \vec{X}_{new}\rangle) &amp;= \text{Var}(\langle \vec{\omega}, \vec{X} - \langle\vec{\omega}_1,\vec{X}\rangle\vec{\omega}_1\rangle) \\

    &amp;= \text{Var}(\langle \vec{\omega}, \vec{X}\rangle) \qquad \text{Since }\vec{\omega}\perp\vec{\omega}_1 \\
    
    &amp;= \langle \vec{\omega}, \text{Cov}(\vec{X})\vec{\omega}\rangle \qquad \text{By the earlier computation}

\end{aligned}
\end{equation}\]</span></p>
<p>So the vector <span class="math inline">\(\vec{\omega}_2\)</span> is given by the new variational problem</p>
<p><span class="math display">\[
\vec{\omega}_2 = \text{argmax} \ \langle \vec{\omega}, \text{Cov}(\vec{X})\vec{\omega}\rangle
\]</span>
where the argmax is taken over all <span class="math inline">\(\vec{\omega}\)</span> with <span class="math inline">\(||\vec{\omega}|| = 1\)</span> and <span class="math inline">\(\vec{\omega} \perp \vec{\omega}_1\)</span>. Now <span class="math inline">\(\langle \vec{\omega}_2, \vec{X}_{new}\rangle = \langle \vec{\omega}_2, \vec{X}\rangle\)</span> is of maximal variance in a direction perpendicular to <span class="math inline">\(\vec{\omega}_1\)</span>.</p>
<p>Notice that this is the same maximization problem as before, but now restricted to a lower dimensional subspace (the subspace that is prependicular to <span class="math inline">\(\vec{\omega}_1\)</span>). The same Lagrange multiplier calculation as before can be applied again in this subspace. This shows that <span class="math inline">\(\vec{\omega}_2\)</span> is an eigenvector of <span class="math inline">\(\text{Cov}(\vec{X})\)</span> with eigenvalue <span class="math inline">\(\langle \vec{\omega}_2, \text{Cov}(\vec{X})\vec{\omega}_2\rangle\)</span>. This eigenvalue must be less than or equal to the eigenvalue of <span class="math inline">\(\vec{\omega}_1\)</span> because the maximum of the same expression is being taken over a smaller set for <span class="math inline">\(\vec{\omega}_2\)</span>.</p>
<p>We can continue this process until all eigenvectors are exhausted. By decomposing <span class="math inline">\(\vec{X}\)</span> into linear combinations of the eigenvectors <span class="math inline">\(\vec{\omega}_i\)</span> we may choose to capture as much or as little of the variance of <span class="math inline">\(\vec{X}\)</span> as we please. For example, by projecting onto the first k eigenvectors we may capture the k-dimensional variance of <span class="math inline">\(\vec{X}\)</span>:</p>
<p><span class="math display">\[
\vec{X}_k := \sum_{i = 1}^k\langle\vec{\omega}_i,\vec{X}\rangle\vec{\omega}_i
\]</span></p>
</div>
<div id="sample-estimators" class="section level2">
<h2>Sample estimators</h2>
<p>In practice we do not know the matrix <span class="math inline">\(\text{Cov}(\vec{X})\)</span>, but instead have a data matrix <span class="math inline">\(\{ \vec{X}_j \}_{j=1}^N\)</span> of row vectors representing realizations of the random vector <span class="math inline">\(\vec{X}\)</span>.</p>
<p>Statistics is often concerned with constructing sample estimators of quantities. If our data rows are sampled IID from the distribution of <span class="math inline">\(\vec{X}\)</span> then in lieu of <span class="math inline">\(\text{Cov}(X^i,X^j)\)</span> we construct the sample covariances:</p>
<p><span class="math display">\[
S^2_{i,j} := \frac{1}{N-1}\sum_{n=1}^N\bigg(X^i_n - \bar{X}^i\bigg)\bigg(X^j_n - \bar{X}^j\bigg)
\]</span>
where <span class="math inline">\(\bar{X}^i\)</span> is the mean of the <span class="math inline">\(i^{\text{th}}\)</span> feature column. This estimator is a statistic constructed for its favorable distributional properties under IID assumptions as <span class="math inline">\(N\)</span> becomes large. In particular, it converges to <span class="math inline">\(\text{Cov}(X^i,X^j)\)</span> in some sense.</p>
</div>
<div id="implementing-in-eigen" class="section level2">
<h2>Implementing in Eigen</h2>
<p>The derivation above gives us one formula to carry out PCA: simply compute the sample covariance matrix of the data and extract its eigenvectors and eigenvalues. This may or may not be the most numerically efficient/stable algorithm to use (I haven’t checked), but this is easy enough to implement in most numerical computing languages. Here we implement it in C++ using the <a href="http://eigen.tuxfamily.org/">Eigen</a> library. To make it more interactive we use the <a href="https://cran.r-project.org/web/packages/RcppEigen/index.html">RcppEigen</a> package in R to allow using the function in R sessions:</p>
<pre class="cpp"><code>#include &lt;RcppEigen.h&gt;

// [[Rcpp::depends(RcppEigen)]]

using namespace Eigen;


// [[Rcpp::export]]
Rcpp::List EigenDecomp(const Map&lt;MatrixXd&gt; M) 
{
    //Constructing sample covariance matrix 
    MatrixXd centered = M.rowwise() - M.colwise().mean();
    MatrixXd cov = centered.adjoint() * centered/(M.rows()-1);
    
    //Using Eigen&#39;s eigensolver (with default settings)
    SelfAdjointEigenSolver&lt;MatrixXd&gt; eig(cov);
    
    VectorXd values = eig.eigenvalues();
    MatrixXd vectors = eig.eigenvectors();
    
    //Returning results as a R-list
    return Rcpp::List::create(Rcpp::Named(&quot;Cov&quot;) = cov,
                           Rcpp::Named(&quot;values&quot;) = values,
                           Rcpp::Named(&quot;vectors&quot;) = vectors);
}</code></pre>
<p>Note on compilation: I’m using a laptop with an i7-8750h CPU running Windows 10. The compiler is the version of <a href="http://mingw-w64.org/doku.php">mingw-w64</a> that comes with <a href="https://cran.r-project.org/bin/windows/Rtools/">Rtools40</a> (i.e. the Windows port of GCC 8.3.0). By creating a Makevars.win file in an <code>./Documents/.R</code> folder file I altered R’s default flags for g++ to use:</p>
<pre><code>CXXFLAGS = -march=native -O3 -Wno-ignored-attributes $(DEBUGFLAG)</code></pre>
<p>Eigen is a template expression library that relies heavily on the compiler using the best options for the machine at hand. Here we’ve used <code>-march=native</code> which enables all instruction subsets supported by my local machine. For more info running <code>g++ -march=native -Q --help=target</code> in the command line will show you what compiler flags this turns on. For example mine enables flags targeting AVX2, as well as a variety of others. The <code>-Wno-ignored-attributes</code> suppresses the large number of ignored attributes warnings that an expression template library like Eigen can produce. Let’s compare with R’s built in PCA function <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp">prcomp</a></p>
<pre class="r"><code>set.seed(42)
X = matrix(rnorm(10000*4), 10000, 4)

R = prcomp(X)

Cpp = EigenDecomp(X)

print(R$sdev^2)</code></pre>
<pre><code>## [1] 1.036884 1.021022 1.013685 1.001778</code></pre>
<pre class="r"><code>print(Cpp$values)</code></pre>
<pre><code>## [1] 1.001778 1.013685 1.021022 1.036884</code></pre>
<p>The eigenvalues are exactly the same, just in opposite order. Next time we might link an optimized BLAS library such as Intel’s MKL, but I suspect the plain Eigen version is quite competitive.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The exact definition of “random variable” or “random vector” is unimportant. For mathematicians this means that there is a probability space <span class="math inline">\((\Omega, \mathcal{M}, \mathbf{P})\)</span> and that <span class="math inline">\(\vec{X}:\Omega \mapsto \mathbb{R}^d\)</span> is a Borel-measurable map.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Note on existence. A vector that attains the maximum must exist because the expression being maximized is continuous (in fact quadratic) in <span class="math inline">\(\vec{\omega}\)</span> and the unit sphere in <span class="math inline">\(\mathbb{R}^d\)</span> is compact.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>And this is indeed the true definition of the <a href="https://en.wikipedia.org/wiki/Gradient#Differential_or_(exterior)_derivative">gradient of a function</a>.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>This is the method of <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier#Modern_formulation_via_differentiable_manifolds">Lagrange multipliers</a>. It can be proven easily as follows. Let <span class="math inline">\(\vec{v}\)</span> be any vector tangent to the sphere at the maximizing point <span class="math inline">\(\vec{\omega}_1\)</span>, and let <span class="math inline">\(\gamma(t)\)</span> be a smooth curve on the sphere going through <span class="math inline">\(\vec{\omega}_1\)</span> with <span class="math inline">\(\gamma&#39;(t) = \vec{v}\)</span>. Then the function <span class="math inline">\(f(\gamma(t))\)</span> achieves a maximum at the value of <span class="math inline">\(t\)</span> at which <span class="math inline">\(\gamma(t) = \vec{\omega}_1\)</span> so it’s derivative must be 0 there. Thus <span class="math inline">\(0 = d/dt(f(\gamma(t))) = df_{\vec{\omega}_1} \ (\gamma&#39;(t)) = \langle \gamma&#39;(t),\nabla_{\vec{\omega}_1}f\rangle = \langle\vec{v},\nabla_{\vec{\omega}_1}f\rangle\)</span>. Since <span class="math inline">\(\vec{v}\)</span> was an arbitrary tangent vector this shows that <span class="math inline">\(\nabla_{\vec{\omega}_1}f\)</span> is orthogonal to every tangent vector and hence is a normal vector.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>The expression <span class="math inline">\(A\perp B\)</span> denotes “<span class="math inline">\(A\)</span> is perpendicular to <span class="math inline">\(B\)</span>”.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
</content:encoded>	
    </item>
    
  </channel>
</rss>
