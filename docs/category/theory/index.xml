<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Theory | Fundamenta Nova</title>
    <link>/category/theory/</link>
      <atom:link href="/category/theory/index.xml" rel="self" type="application/rss+xml" />
    <description>Theory</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 02 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Theory</title>
      <link>/category/theory/</link>
    </image>
    
    <item>
      <title>In Machine Learning, why is Regularization called Regularization?</title>
      <link>/post/004_regularization/main/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/004_regularization/main/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Many newcomers to machine learning know about regularization, but they may not understand it yet. In particular, they may not know why regularization has that name. In this post we discuss the numerical and statistical significance of regularization methods in machine learning and more general statistical models. We’ll try to introduce why one may want to use regularization methods in the first place and how to interpret the fitted model from a statistical point of view.&lt;/p&gt;
&lt;p&gt;The post will be long because there are a lot of cute nooks and crannies, and we’ll assume you know your linear algebra. However, if you already know what an inner product is then we think this post will be worth your time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrices-and-linear-ill-posed-problems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matrices and Linear Ill-Posed Problems&lt;/h2&gt;
&lt;p&gt;Suppose that we have a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\in \mathbb{R}^{N \times p}\)&lt;/span&gt;, a vector &lt;span class=&#34;math inline&#34;&gt;\(y\in \mathbb{R}^N\)&lt;/span&gt;, and that we seek a vector &lt;span class=&#34;math inline&#34;&gt;\(\beta\in\mathbb{R}^p\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(A\beta = y\)&lt;/span&gt;. How would one solve this problem? One answer might be to simply apply the inverse matrix to both sides of the equation: &lt;span class=&#34;math inline&#34;&gt;\(\beta = A^{-1}y\)&lt;/span&gt;. However, there are three problems with this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A matrix inverse &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; may not exist.&lt;/li&gt;
&lt;li&gt;Even if the matrix inverse exists it can be extremely expensive to calculate this inverse and apply the result to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Even if we are somehow able to calculate &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}y\)&lt;/span&gt; the solution may not be very stable. Small numerical changes in either &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; may lead to large changes in the solution &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Issue 2 above is not really a problem in the sense that &lt;a href=&#34;https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/&#34;&gt;one should never really need to find &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; to compute &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}y\)&lt;/span&gt;&lt;/a&gt;. Instead the most efficient numerical algorithms typically compute &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}y\)&lt;/span&gt; by using special factorizations of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/QR_decomposition&#34;&gt;QR decomposition&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The other two issues are very important and are inextricably linked to each other and to regularization in machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Issue 3 means just what is says: that the solution &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; may change a lot if the known data &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and/or &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; only change a little.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Issue 1 means that the matrix is not invertible. A matrix that is not invertible is called &lt;strong&gt;&lt;em&gt;singular&lt;/em&gt;&lt;/strong&gt;. A matrix that is invertible is usually called &lt;strong&gt;&lt;em&gt;nonsingular&lt;/em&gt;&lt;/strong&gt;, but a less common synonym is &lt;a href=&#34;https://mathworld.wolfram.com/RegularMatrix.html&#34;&gt;&lt;strong&gt;&lt;em&gt;regular&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; and this is where the name &lt;em&gt;regularization&lt;/em&gt; comes from. When a matrix is singular it means that the problem &lt;span class=&#34;math inline&#34;&gt;\(A\beta = y\)&lt;/span&gt; may have either no solution at all or have at least 2 distinct solutions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Any one of these issues being true means that the linear problem &lt;span class=&#34;math inline&#34;&gt;\(A\beta = y\)&lt;/span&gt; is ill-posed in the sense that it violates Hadamard’s conditions for a &lt;a href=&#34;https://en.wikipedia.org/wiki/Well-posed_problem&#34;&gt;well-posed problem&lt;/a&gt;. To be well-posed:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A problem must have a solution&lt;/li&gt;
&lt;li&gt;The solution must be unique&lt;/li&gt;
&lt;li&gt;The solution’s behavior must be stable/continuous with respect to the data&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These conditions are of extreme practical importance. They basically define what it means for a problem to be solvable &lt;strong&gt;&lt;em&gt;in practice&lt;/em&gt;&lt;/strong&gt;. In the case of the linear algebra problem above regularization means “&lt;em&gt;making the matrix regular&lt;/em&gt;” so that these conditions will hold true on the regularized problem. That’s where the name comes from.&lt;/p&gt;
&lt;p&gt;Ok, cool but why are these conditions important to ML or statistics? Consider the case of Maximum Likelihood Estimation (MLE) of a parametric model (although the lesson applies more generally):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In MLE we estimate a model’s unknown parameters by maximizing the log-likelihood. If no such maximizing values of the parameters can be found then the optimization problem does not have a solution and we can not obtain estimates for the unknown parameters to begin with! So, mirroring the first Hadamard condition, we require a maximizer to exist.&lt;/li&gt;
&lt;li&gt;Non-Bayesian statistical models naturally assume that a single fixed set of parameters exists that specifies the relevant distributions. If MLE gives multiple sets of parameters that maximize the likelihood (as happens in the presence of multiple local maxima) we may have no way to tell which maximizer is the one that estimates the actual parameters the best! So, mirroring the second Hadamard condition, we require the maximizer to be unique.&lt;/li&gt;
&lt;li&gt;Statistical models assume that data is in part random and so is subject to changes. If the estimated values of the parameters change a lot when the data changes a little then it’s impossible to tell when the MLE estimated parameters are in fact good estimates of the true parameters and when they are not! So, mirroring the third Hadamard condition, we require the maximizer to be stable with respect to changes in the data used for the fit.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Regularization is one way to change the problem so that these conditions are met.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some examples&lt;/h2&gt;
&lt;p&gt;Ok, let’s look at some examples!&lt;/p&gt;
&lt;div id=&#34;shifting-the-eigenvalues-of-a-symmetric-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Shifting the eigenvalues of a Symmetric Matrix&lt;/h3&gt;
&lt;p&gt;Suppose we are asked to solve the linear inverse problem from above &lt;span class=&#34;math inline&#34;&gt;\(A\beta = y\)&lt;/span&gt;, but that this time &lt;span class=&#34;math inline&#34;&gt;\(A\in \mathbb{R}^{N\times N}\)&lt;/span&gt; is a symmetric matrix. The &lt;a href=&#34;https://pages.mtu.edu/~msgocken/ma5630spring2003/lectures/spectral/spectral/node2.html&#34;&gt;spectral theorem for symmetric matrices&lt;/a&gt; tells us that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be represented as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
A = QDQ^T
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is an &lt;a href=&#34;https://en.wikipedia.org/wiki/Orthogonal_matrix&#34;&gt;orthogonal matrix&lt;/a&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is a diagonal matrix. Moreover, the columns of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; are the eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, and the diagonal elements of &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; are the corresponding (real) eigenvalues. &lt;a href=&#34;https://mast.queensu.ca/~br66/419/spectraltheoremproof.pdf&#34;&gt;Here’s&lt;/a&gt; a proof if you care for it.&lt;/p&gt;
&lt;p&gt;This representation shows us exactly what the theoretical difficulty is in the inverse problem. Inverting &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is easy. Indeed orthogonal matrices are always invertible, with the inverse given by the transpose: &lt;span class=&#34;math inline&#34;&gt;\(Q^{-1} = Q^T\)&lt;/span&gt;. The geometric significance of orthogonal matrices comes from the fact (basically their definition) that they preserve the inner product of vectors: If we denote the inner product by &lt;span class=&#34;math inline&#34;&gt;\(\langle \cdot,\cdot\rangle\)&lt;/span&gt; then for any &lt;span class=&#34;math inline&#34;&gt;\(x,y \in R^N\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\langle x, y \rangle = \langle Qx, Qy \rangle
\]&lt;/span&gt;
Thus (using the definition of the transpose) &lt;span class=&#34;math inline&#34;&gt;\(\langle x, y \rangle = \langle Q^TQx, y \rangle\)&lt;/span&gt;. Since this holds for any &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(x = Q^TQx\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Since this holds for all &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(Q^TQ = I\)&lt;/span&gt; and so &lt;span class=&#34;math inline&#34;&gt;\(Q^T\)&lt;/span&gt; is the inverse of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;No, the difficulty is simply in inverting the diagonal matrix &lt;span class=&#34;math inline&#34;&gt;\(D = \text{diag}(\sigma_1, \sigma_2, ..., \sigma_N)\)&lt;/span&gt;. If none of the eigenvalues &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(D^{-1} = \text{diag}(\sigma_1^{-1}, \sigma_2^{-1}, ..., \sigma_N^{-1})\)&lt;/span&gt;. In this case there isn’t any direction that &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; (and hence &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;) squashes into &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. However, if some of the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt;’s are &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; then we can not invert &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; and the problem will fail to satisfy at least one of the first 2 Hadamard conditions. Even if none of the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt;’s are exactly &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, some may be numerically very close to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; in comparison to the others. In that case the value of their reciprocals may be enormously large and may lead to numerical instability in the problem, violating the 3rd Hadamard condition. This would be a big problem in practice because computers &lt;a href=&#34;https://floating-point-gui.de/errors/propagation/&#34;&gt;hate mixing floating point numbers that are drastically different in size&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To address this issue we note that we can shift the eigenvalues of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; by adding a multiple of an identity matrix:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
A \to  A + \lambda I
\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is an eigenvalue of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(\sigma+\lambda\)&lt;/span&gt; is an eigenvalue of &lt;span class=&#34;math inline&#34;&gt;\(A+\lambda I\)&lt;/span&gt;. This is because every vector is an eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;: If &lt;span class=&#34;math inline&#34;&gt;\(Av = \sigma v\)&lt;/span&gt; then trivially &lt;span class=&#34;math inline&#34;&gt;\(\lambda Iv = \lambda v\)&lt;/span&gt;, so adding gives &lt;span class=&#34;math inline&#34;&gt;\((A+\lambda I)v = (\sigma + \lambda)v\)&lt;/span&gt;. This can also be seen form the representation above:
&lt;span class=&#34;math display&#34;&gt;\[
A + \lambda I = QDQ^T + \lambda QQ^T = Q(D + \lambda I)Q^T = Q\tilde{D}Q^T
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\tilde{D}= \text{diag}(\sigma_1 + \lambda, \sigma_2 + \lambda, ..., \sigma_N + \lambda)\)&lt;/span&gt;. Therefore, if we choose &lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;gt; \min\{\sigma_1, \sigma_2, ..., \sigma_N\}+ \delta\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\delta &amp;gt; 0\)&lt;/span&gt;, then all the shifted eigenvalues satisfy &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i + \lambda &amp;gt; \delta &amp;gt; 0\)&lt;/span&gt; and the new shifted problem&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(A+\lambda I)x = y
\]&lt;/span&gt;
will be solvable with solution given by
&lt;span class=&#34;math display&#34;&gt;\[
x = Q(D + \lambda I)^{-1}Q^Ty
\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is sufficiently large this inverse will exist and will be numerically stable (all of the eigenvalues will have been shifted away from 0).&lt;/p&gt;
&lt;p&gt;Making the change &lt;span class=&#34;math inline&#34;&gt;\(A \to A + \lambda I\)&lt;/span&gt; regularized the problem into one that satisfied Hadamard’s conditions, which is fundamentally the point of regularization. The change we made was essentially to replace the &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; with the approximation &lt;span class=&#34;math inline&#34;&gt;\((A+\lambda I)^{-1}\)&lt;/span&gt;, but we could have used other approximations as well, for example partial sums of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Invertible_matrix#By_Neumann_series&#34;&gt;Neumann Series Expansion of the &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt;&lt;/a&gt;. Regardless, the general principle illustrated above is basically to replace one problem by an approximate problem that does not suffer the same existence/stability issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regularization-as-a-perturbation-of-an-invertible-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regularization as a perturbation of an invertible matrix&lt;/h3&gt;
&lt;p&gt;Above we regularized the ill-posed problem &lt;span class=&#34;math inline&#34;&gt;\(A\beta = y\)&lt;/span&gt; by replacing it with the problem &lt;span class=&#34;math inline&#34;&gt;\((A+\lambda I)x = y\)&lt;/span&gt;. Let’s go a bit deeper with this process.&lt;strong&gt;You may skip this section on your first read.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dividing by &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, the problem &lt;span class=&#34;math inline&#34;&gt;\((A+\lambda I)x = y\)&lt;/span&gt; is equivalent to the problem&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\epsilon A+I)x = \epsilon y
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon := \frac{1}{\lambda}\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is large &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is small, and vice versa. Hence for large &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; the matrix &lt;span class=&#34;math inline&#34;&gt;\(\epsilon A+I\)&lt;/span&gt; can be seen as a small perturbation from the identity matrix &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now because &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; is invertible then for a small enough &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; (and hence for a large enough &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;) the preturbed matrix &lt;span class=&#34;math inline&#34;&gt;\(\epsilon A+I\)&lt;/span&gt; is also invertible! Why? That’s a great question! The previous section gave one proof, but there are some much nicer ways to see why:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Consider the function
&lt;span class=&#34;math display&#34;&gt;\[
\det:\mathbb{R}^{N\times N} \to \mathbb{R}
\]&lt;/span&gt;
that maps a matrix to it’s &lt;a href=&#34;https://en.wikipedia.org/wiki/Determinant&#34;&gt;determinant&lt;/a&gt;. Because the space &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{N\times N}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(N\times N\)&lt;/span&gt; matrices is just the Euclidean inner product space &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{N^2}\)&lt;/span&gt; with some extra algebraic structure, and because &lt;span class=&#34;math inline&#34;&gt;\(\det(A)\)&lt;/span&gt; is a polynomial function of the elements of a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\det\)&lt;/span&gt; is a continuous function on &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{N\times N}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;By &lt;a href=&#34;https://en.wikipedia.org/wiki/Cramer%27s_rule&#34;&gt;Cramer’s Rule&lt;/a&gt;, a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is invertible if and only if &lt;span class=&#34;math inline&#34;&gt;\(\det(A) \ne 0\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(\det\)&lt;/span&gt; is a continuous function on &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{N\times N}\)&lt;/span&gt; then the set of invertible matrices is an open subset of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{N\times N}\)&lt;/span&gt;! Hence for every invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(L\in\mathbb{R}^{N\times N}\)&lt;/span&gt; and every arbitrary matrix &lt;span class=&#34;math inline&#34;&gt;\(A\in\mathbb{R}^{N\times N}\)&lt;/span&gt; there exists an &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_0 &amp;gt; 0\)&lt;/span&gt; such that for all &lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;lt; \epsilon_0\)&lt;/span&gt; the matrix &lt;span class=&#34;math inline&#34;&gt;\(\epsilon A+L\)&lt;/span&gt; is invertible. &lt;strong&gt;QED&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There’s a version of the theorem in Banach spaces, but we don’t need it.&lt;/p&gt;
&lt;p&gt;Noticed that the only thing we needed about the matrix &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; in the above proof was that it was invertible. Therefore, we never needed to restrict attention to just the identity matrix &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;, but could have used any invertible matrix to regularize the problem:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(A+\lambda L)x = y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is any convenient invertible matrix. Below, where we regularize OLS, we are not restricted to using the identity matrix &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; to regularize, but can use any invertible symmetric matrix (preferably one that is positive definite so that a minimizer continues to exist).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;l2-regularization-of-ols&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularization of OLS&lt;/h3&gt;
&lt;p&gt;With this example we begin moving towards the statistical part of the post. One of the most widely known examples of regularization is what is often called &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularization, or &lt;a href=&#34;https://en.wikipedia.org/wiki/Tikhonov_regularization&#34;&gt;Tikhonov&lt;/a&gt; regularization of Ordinary Least Squares.&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is a real valued random variable and &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; is a random vector with values in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^p\)&lt;/span&gt;. Suppose that we have the conditional relationship&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y \ \ | \ \ \vec{X} \sim \mathcal{N}(\ \langle \vec{X}  , \beta\rangle \ ,\ \sigma^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(\mu, \sigma^2)\)&lt;/span&gt; denotes the univariate normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Here (and everywhere else) the symbol &lt;span class=&#34;math inline&#34;&gt;\(\langle v,w\rangle\)&lt;/span&gt; represents the inner product of two vectors &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. This is the most natural probability model that leads to linear regression. In practice the parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; that specify this conditional distribution are unknown and it is desired that they be estimated from data.&lt;/p&gt;
&lt;p&gt;In this canonical situation we assume that we have a data set &lt;span class=&#34;math inline&#34;&gt;\(\{ (Y_i, \vec{X}_i)\}_{i = 1}^N\)&lt;/span&gt; consisting of samples generated independently of one another from a fixed multivariate distribution for &lt;span class=&#34;math inline&#34;&gt;\((Y, \vec{X})\)&lt;/span&gt; (i.e. we assume our data was sampled IID). To fit the unknown parameters we use MLE. We may choose to use the conditional density &lt;span class=&#34;math inline&#34;&gt;\(p(Y|\vec{X})\)&lt;/span&gt; in the likelihood and this would make it &lt;strong&gt;&lt;em&gt;conditional&lt;/em&gt;&lt;/strong&gt; MLE. Or we may choose the unconditional multivariate density &lt;span class=&#34;math inline&#34;&gt;\(p(Y, \vec{X})\)&lt;/span&gt;. However, if we assume that the marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, (i.e. &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{X})\)&lt;/span&gt;), does not depend on either &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; then because &lt;span class=&#34;math inline&#34;&gt;\(p(Y,\vec{X}) = p(Y|\vec{X})p(\vec{X})\)&lt;/span&gt; building the likelihood using either &lt;span class=&#34;math inline&#34;&gt;\(p(Y|\vec{X})\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(p(Y,\vec{X})\)&lt;/span&gt; will lead to the same maximization problem because they differ by a constant factor (constant in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; that is). So we will use the conditional density &lt;span class=&#34;math inline&#34;&gt;\(p(Y|\vec{X})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Because the data is assumed to be generated IID then the full likelihood of the data is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L}(\beta, \sigma\ |\ \{ (Y_i, \vec{X}_i)\}) = \prod_{i = 1}^N p(Y_i|\vec{X}_i) = \prod_{i=1}^N\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\big(\frac{Y_i - \langle \vec{X}_i  , \beta\rangle}{\sigma}\big)^2} = \frac{1}{(\sigma^22\pi)^{N/2}}e^{\sum_{i=1}^N-\frac{1}{2}\big(\frac{Y_i - \langle \vec{X}_i  , \beta\rangle}{\sigma}\big)^2}
\]&lt;/span&gt;
Because the function &lt;span class=&#34;math inline&#34;&gt;\(f(x) := -\log(x)\)&lt;/span&gt; is decreasing we may instead minimize the negative of the log of this expression:
&lt;span class=&#34;math display&#34;&gt;\[
-\log(\mathcal{L}(\beta,\sigma\ |\ \{ (Y_i, \vec{X}_i)\})) = \frac{N}{2}\log(\sigma^22\pi) + \frac{1}{2\sigma^2}\sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i  , \beta\rangle\bigg)^2
\]&lt;/span&gt;
We first minimize with respect to &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; as this is necessary to do first before finding the minimizing value of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. To do this we need to minimize the only term that depends on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, namely the sum of squares &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta) := \sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)&lt;/span&gt; (hence Least Squares regression).&lt;/p&gt;
&lt;div id=&#34;a-geometric-interlude&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;A geometric interlude&lt;/h4&gt;
&lt;p&gt;Before we do that, let’s think about what the expression &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta)\)&lt;/span&gt; is. The term &lt;span class=&#34;math inline&#34;&gt;\(\langle \vec{X}_i , \beta\rangle\)&lt;/span&gt; is linear in the unknowns &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, and hence so is &lt;span class=&#34;math inline&#34;&gt;\(Y_i - \langle \vec{X}_i , \beta\rangle\)&lt;/span&gt;. Therefore, the square &lt;span class=&#34;math inline&#34;&gt;\(\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)&lt;/span&gt; is quadratic in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Thus since the full expression &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)&lt;/span&gt; is a sum of quadratic functions in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; it too is a quadratic function in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Since all terms in the sum are squares, the full sum is never negative and its graph in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Paraboloid&#34;&gt;non-hyperbolic paraboloid&lt;/a&gt;. Usually such shapes look like bowls. However, some can degenerate so that they become flat in one or more directions. Here’re some examples in R:&lt;/p&gt;
&lt;p&gt;Non-degenarte paraboloids look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nice.paraboloid = function(x,y)
{
    return(x^2+0.5*y^2)
}
x = y = seq(from = -4, to = 4, by = 0.2)
z = outer(x, y, nice.paraboloid)

persp(x, y, z,
      main=&amp;quot;Plot of a Non-degenerate 2D Paraboloid&amp;quot;,
      theta = 30, phi = 15,
      col = &amp;quot;springgreen&amp;quot;, shade = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/004_Regularization/main_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see such a paraboloid is bowl shaped. More technically it’s strictly convex, with a clear unique minimum point. However, paraboloids can degenerate so that they flatten out in some directions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;degenerate.paraboloid = function(x,y)
{
    return(x^2) #Does not change value as y changes
}
x = y = seq(from = -4, to = 4, by = 0.2)
z = outer(x, y, degenerate.paraboloid)

persp(x, y, z,
      main=&amp;quot;Plot of a Degenerate 2D Paraboloid&amp;quot;,
      theta = 30, phi = 15,
      col = &amp;quot;springgreen&amp;quot;, shade = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/004_Regularization/main_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example we changed the coefficient of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; from 0.5 to 0. The result is that in the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-direction the paraboloid flattened out and it no longer looks bowl shaped. Instead there are infinitely many minimum points all on the axis &lt;span class=&#34;math inline&#34;&gt;\(\{(x,y): x = 0\}\)&lt;/span&gt;. Note that if instead of making the coefficient of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; equal to 0 we had made it a positive number very close to zero then the mimima would become unique but would become hard to distinguish from nearby points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tricky.paraboloid = function(x,y)
{
    return(x^2+ 0.05*y^2) #Notice the coefficient of y is quite small
}
x = y = seq(from = -4, to = 4, by = 0.2)
z = outer(x, y, tricky.paraboloid)

persp(x, y, z,
      main=&amp;quot;Plot of a Nearly-Degenerate 2D Paraboloid&amp;quot;,
      theta = 30, phi = 15,
      col = &amp;quot;springgreen&amp;quot;, shade = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/004_Regularization/main_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These pictures show what can go wrong with the minima of quadratic functions like &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta)\)&lt;/span&gt; and why regularization may be needed. Now to get back to the minimizing the sum of squares &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta) = \sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)&lt;/span&gt;. If you’re reading this article I’m going to assume you’ve seen this derivation before so I’ll move a bit fast.&lt;/p&gt;
&lt;p&gt;First we define &lt;span class=&#34;math inline&#34;&gt;\(Y\in \mathbb{R}^{N\times 1}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th component equal to &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt;. (Note an abuse of notation we are making: earlier &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; denoted a real valued random variable, but now we are using the same symbol to denote the vector of the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; realizations of this random variable.) In addition, let &lt;span class=&#34;math inline&#34;&gt;\(X\in \mathbb{R}^{N\times p}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th row equal to &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}_i\)&lt;/span&gt;. Then in matrix notation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i  , \beta\rangle\bigg)^2 = (Y - X\beta)^T(Y - X\beta)
\]&lt;/span&gt;
Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} = \text{argmax}_{\beta} \ \ (Y - X\beta)^T(Y - X\beta)\)&lt;/span&gt; be the sought after minimizer. Since &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; is a minizer in the interior of the domain of &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta)\)&lt;/span&gt;, the gradient of &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta)\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; must be 0:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-2X^TY + 2X^TX\hat{\beta} = 0
\]&lt;/span&gt;
therefore we obtain the normal equations&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
X^TX\hat{\beta} = X^TY
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is basically the same linear algebra problem as before: If the inverse &lt;span class=&#34;math inline&#34;&gt;\((X^TX)^{-1}\)&lt;/span&gt; existed and was numerically nice then we can solve for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} = (X^TX)^{-1}X^TY\)&lt;/span&gt;. However, if this matrix inverse does not exist (as can happen when we do not have enough rows/samples for the given number of columns/unknowns) then this formula is not useful.&lt;/p&gt;
&lt;p&gt;But as before we can simply regularize by replacing the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(X^TX + \lambda I\)&lt;/span&gt; for some sufficiently large &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Actually since &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is non-negative definite&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and symmetric all of it’s eigenvalues are non-negative. So any &lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt; would be sufficient to shift the eigenvalues into positive numbers. Now the regularized problem becomes &lt;span class=&#34;math inline&#34;&gt;\((X^TX + \lambda I)\hat{\beta} = X^TY\)&lt;/span&gt;. Therefore we get the regularized MLE solution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_{reg} := (X^TX + \lambda I)^{-1}X^TY
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Does this regularized problem correspond to its own minimization problem? Yes! Working backwards, this new problem is equivalent to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-2X^TY + 2X^TX\hat{\beta} + 2\lambda\hat{\beta}= 0
\]&lt;/span&gt;
The left had side is the gradient of &lt;span class=&#34;math inline&#34;&gt;\((Y - X\beta)^T(Y - X\beta) + \lambda\sum_{j=1}^p\beta_j^2 = (Y - X\beta)^T(Y - X\beta) + \lambda \langle \beta, \beta\rangle\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\beta = \hat{\beta}\)&lt;/span&gt; as can be checked. So the regularized problem corresponds to trying to minimize the expression &lt;span class=&#34;math inline&#34;&gt;\((Y - X\beta)^T(Y - X\beta) + \lambda\sum_{j=1}^p\beta_j^2 = (Y - X\beta)^T(Y - X\beta) + \lambda \langle \beta, \beta\rangle\)&lt;/span&gt;. This of course is &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularization. Thus we have derived &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularization for OLS simply by seeking to transform the inverse problem that arises in OLS so that it may satisfy the Hadamard conditions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;an-illustrative-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;An Illustrative Example&lt;/h3&gt;
&lt;p&gt;Below we can see geometrically what regularization does. The sum of squares expression &lt;span class=&#34;math inline&#34;&gt;\((Y - X\beta)^T(Y - X\beta)\)&lt;/span&gt; is quadratic in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, but may have a graph that is a degenerate paraboloid. This is what causes it to have multiple minimizers in OLS and what makes the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; singular (more on this point in the next section). However the expression &lt;span class=&#34;math inline&#34;&gt;\(\lambda\langle \beta, \beta\rangle = \lambda\sum_{j = 1}^p\beta_j^2\)&lt;/span&gt; is a strictly positive-definite quadratic form. Its graph is a non-degenerate bowl shaped paraboloid.&lt;/p&gt;
&lt;p&gt;Adding a non-degenerate paraboloid to something that is not bowl shaped makes the second graph more bowl shaped! Moreover it shifts the minimum of the 2nd graph towards the mimimum of the bowl. As an illustration, let’s take a look at an example where this is easy to see.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())
bumpy.function = function(x,y)
{
    return(sin(x)+sin(y))
}

nice.paraboloid = function(x,y)
{
    return(0.15*(x^2 + y^2)) #lambda = 0.15 is used
}

x = y = seq(from = -4, to = 4, by = 0.2)
bumpy = outer(x, y, bumpy.function)
paraboloid = outer(x, y, nice.paraboloid)
bumpy.plus.paraboloid = bumpy + paraboloid

persp(x, y, bumpy,
      main=&amp;quot;Graph of a bumpy function with multiple minima&amp;quot;,
      theta = 30, phi = 15,
      col = &amp;quot;springgreen&amp;quot;, shade = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/004_Regularization/main_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;persp(x, y, paraboloid,
      main=&amp;quot;Graph of a nice paraboloid&amp;quot;,
      theta = 30, phi = 15,
      col = &amp;quot;springgreen&amp;quot;, shade = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/004_Regularization/main_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;persp(x, y, bumpy.plus.paraboloid,
      main=&amp;quot;Graph of a regularized bumpy function = bumpy function + paraboloid&amp;quot;,
      theta = 30, phi = 15,
      col = &amp;quot;springgreen&amp;quot;, shade = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/004_Regularization/main_files/figure-html/unnamed-chunk-4-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we see that the most important geometric aspect of the regularizing term &lt;span class=&#34;math inline&#34;&gt;\(\lambda\langle\beta, \beta\rangle\)&lt;/span&gt; is the fact that it is &lt;a href=&#34;https://en.wikipedia.org/wiki/Convex_function&#34;&gt;strictly convex&lt;/a&gt;!! &lt;strong&gt;&lt;em&gt;Although we will not dwell on it, it is impossible to overstate the theoretical importance of the previous sentence.&lt;/em&gt;&lt;/strong&gt; As a matter of fact, geometrically speaking it’s clear that had we added &lt;strong&gt;any&lt;/strong&gt; strictly convex function to the bumpy function we would have gotten something more bowl shaped. We will not go further into it here but you should know that &lt;a href=&#34;https://en.wikipedia.org/wiki/Convex_analysis&#34;&gt;convexity is one of those properties in mathematics out of which entire fields are created&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-hessian-matrix-and-more-complex-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Hessian matrix and more complex models&lt;/h3&gt;
&lt;p&gt;In the OLS problem above the Maximum Likelihood estimator turned out to be the one that minimized the sum of squares expression &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta) := (Y - X\beta)^T(Y - X\beta)\)&lt;/span&gt;. This expression can be expanded in matrix notation as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y^TY - Y^TX\beta - \beta^TX^TY + \beta^TX^TX\beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We see that there is a quadratic term (&lt;span class=&#34;math inline&#34;&gt;\(\beta^TX^TX\beta\)&lt;/span&gt;) and the rest are terms with powers of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; less than 2. The matrix of second derivatives of this expression (known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Hessian_matrix&#34;&gt;Hessian matrix&lt;/a&gt;) is therefore just the matrix of coefficients of this quadratic term: &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;. This Hessian is exactly what was the star of the show in the OLS problem!&lt;/p&gt;
&lt;p&gt;The Hessian of a function at a point tells us the convexity of the function at the point. &lt;a href=&#34;https://en.wikipedia.org/wiki/Second_partial_derivative_test&#34;&gt;If the Hessian is positive definite, then near the minimizing point the function is bowl shaped. If the Hessian is negative definite then near a maximizing point the function is shaped like an upside down bowl.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Moreover, The Hessian is always a symmetric matrix by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives#Formal_expressions_of_symmetry&#34;&gt;equality of cross-derivatives&lt;/a&gt;. So the previous point is really a statement about Hessian’s eigenvalues. In short, if the Hessian has all positive eigenvalues then it is positive definite and the function is bowl shaped near its minimizer.&lt;/p&gt;
&lt;p&gt;The OLS Hessian matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is symmetric and non-negative definite, so the graph of the sum of squares &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta) := (Y - X\beta)^T(Y - X\beta)\)&lt;/span&gt; can only fail to be bowl shaped near the minimizer &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; if the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; has eigenvalues that are equal 0 (or positive but close to 0 in the case numerical instability). In which case, the graph of &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta)\)&lt;/span&gt; is a degenerate non-hyperbolic paraboloid and there are multiple minimizing solutions &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thus regularizing the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is just regularizing the Hessian matrix of the function &lt;span class=&#34;math inline&#34;&gt;\(SSE(\beta)\)&lt;/span&gt; we want to minimize!!&lt;/strong&gt; This fact is what allows us to take the idea beyond OLS.&lt;/p&gt;
&lt;p&gt;Indeed if &lt;span class=&#34;math inline&#34;&gt;\(f(\beta)\)&lt;/span&gt; is any 2nd order differentiable cost function in any machine learning model then by the linearity of the derivative&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Hessian}(f + \lambda \langle \beta, \beta\rangle) = \text{Hessian}(f) + \lambda\text{Hessian}(\langle \beta, \beta\rangle) = \text{Hessian}(f) + \lambda I
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There we used the fact that &lt;span class=&#34;math inline&#34;&gt;\(\text{Hessian}(\langle \beta, \beta\rangle) = \text{Hessian}(\sum_i^p \beta_i^2) = I\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is almost arbitrary we see that we can apply &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-regularization to a very large family of problems, with the goal being to regularize the Hessian of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. As an example let’s look at some other kinds of regression problems. For OLS we assumed the conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(Y \ \ | \ \ \vec{X} \sim \mathcal{N}(\ \langle \vec{X} , \beta\rangle \ ,\ \sigma^2)\)&lt;/span&gt;, but we may have chosen a different conditional distribution.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; takes on only values in the set &lt;span class=&#34;math inline&#34;&gt;\(\{0,1\}\)&lt;/span&gt; then it is a Bernoulli random variable. This is the case in logistic regression where the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; is
&lt;span class=&#34;math display&#34;&gt;\[
Y \ \ | \ \ \vec{X} \sim \mathcal{B}(\ p = \phi(\langle \vec{X}  , \beta\rangle) \ )
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B(p)}\)&lt;/span&gt; is a Bernoulli distribution with probability of a positive event equal to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p = \text{E}[Y|\vec{X}] = \phi(\langle \vec{X} , \beta\rangle)\)&lt;/span&gt; is the probability of a positive event given &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\phi(t) = \frac{e^t}{1+e^t}\)&lt;/span&gt; is the standard logit. In this case the conditional density &lt;span class=&#34;math inline&#34;&gt;\(p(Y|\vec{X})\)&lt;/span&gt; can be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(Y|\vec{X}) = p^Y\big(1 - p)\big)^{1-Y} =\phi(\langle \vec{X}  , \beta\rangle)^Y\big(1 - \phi(\langle \vec{X}  , \beta\rangle)\big)^{1-Y}
\]&lt;/span&gt;
So the negative log-likelihood is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-\mathcal{l}(\beta) = -\sum_{i=1}^N Y_i\log(\phi(\langle \vec{X}_i  , \beta\rangle)) + (1-Y_i)\log(1-\phi(\langle \vec{X}_i  , \beta\rangle))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This function may or may not look bowl shaped (i.e. strictly convex) near the &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; that minimizes it. In case it doesn’t we can make it so by adding &lt;span class=&#34;math inline&#34;&gt;\(\lambda \langle \beta, \beta\rangle = \lambda\sum_{j = 1}^p\beta_j^2\)&lt;/span&gt; for some sufficiently large &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and minimizing this new problem. The same applies to generalized linear models, neural networks, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bias-variance-trade-offs-and-regularization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bias-Variance trade offs and Regularization&lt;/h2&gt;
&lt;p&gt;Above we used regularization methods to make a problem “nicer” in the numerical sense (i.e. satisfying Hadamard’s conditions). But what does “nicer” mean in the statistical context? That is a multifaceted question. The first step is to recognize that what might be viewed as instability from the numerical point of view, can be understood as high variance from the statistical point of view.&lt;/p&gt;
&lt;p&gt;We illustrate with the OLS estimator. Suppose that the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is indeed invertible. The standard OLS estimator is the random vector given by the normal equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta} = (X^TX)^{-1}X^TY
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We see that this is an unbiased estimator of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{E}[\hat{\beta}] = \text{E}\big[\text{E}[\hat{\beta}|X]\big] = \text{E}\bigg[(X^TX)^{-1}X^T\text{E}[Y|X]\bigg] = \text{E}\bigg[(X^TX)^{-1}X^TX\beta\bigg] = \text{E}[\beta] = \beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Moreover, it’s easy enough to compute the conditional covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}[\hat{\beta}|X] = (X^TX)^{-1}X^T\cdot \text{Var}[Y|X] \cdot X(X^TX)^{-1} = (X^TX)^{-1}X^T\cdot \sigma^2 I \cdot X(X^TX)^{-1} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \sigma^2 (X^TX)^{-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_total_variance&#34;&gt;unconditional covariance matrix can be computed as&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}[\hat{\beta}] = \text{E}[\text{Var}[\hat{\beta}|X]] + \text{Var}[\text{E}[\hat{\beta}|X]] 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \sigma^2\text{E}[(X^TX)^{-1}] + \text{Var}[\beta] = \sigma^2\text{E}[(X^TX)^{-1}]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is harder to compute because it depends on the distribution of the random matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Regardless, we can see that what controls the variance of the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; (whether conditional or not) is the inverse matrix &lt;span class=&#34;math inline&#34;&gt;\((X^TX)^{-1}\)&lt;/span&gt;. This is interesting because it shows that the matrix we identified as the Hessian of the OLS cost function (&lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;) is also the matrix that controls the covariance of the OLS estimator.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If any of the eigenvalues of the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; were “close” to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; then the eigenvalues of the inverse will be very large, causing the variance of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; to be very large. If you’re familiar with VIFs, this is what causes large &lt;a href=&#34;https://en.wikipedia.org/wiki/Variance_inflation_factor&#34;&gt;variance inflation factors&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Regularization is used to reduce the variance in this estimator. If we denote the regularized estimator by:
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_{reg} = (X^TX + \lambda I)^{-1}X^TY
\]&lt;/span&gt;
Then this estimator is biased away from &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. To see this we first compute the conditional mean:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{E}[\hat{\beta}_{reg}|X] = (X^TX + \lambda I)^{-1}X^TX\beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= (X^TX + \lambda I)^{-1}(X^TX + \lambda I)\beta - \lambda (X^TX + \lambda I)^{-1}\beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \beta - \lambda (X^TX + \lambda I)^{-1}\beta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence
&lt;span class=&#34;math display&#34;&gt;\[
\text{E}[\hat{\beta}_{reg}] = \beta - \lambda\text{E}\big[(X^TX + \lambda I)^{-1}\big]\beta
\]&lt;/span&gt;
which is “&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; minus something” and hence not equal to &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. However the effect on the variance is better:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}[\hat{\beta}_{reg}|X] = (X^TX + \lambda I)^{-1}X^T\cdot\text{Var}[Y|X]\cdot X(X^TX + \lambda I)^{-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \sigma^2(X^TX + \lambda I)^{-1}X^TX(X^TX + \lambda I)^{-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \sigma^2(X^TX + \lambda I)^{-1}(X^TX+\lambda I)(X^TX + \lambda I)^{-1} - \sigma^2\lambda(X^TX + \lambda I)^{-2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
=  \sigma^2(X^TX + \lambda I)^{-1} - \sigma^2\lambda(X^TX + \lambda I)^{-2}
\]&lt;/span&gt;
This variance formula may look messy but the gist is that instead of inverting &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; we are inverting &lt;span class=&#34;math inline&#34;&gt;\(X^TX + \lambda I\)&lt;/span&gt;. The matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX + \lambda I\)&lt;/span&gt; has larger eigenvalues than the matrix &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;. Therefore &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}[\hat{\beta}_{reg}|X] = \sigma^2(X^TX + \lambda I)^{-1} - \sigma^2\lambda(X^TX + \lambda I)^{-2}\)&lt;/span&gt; is smaller than &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}[\hat{\beta}|X] = \sigma^2 (X^TX)^{-1}\)&lt;/span&gt; in the sense that it has smaller eigenvalues. &lt;strong&gt;Thus regularization has increased bias, but reduced variance.&lt;/strong&gt; Similar effects hold for more complex models than OLS, but instead of chasing formulas the read should try cooking up some numerical examples via Monte Carlo.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-the-bayesian-view-point&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What about the Bayesian view point?&lt;/h2&gt;
&lt;p&gt;A very natural perspective on regularization can be found in Bayesian modeling, where regularization terms amount to simply specifying prior distributions. However, this is standard Bayesian theory and this post is already long enough :P&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Indeed suppose &lt;span class=&#34;math inline&#34;&gt;\(v\in \mathbb{R}^p\)&lt;/span&gt; is any vector. Then &lt;span class=&#34;math inline&#34;&gt;\(\langle v, X^TXv\rangle = \langle Xv,Xv\rangle = ||Xv||^2 \ge 0\)&lt;/span&gt;. Hence &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; is always non-negative definite.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;This is a general feature of Maximum Likelihood estimators called “&lt;em&gt;asymptotic efficiency&lt;/em&gt;”, where the covariance matrix of the MLE estimator approaches a “best possible” covariance matrix as the sample size increases. Essentially the best possible covariance matrix that an unbiased estimator of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; can have is given given by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound&#34;&gt;Cramer-Rao Bound&lt;/a&gt; and is determined by the inverse of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fisher_information&#34;&gt;Fisher Information matrix&lt;/a&gt;, whose &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt;-component is &lt;span class=&#34;math inline&#34;&gt;\(-\text{E}[\partial^2\log(p(Y, \vec{X}| \beta))/\partial \beta_i\partial \beta_j]\)&lt;/span&gt;. The beauty is that the Hessian of the negative loglikelihood is the sample estimator of this Fisher Information (where expectation is replaced by average over samples). This is why the Hessian is showing up as the determining factor in estimator covariance.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      <content:encoded>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Many newcomers to machine learning know about regularization, but they may not understand it yet. In particular, they may not know why regularization has that name. In this post we discuss the numerical and statistical significance of regularization methods in machine learning and more general statistical models. We’ll try to introduce why one may want to use regularization methods in the first place and how to interpret the fitted model from a statistical point of view.</p>
<p>The post will be long because there are a lot of cute nooks and crannies, and we’ll assume you know your linear algebra. However, if you already know what an inner product is then we think this post will be worth your time.</p>
</div>
<div id="matrices-and-linear-ill-posed-problems" class="section level2">
<h2>Matrices and Linear Ill-Posed Problems</h2>
<p>Suppose that we have a matrix <span class="math inline">\(A\in \mathbb{R}^{N \times p}\)</span>, a vector <span class="math inline">\(y\in \mathbb{R}^N\)</span>, and that we seek a vector <span class="math inline">\(\beta\in\mathbb{R}^p\)</span> such that <span class="math inline">\(A\beta = y\)</span>. How would one solve this problem? One answer might be to simply apply the inverse matrix to both sides of the equation: <span class="math inline">\(\beta = A^{-1}y\)</span>. However, there are three problems with this:</p>
<ol style="list-style-type: decimal">
<li>A matrix inverse <span class="math inline">\(A^{-1}\)</span> may not exist.</li>
<li>Even if the matrix inverse exists it can be extremely expensive to calculate this inverse and apply the result to <span class="math inline">\(y\)</span>.</li>
<li>Even if we are somehow able to calculate <span class="math inline">\(A^{-1}y\)</span> the solution may not be very stable. Small numerical changes in either <span class="math inline">\(A\)</span> or <span class="math inline">\(y\)</span> may lead to large changes in the solution <span class="math inline">\(\beta\)</span>.</li>
</ol>
<p>Issue 2 above is not really a problem in the sense that <a href="https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/">one should never really need to find <span class="math inline">\(A^{-1}\)</span> to compute <span class="math inline">\(A^{-1}y\)</span></a>. Instead the most efficient numerical algorithms typically compute <span class="math inline">\(A^{-1}y\)</span> by using special factorizations of <span class="math inline">\(A\)</span>, such as <a href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a>.</p>
<p>The other two issues are very important and are inextricably linked to each other and to regularization in machine learning.</p>
<ul>
<li><p>Issue 3 means just what is says: that the solution <span class="math inline">\(\beta\)</span> may change a lot if the known data <span class="math inline">\(A\)</span> and/or <span class="math inline">\(y\)</span> only change a little.</p></li>
<li><p>Issue 1 means that the matrix is not invertible. A matrix that is not invertible is called <strong><em>singular</em></strong>. A matrix that is invertible is usually called <strong><em>nonsingular</em></strong>, but a less common synonym is <a href="https://mathworld.wolfram.com/RegularMatrix.html"><strong><em>regular</em></strong></a> and this is where the name <em>regularization</em> comes from. When a matrix is singular it means that the problem <span class="math inline">\(A\beta = y\)</span> may have either no solution at all or have at least 2 distinct solutions.</p></li>
</ul>
<p>Any one of these issues being true means that the linear problem <span class="math inline">\(A\beta = y\)</span> is ill-posed in the sense that it violates Hadamard’s conditions for a <a href="https://en.wikipedia.org/wiki/Well-posed_problem">well-posed problem</a>. To be well-posed:</p>
<ol style="list-style-type: decimal">
<li>A problem must have a solution</li>
<li>The solution must be unique</li>
<li>The solution’s behavior must be stable/continuous with respect to the data</li>
</ol>
<p>These conditions are of extreme practical importance. They basically define what it means for a problem to be solvable <strong><em>in practice</em></strong>. In the case of the linear algebra problem above regularization means “<em>making the matrix regular</em>” so that these conditions will hold true on the regularized problem. That’s where the name comes from.</p>
<p>Ok, cool but why are these conditions important to ML or statistics? Consider the case of Maximum Likelihood Estimation (MLE) of a parametric model (although the lesson applies more generally):</p>
<ol style="list-style-type: decimal">
<li>In MLE we estimate a model’s unknown parameters by maximizing the log-likelihood. If no such maximizing values of the parameters can be found then the optimization problem does not have a solution and we can not obtain estimates for the unknown parameters to begin with! So, mirroring the first Hadamard condition, we require a maximizer to exist.</li>
<li>Non-Bayesian statistical models naturally assume that a single fixed set of parameters exists that specifies the relevant distributions. If MLE gives multiple sets of parameters that maximize the likelihood (as happens in the presence of multiple local maxima) we may have no way to tell which maximizer is the one that estimates the actual parameters the best! So, mirroring the second Hadamard condition, we require the maximizer to be unique.</li>
<li>Statistical models assume that data is in part random and so is subject to changes. If the estimated values of the parameters change a lot when the data changes a little then it’s impossible to tell when the MLE estimated parameters are in fact good estimates of the true parameters and when they are not! So, mirroring the third Hadamard condition, we require the maximizer to be stable with respect to changes in the data used for the fit.</li>
</ol>
<p>Regularization is one way to change the problem so that these conditions are met.</p>
</div>
<div id="some-examples" class="section level2">
<h2>Some examples</h2>
<p>Ok, let’s look at some examples!</p>
<div id="shifting-the-eigenvalues-of-a-symmetric-matrix" class="section level3">
<h3>Shifting the eigenvalues of a Symmetric Matrix</h3>
<p>Suppose we are asked to solve the linear inverse problem from above <span class="math inline">\(A\beta = y\)</span>, but that this time <span class="math inline">\(A\in \mathbb{R}^{N\times N}\)</span> is a symmetric matrix. The <a href="https://pages.mtu.edu/~msgocken/ma5630spring2003/lectures/spectral/spectral/node2.html">spectral theorem for symmetric matrices</a> tells us that <span class="math inline">\(A\)</span> can be represented as</p>
<p><span class="math display">\[
A = QDQ^T
\]</span>
where <span class="math inline">\(Q\)</span> is an <a href="https://en.wikipedia.org/wiki/Orthogonal_matrix">orthogonal matrix</a> and <span class="math inline">\(D\)</span> is a diagonal matrix. Moreover, the columns of <span class="math inline">\(Q\)</span> are the eigenvectors of <span class="math inline">\(A\)</span>, and the diagonal elements of <span class="math inline">\(D\)</span> are the corresponding (real) eigenvalues. <a href="https://mast.queensu.ca/~br66/419/spectraltheoremproof.pdf">Here’s</a> a proof if you care for it.</p>
<p>This representation shows us exactly what the theoretical difficulty is in the inverse problem. Inverting <span class="math inline">\(Q\)</span> is easy. Indeed orthogonal matrices are always invertible, with the inverse given by the transpose: <span class="math inline">\(Q^{-1} = Q^T\)</span>. The geometric significance of orthogonal matrices comes from the fact (basically their definition) that they preserve the inner product of vectors: If we denote the inner product by <span class="math inline">\(\langle \cdot,\cdot\rangle\)</span> then for any <span class="math inline">\(x,y \in R^N\)</span></p>
<p><span class="math display">\[
\langle x, y \rangle = \langle Qx, Qy \rangle
\]</span>
Thus (using the definition of the transpose) <span class="math inline">\(\langle x, y \rangle = \langle Q^TQx, y \rangle\)</span>. Since this holds for any <span class="math inline">\(y\)</span>, then <span class="math inline">\(x = Q^TQx\)</span> for all <span class="math inline">\(x\)</span>. Since this holds for all <span class="math inline">\(x\)</span> then <span class="math inline">\(Q^TQ = I\)</span> and so <span class="math inline">\(Q^T\)</span> is the inverse of <span class="math inline">\(Q\)</span>.</p>
<p>No, the difficulty is simply in inverting the diagonal matrix <span class="math inline">\(D = \text{diag}(\sigma_1, \sigma_2, ..., \sigma_N)\)</span>. If none of the eigenvalues <span class="math inline">\(\sigma_i\)</span> are <span class="math inline">\(0\)</span> then <span class="math inline">\(D^{-1} = \text{diag}(\sigma_1^{-1}, \sigma_2^{-1}, ..., \sigma_N^{-1})\)</span>. In this case there isn’t any direction that <span class="math inline">\(D\)</span> (and hence <span class="math inline">\(A\)</span>) squashes into <span class="math inline">\(0\)</span>. However, if some of the <span class="math inline">\(\sigma_i\)</span>’s are <span class="math inline">\(0\)</span> then we can not invert <span class="math inline">\(D\)</span> and the problem will fail to satisfy at least one of the first 2 Hadamard conditions. Even if none of the <span class="math inline">\(\sigma_i\)</span>’s are exactly <span class="math inline">\(0\)</span>, some may be numerically very close to <span class="math inline">\(0\)</span> in comparison to the others. In that case the value of their reciprocals may be enormously large and may lead to numerical instability in the problem, violating the 3rd Hadamard condition. This would be a big problem in practice because computers <a href="https://floating-point-gui.de/errors/propagation/">hate mixing floating point numbers that are drastically different in size</a>.</p>
<p>To address this issue we note that we can shift the eigenvalues of <span class="math inline">\(A\)</span> by adding a multiple of an identity matrix:</p>
<p><span class="math display">\[
A \to  A + \lambda I
\]</span>
If <span class="math inline">\(\sigma\)</span> is an eigenvalue of <span class="math inline">\(A\)</span> then <span class="math inline">\(\sigma+\lambda\)</span> is an eigenvalue of <span class="math inline">\(A+\lambda I\)</span>. This is because every vector is an eigenvector of <span class="math inline">\(I\)</span>: If <span class="math inline">\(Av = \sigma v\)</span> then trivially <span class="math inline">\(\lambda Iv = \lambda v\)</span>, so adding gives <span class="math inline">\((A+\lambda I)v = (\sigma + \lambda)v\)</span>. This can also be seen form the representation above:
<span class="math display">\[
A + \lambda I = QDQ^T + \lambda QQ^T = Q(D + \lambda I)Q^T = Q\tilde{D}Q^T
\]</span>
where <span class="math inline">\(\tilde{D}= \text{diag}(\sigma_1 + \lambda, \sigma_2 + \lambda, ..., \sigma_N + \lambda)\)</span>. Therefore, if we choose <span class="math inline">\(\lambda &gt; \min\{\sigma_1, \sigma_2, ..., \sigma_N\}+ \delta\)</span> for some <span class="math inline">\(\delta &gt; 0\)</span>, then all the shifted eigenvalues satisfy <span class="math inline">\(\sigma_i + \lambda &gt; \delta &gt; 0\)</span> and the new shifted problem</p>
<p><span class="math display">\[
(A+\lambda I)x = y
\]</span>
will be solvable with solution given by
<span class="math display">\[
x = Q(D + \lambda I)^{-1}Q^Ty
\]</span>
If <span class="math inline">\(\lambda\)</span> is sufficiently large this inverse will exist and will be numerically stable (all of the eigenvalues will have been shifted away from 0).</p>
<p>Making the change <span class="math inline">\(A \to A + \lambda I\)</span> regularized the problem into one that satisfied Hadamard’s conditions, which is fundamentally the point of regularization. The change we made was essentially to replace the <span class="math inline">\(A^{-1}\)</span> with the approximation <span class="math inline">\((A+\lambda I)^{-1}\)</span>, but we could have used other approximations as well, for example partial sums of the <a href="https://en.wikipedia.org/wiki/Invertible_matrix#By_Neumann_series">Neumann Series Expansion of the <span class="math inline">\(A^{-1}\)</span></a>. Regardless, the general principle illustrated above is basically to replace one problem by an approximate problem that does not suffer the same existence/stability issues.</p>
</div>
<div id="regularization-as-a-perturbation-of-an-invertible-matrix" class="section level3">
<h3>Regularization as a perturbation of an invertible matrix</h3>
<p>Above we regularized the ill-posed problem <span class="math inline">\(A\beta = y\)</span> by replacing it with the problem <span class="math inline">\((A+\lambda I)x = y\)</span>. Let’s go a bit deeper with this process.<strong>You may skip this section on your first read.</strong></p>
<p>Dividing by <span class="math inline">\(\lambda\)</span>, the problem <span class="math inline">\((A+\lambda I)x = y\)</span> is equivalent to the problem</p>
<p><span class="math display">\[
(\epsilon A+I)x = \epsilon y
\]</span>
where <span class="math inline">\(\epsilon := \frac{1}{\lambda}\)</span>. When <span class="math inline">\(\lambda\)</span> is large <span class="math inline">\(\epsilon\)</span> is small, and vice versa. Hence for large <span class="math inline">\(\lambda\)</span> the matrix <span class="math inline">\(\epsilon A+I\)</span> can be seen as a small perturbation from the identity matrix <span class="math inline">\(I\)</span>.</p>
<p>Now because <span class="math inline">\(I\)</span> is invertible then for a small enough <span class="math inline">\(\epsilon\)</span> (and hence for a large enough <span class="math inline">\(\lambda\)</span>) the preturbed matrix <span class="math inline">\(\epsilon A+I\)</span> is also invertible! Why? That’s a great question! The previous section gave one proof, but there are some much nicer ways to see why:</p>
<p><strong>Proof:</strong> Consider the function
<span class="math display">\[
\det:\mathbb{R}^{N\times N} \to \mathbb{R}
\]</span>
that maps a matrix to it’s <a href="https://en.wikipedia.org/wiki/Determinant">determinant</a>. Because the space <span class="math inline">\(\mathbb{R}^{N\times N}\)</span> of <span class="math inline">\(N\times N\)</span> matrices is just the Euclidean inner product space <span class="math inline">\(\mathbb{R}^{N^2}\)</span> with some extra algebraic structure, and because <span class="math inline">\(\det(A)\)</span> is a polynomial function of the elements of a matrix <span class="math inline">\(A\)</span>, <span class="math inline">\(\det\)</span> is a continuous function on <span class="math inline">\(\mathbb{R}^{N\times N}\)</span>.</p>
<p>By <a href="https://en.wikipedia.org/wiki/Cramer%27s_rule">Cramer’s Rule</a>, a matrix <span class="math inline">\(A\)</span> is invertible if and only if <span class="math inline">\(\det(A) \ne 0\)</span>. Because <span class="math inline">\(\det\)</span> is a continuous function on <span class="math inline">\(\mathbb{R}^{N\times N}\)</span> then the set of invertible matrices is an open subset of <span class="math inline">\(\mathbb{R}^{N\times N}\)</span>! Hence for every invertible matrix <span class="math inline">\(L\in\mathbb{R}^{N\times N}\)</span> and every arbitrary matrix <span class="math inline">\(A\in\mathbb{R}^{N\times N}\)</span> there exists an <span class="math inline">\(\epsilon_0 &gt; 0\)</span> such that for all <span class="math inline">\(\epsilon &lt; \epsilon_0\)</span> the matrix <span class="math inline">\(\epsilon A+L\)</span> is invertible. <strong>QED</strong></p>
<p>There’s a version of the theorem in Banach spaces, but we don’t need it.</p>
<p>Noticed that the only thing we needed about the matrix <span class="math inline">\(I\)</span> in the above proof was that it was invertible. Therefore, we never needed to restrict attention to just the identity matrix <span class="math inline">\(I\)</span>, but could have used any invertible matrix to regularize the problem:</p>
<p><span class="math display">\[
(A+\lambda L)x = y
\]</span></p>
<p>where <span class="math inline">\(L\)</span> is any convenient invertible matrix. Below, where we regularize OLS, we are not restricted to using the identity matrix <span class="math inline">\(I\)</span> to regularize, but can use any invertible symmetric matrix (preferably one that is positive definite so that a minimizer continues to exist).</p>
</div>
<div id="l2-regularization-of-ols" class="section level3">
<h3><span class="math inline">\(L^2\)</span>-regularization of OLS</h3>
<p>With this example we begin moving towards the statistical part of the post. One of the most widely known examples of regularization is what is often called <span class="math inline">\(L^2\)</span>-regularization, or <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov</a> regularization of Ordinary Least Squares.</p>
<p>Suppose <span class="math inline">\(Y\)</span> is a real valued random variable and <span class="math inline">\(\vec{X}\)</span> is a random vector with values in <span class="math inline">\(\mathbb{R}^p\)</span>. Suppose that we have the conditional relationship</p>
<p><span class="math display">\[
Y \ \ | \ \ \vec{X} \sim \mathcal{N}(\ \langle \vec{X}  , \beta\rangle \ ,\ \sigma^2)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span> denotes the univariate normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Here (and everywhere else) the symbol <span class="math inline">\(\langle v,w\rangle\)</span> represents the inner product of two vectors <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span>. This is the most natural probability model that leads to linear regression. In practice the parameters <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span> that specify this conditional distribution are unknown and it is desired that they be estimated from data.</p>
<p>In this canonical situation we assume that we have a data set <span class="math inline">\(\{ (Y_i, \vec{X}_i)\}_{i = 1}^N\)</span> consisting of samples generated independently of one another from a fixed multivariate distribution for <span class="math inline">\((Y, \vec{X})\)</span> (i.e. we assume our data was sampled IID). To fit the unknown parameters we use MLE. We may choose to use the conditional density <span class="math inline">\(p(Y|\vec{X})\)</span> in the likelihood and this would make it <strong><em>conditional</em></strong> MLE. Or we may choose the unconditional multivariate density <span class="math inline">\(p(Y, \vec{X})\)</span>. However, if we assume that the marginal distribution of <span class="math inline">\(\vec{X}\)</span>, (i.e. <span class="math inline">\(p(\vec{X})\)</span>), does not depend on either <span class="math inline">\(\beta\)</span> or <span class="math inline">\(\sigma\)</span> then because <span class="math inline">\(p(Y,\vec{X}) = p(Y|\vec{X})p(\vec{X})\)</span> building the likelihood using either <span class="math inline">\(p(Y|\vec{X})\)</span> or <span class="math inline">\(p(Y,\vec{X})\)</span> will lead to the same maximization problem because they differ by a constant factor (constant in <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span> that is). So we will use the conditional density <span class="math inline">\(p(Y|\vec{X})\)</span>.</p>
<p>Because the data is assumed to be generated IID then the full likelihood of the data is</p>
<p><span class="math display">\[
\mathcal{L}(\beta, \sigma\ |\ \{ (Y_i, \vec{X}_i)\}) = \prod_{i = 1}^N p(Y_i|\vec{X}_i) = \prod_{i=1}^N\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\big(\frac{Y_i - \langle \vec{X}_i  , \beta\rangle}{\sigma}\big)^2} = \frac{1}{(\sigma^22\pi)^{N/2}}e^{\sum_{i=1}^N-\frac{1}{2}\big(\frac{Y_i - \langle \vec{X}_i  , \beta\rangle}{\sigma}\big)^2}
\]</span>
Because the function <span class="math inline">\(f(x) := -\log(x)\)</span> is decreasing we may instead minimize the negative of the log of this expression:
<span class="math display">\[
-\log(\mathcal{L}(\beta,\sigma\ |\ \{ (Y_i, \vec{X}_i)\})) = \frac{N}{2}\log(\sigma^22\pi) + \frac{1}{2\sigma^2}\sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i  , \beta\rangle\bigg)^2
\]</span>
We first minimize with respect to <span class="math inline">\(\beta\)</span> as this is necessary to do first before finding the minimizing value of <span class="math inline">\(\sigma\)</span>. To do this we need to minimize the only term that depends on <span class="math inline">\(\beta\)</span>, namely the sum of squares <span class="math inline">\(SSE(\beta) := \sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)</span> (hence Least Squares regression).</p>
<div id="a-geometric-interlude" class="section level4">
<h4>A geometric interlude</h4>
<p>Before we do that, let’s think about what the expression <span class="math inline">\(SSE(\beta)\)</span> is. The term <span class="math inline">\(\langle \vec{X}_i , \beta\rangle\)</span> is linear in the unknowns <span class="math inline">\(\beta\)</span>, and hence so is <span class="math inline">\(Y_i - \langle \vec{X}_i , \beta\rangle\)</span>. Therefore, the square <span class="math inline">\(\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)</span> is quadratic in <span class="math inline">\(\beta\)</span>. Thus since the full expression <span class="math inline">\(\sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)</span> is a sum of quadratic functions in <span class="math inline">\(\beta\)</span> it too is a quadratic function in <span class="math inline">\(\beta\)</span>. Since all terms in the sum are squares, the full sum is never negative and its graph in <span class="math inline">\(\beta\)</span> is a <a href="https://en.wikipedia.org/wiki/Paraboloid">non-hyperbolic paraboloid</a>. Usually such shapes look like bowls. However, some can degenerate so that they become flat in one or more directions. Here’re some examples in R:</p>
<p>Non-degenarte paraboloids look like this:</p>
<pre class="r"><code>nice.paraboloid = function(x,y)
{
    return(x^2+0.5*y^2)
}
x = y = seq(from = -4, to = 4, by = 0.2)
z = outer(x, y, nice.paraboloid)

persp(x, y, z,
      main=&quot;Plot of a Non-degenerate 2D Paraboloid&quot;,
      theta = 30, phi = 15,
      col = &quot;springgreen&quot;, shade = 0.5)</code></pre>
<p><img src="/post/004_Regularization/main_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>We can see such a paraboloid is bowl shaped. More technically it’s strictly convex, with a clear unique minimum point. However, paraboloids can degenerate so that they flatten out in some directions:</p>
<pre class="r"><code>degenerate.paraboloid = function(x,y)
{
    return(x^2) #Does not change value as y changes
}
x = y = seq(from = -4, to = 4, by = 0.2)
z = outer(x, y, degenerate.paraboloid)

persp(x, y, z,
      main=&quot;Plot of a Degenerate 2D Paraboloid&quot;,
      theta = 30, phi = 15,
      col = &quot;springgreen&quot;, shade = 0.5)</code></pre>
<p><img src="/post/004_Regularization/main_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>In this example we changed the coefficient of <span class="math inline">\(y\)</span> from 0.5 to 0. The result is that in the <span class="math inline">\(y\)</span>-direction the paraboloid flattened out and it no longer looks bowl shaped. Instead there are infinitely many minimum points all on the axis <span class="math inline">\(\{(x,y): x = 0\}\)</span>. Note that if instead of making the coefficient of <span class="math inline">\(y\)</span> equal to 0 we had made it a positive number very close to zero then the mimima would become unique but would become hard to distinguish from nearby points:</p>
<pre class="r"><code>tricky.paraboloid = function(x,y)
{
    return(x^2+ 0.05*y^2) #Notice the coefficient of y is quite small
}
x = y = seq(from = -4, to = 4, by = 0.2)
z = outer(x, y, tricky.paraboloid)

persp(x, y, z,
      main=&quot;Plot of a Nearly-Degenerate 2D Paraboloid&quot;,
      theta = 30, phi = 15,
      col = &quot;springgreen&quot;, shade = 0.5)</code></pre>
<p><img src="/post/004_Regularization/main_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>These pictures show what can go wrong with the minima of quadratic functions like <span class="math inline">\(SSE(\beta)\)</span> and why regularization may be needed. Now to get back to the minimizing the sum of squares <span class="math inline">\(SSE(\beta) = \sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i , \beta\rangle\bigg)^2\)</span>. If you’re reading this article I’m going to assume you’ve seen this derivation before so I’ll move a bit fast.</p>
<p>First we define <span class="math inline">\(Y\in \mathbb{R}^{N\times 1}\)</span> with <span class="math inline">\(i\)</span>-th component equal to <span class="math inline">\(Y_i\)</span>. (Note an abuse of notation we are making: earlier <span class="math inline">\(Y\)</span> denoted a real valued random variable, but now we are using the same symbol to denote the vector of the <span class="math inline">\(N\)</span> realizations of this random variable.) In addition, let <span class="math inline">\(X\in \mathbb{R}^{N\times p}\)</span> with <span class="math inline">\(i\)</span>-th row equal to <span class="math inline">\(\vec{X}_i\)</span>. Then in matrix notation</p>
<p><span class="math display">\[
\sum_{i=1}^N\bigg(Y_i - \langle \vec{X}_i  , \beta\rangle\bigg)^2 = (Y - X\beta)^T(Y - X\beta)
\]</span>
Let <span class="math inline">\(\hat{\beta} = \text{argmax}_{\beta} \ \ (Y - X\beta)^T(Y - X\beta)\)</span> be the sought after minimizer. Since <span class="math inline">\(\hat{\beta}\)</span> is a minizer in the interior of the domain of <span class="math inline">\(SSE(\beta)\)</span>, the gradient of <span class="math inline">\(SSE(\beta)\)</span> at <span class="math inline">\(\hat{\beta}\)</span> must be 0:</p>
<p><span class="math display">\[
-2X^TY + 2X^TX\hat{\beta} = 0
\]</span>
therefore we obtain the normal equations</p>
<p><span class="math display">\[
X^TX\hat{\beta} = X^TY
\]</span></p>
<p>This is basically the same linear algebra problem as before: If the inverse <span class="math inline">\((X^TX)^{-1}\)</span> existed and was numerically nice then we can solve for <span class="math inline">\(\hat{\beta} = (X^TX)^{-1}X^TY\)</span>. However, if this matrix inverse does not exist (as can happen when we do not have enough rows/samples for the given number of columns/unknowns) then this formula is not useful.</p>
<p>But as before we can simply regularize by replacing the matrix <span class="math inline">\(X^TX\)</span> by <span class="math inline">\(X^TX + \lambda I\)</span> for some sufficiently large <span class="math inline">\(\lambda\)</span>. Actually since <span class="math inline">\(X^TX\)</span> is non-negative definite<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> and symmetric all of it’s eigenvalues are non-negative. So any <span class="math inline">\(\lambda &gt; 0\)</span> would be sufficient to shift the eigenvalues into positive numbers. Now the regularized problem becomes <span class="math inline">\((X^TX + \lambda I)\hat{\beta} = X^TY\)</span>. Therefore we get the regularized MLE solution:</p>
<p><span class="math display">\[
\hat{\beta}_{reg} := (X^TX + \lambda I)^{-1}X^TY
\]</span></p>
<p>Does this regularized problem correspond to its own minimization problem? Yes! Working backwards, this new problem is equivalent to</p>
<p><span class="math display">\[
-2X^TY + 2X^TX\hat{\beta} + 2\lambda\hat{\beta}= 0
\]</span>
The left had side is the gradient of <span class="math inline">\((Y - X\beta)^T(Y - X\beta) + \lambda\sum_{j=1}^p\beta_j^2 = (Y - X\beta)^T(Y - X\beta) + \lambda \langle \beta, \beta\rangle\)</span> at <span class="math inline">\(\beta = \hat{\beta}\)</span> as can be checked. So the regularized problem corresponds to trying to minimize the expression <span class="math inline">\((Y - X\beta)^T(Y - X\beta) + \lambda\sum_{j=1}^p\beta_j^2 = (Y - X\beta)^T(Y - X\beta) + \lambda \langle \beta, \beta\rangle\)</span>. This of course is <span class="math inline">\(L^2\)</span>-regularization. Thus we have derived <span class="math inline">\(L^2\)</span>-regularization for OLS simply by seeking to transform the inverse problem that arises in OLS so that it may satisfy the Hadamard conditions.</p>
</div>
</div>
<div id="an-illustrative-example" class="section level3">
<h3>An Illustrative Example</h3>
<p>Below we can see geometrically what regularization does. The sum of squares expression <span class="math inline">\((Y - X\beta)^T(Y - X\beta)\)</span> is quadratic in <span class="math inline">\(\beta\)</span>, but may have a graph that is a degenerate paraboloid. This is what causes it to have multiple minimizers in OLS and what makes the matrix <span class="math inline">\(X^TX\)</span> singular (more on this point in the next section). However the expression <span class="math inline">\(\lambda\langle \beta, \beta\rangle = \lambda\sum_{j = 1}^p\beta_j^2\)</span> is a strictly positive-definite quadratic form. Its graph is a non-degenerate bowl shaped paraboloid.</p>
<p>Adding a non-degenerate paraboloid to something that is not bowl shaped makes the second graph more bowl shaped! Moreover it shifts the minimum of the 2nd graph towards the mimimum of the bowl. As an illustration, let’s take a look at an example where this is easy to see.</p>
<pre class="r"><code>rm(list = ls())
bumpy.function = function(x,y)
{
    return(sin(x)+sin(y))
}

nice.paraboloid = function(x,y)
{
    return(0.15*(x^2 + y^2)) #lambda = 0.15 is used
}

x = y = seq(from = -4, to = 4, by = 0.2)
bumpy = outer(x, y, bumpy.function)
paraboloid = outer(x, y, nice.paraboloid)
bumpy.plus.paraboloid = bumpy + paraboloid

persp(x, y, bumpy,
      main=&quot;Graph of a bumpy function with multiple minima&quot;,
      theta = 30, phi = 15,
      col = &quot;springgreen&quot;, shade = 0.5)</code></pre>
<p><img src="/post/004_Regularization/main_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>persp(x, y, paraboloid,
      main=&quot;Graph of a nice paraboloid&quot;,
      theta = 30, phi = 15,
      col = &quot;springgreen&quot;, shade = 0.5)</code></pre>
<p><img src="/post/004_Regularization/main_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<pre class="r"><code>persp(x, y, bumpy.plus.paraboloid,
      main=&quot;Graph of a regularized bumpy function = bumpy function + paraboloid&quot;,
      theta = 30, phi = 15,
      col = &quot;springgreen&quot;, shade = 0.5)</code></pre>
<p><img src="/post/004_Regularization/main_files/figure-html/unnamed-chunk-4-3.png" width="672" /></p>
<p>Here we see that the most important geometric aspect of the regularizing term <span class="math inline">\(\lambda\langle\beta, \beta\rangle\)</span> is the fact that it is <a href="https://en.wikipedia.org/wiki/Convex_function">strictly convex</a>!! <strong><em>Although we will not dwell on it, it is impossible to overstate the theoretical importance of the previous sentence.</em></strong> As a matter of fact, geometrically speaking it’s clear that had we added <strong>any</strong> strictly convex function to the bumpy function we would have gotten something more bowl shaped. We will not go further into it here but you should know that <a href="https://en.wikipedia.org/wiki/Convex_analysis">convexity is one of those properties in mathematics out of which entire fields are created</a>.</p>
</div>
<div id="the-hessian-matrix-and-more-complex-models" class="section level3">
<h3>The Hessian matrix and more complex models</h3>
<p>In the OLS problem above the Maximum Likelihood estimator turned out to be the one that minimized the sum of squares expression <span class="math inline">\(SSE(\beta) := (Y - X\beta)^T(Y - X\beta)\)</span>. This expression can be expanded in matrix notation as</p>
<p><span class="math display">\[
Y^TY - Y^TX\beta - \beta^TX^TY + \beta^TX^TX\beta
\]</span></p>
<p>We see that there is a quadratic term (<span class="math inline">\(\beta^TX^TX\beta\)</span>) and the rest are terms with powers of <span class="math inline">\(\beta\)</span> less than 2. The matrix of second derivatives of this expression (known as the <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian matrix</a>) is therefore just the matrix of coefficients of this quadratic term: <span class="math inline">\(X^TX\)</span>. This Hessian is exactly what was the star of the show in the OLS problem!</p>
<p>The Hessian of a function at a point tells us the convexity of the function at the point. <a href="https://en.wikipedia.org/wiki/Second_partial_derivative_test">If the Hessian is positive definite, then near the minimizing point the function is bowl shaped. If the Hessian is negative definite then near a maximizing point the function is shaped like an upside down bowl.</a></p>
<p>Moreover, The Hessian is always a symmetric matrix by the <a href="https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives#Formal_expressions_of_symmetry">equality of cross-derivatives</a>. So the previous point is really a statement about Hessian’s eigenvalues. In short, if the Hessian has all positive eigenvalues then it is positive definite and the function is bowl shaped near its minimizer.</p>
<p>The OLS Hessian matrix <span class="math inline">\(X^TX\)</span> is symmetric and non-negative definite, so the graph of the sum of squares <span class="math inline">\(SSE(\beta) := (Y - X\beta)^T(Y - X\beta)\)</span> can only fail to be bowl shaped near the minimizer <span class="math inline">\(\hat{\beta}\)</span> if the matrix <span class="math inline">\(X^TX\)</span> has eigenvalues that are equal 0 (or positive but close to 0 in the case numerical instability). In which case, the graph of <span class="math inline">\(SSE(\beta)\)</span> is a degenerate non-hyperbolic paraboloid and there are multiple minimizing solutions <span class="math inline">\(\beta\)</span>.</p>
<p><strong>Thus regularizing the matrix <span class="math inline">\(X^TX\)</span> is just regularizing the Hessian matrix of the function <span class="math inline">\(SSE(\beta)\)</span> we want to minimize!!</strong> This fact is what allows us to take the idea beyond OLS.</p>
<p>Indeed if <span class="math inline">\(f(\beta)\)</span> is any 2nd order differentiable cost function in any machine learning model then by the linearity of the derivative</p>
<p><span class="math display">\[
\text{Hessian}(f + \lambda \langle \beta, \beta\rangle) = \text{Hessian}(f) + \lambda\text{Hessian}(\langle \beta, \beta\rangle) = \text{Hessian}(f) + \lambda I
\]</span></p>
<p>There we used the fact that <span class="math inline">\(\text{Hessian}(\langle \beta, \beta\rangle) = \text{Hessian}(\sum_i^p \beta_i^2) = I\)</span>. Because <span class="math inline">\(f\)</span> is almost arbitrary we see that we can apply <span class="math inline">\(L^2\)</span>-regularization to a very large family of problems, with the goal being to regularize the Hessian of <span class="math inline">\(f\)</span>. As an example let’s look at some other kinds of regression problems. For OLS we assumed the conditional distribution <span class="math inline">\(Y \ \ | \ \ \vec{X} \sim \mathcal{N}(\ \langle \vec{X} , \beta\rangle \ ,\ \sigma^2)\)</span>, but we may have chosen a different conditional distribution.</p>
<p>If <span class="math inline">\(Y\)</span> takes on only values in the set <span class="math inline">\(\{0,1\}\)</span> then it is a Bernoulli random variable. This is the case in logistic regression where the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\vec{X}\)</span> is
<span class="math display">\[
Y \ \ | \ \ \vec{X} \sim \mathcal{B}(\ p = \phi(\langle \vec{X}  , \beta\rangle) \ )
\]</span></p>
<p>where <span class="math inline">\(\mathcal{B(p)}\)</span> is a Bernoulli distribution with probability of a positive event equal to <span class="math inline">\(p\)</span>, <span class="math inline">\(p = \text{E}[Y|\vec{X}] = \phi(\langle \vec{X} , \beta\rangle)\)</span> is the probability of a positive event given <span class="math inline">\(\vec{X}\)</span>, and <span class="math inline">\(\phi(t) = \frac{e^t}{1+e^t}\)</span> is the standard logit. In this case the conditional density <span class="math inline">\(p(Y|\vec{X})\)</span> can be written as</p>
<p><span class="math display">\[
p(Y|\vec{X}) = p^Y\big(1 - p)\big)^{1-Y} =\phi(\langle \vec{X}  , \beta\rangle)^Y\big(1 - \phi(\langle \vec{X}  , \beta\rangle)\big)^{1-Y}
\]</span>
So the negative log-likelihood is given by</p>
<p><span class="math display">\[
-\mathcal{l}(\beta) = -\sum_{i=1}^N Y_i\log(\phi(\langle \vec{X}_i  , \beta\rangle)) + (1-Y_i)\log(1-\phi(\langle \vec{X}_i  , \beta\rangle))
\]</span></p>
<p>This function may or may not look bowl shaped (i.e. strictly convex) near the <span class="math inline">\(\hat{\beta}\)</span> that minimizes it. In case it doesn’t we can make it so by adding <span class="math inline">\(\lambda \langle \beta, \beta\rangle = \lambda\sum_{j = 1}^p\beta_j^2\)</span> for some sufficiently large <span class="math inline">\(\lambda\)</span> and minimizing this new problem. The same applies to generalized linear models, neural networks, etc.</p>
</div>
</div>
<div id="bias-variance-trade-offs-and-regularization" class="section level2">
<h2>Bias-Variance trade offs and Regularization</h2>
<p>Above we used regularization methods to make a problem “nicer” in the numerical sense (i.e. satisfying Hadamard’s conditions). But what does “nicer” mean in the statistical context? That is a multifaceted question. The first step is to recognize that what might be viewed as instability from the numerical point of view, can be understood as high variance from the statistical point of view.</p>
<p>We illustrate with the OLS estimator. Suppose that the matrix <span class="math inline">\(X^TX\)</span> is indeed invertible. The standard OLS estimator is the random vector given by the normal equations:</p>
<p><span class="math display">\[
\hat{\beta} = (X^TX)^{-1}X^TY
\]</span></p>
<p>We see that this is an unbiased estimator of <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\text{E}[\hat{\beta}] = \text{E}\big[\text{E}[\hat{\beta}|X]\big] = \text{E}\bigg[(X^TX)^{-1}X^T\text{E}[Y|X]\bigg] = \text{E}\bigg[(X^TX)^{-1}X^TX\beta\bigg] = \text{E}[\beta] = \beta
\]</span></p>
<p>Moreover, it’s easy enough to compute the conditional covariance matrix of <span class="math inline">\(\hat{\beta}\)</span>:
<span class="math display">\[
\text{Var}[\hat{\beta}|X] = (X^TX)^{-1}X^T\cdot \text{Var}[Y|X] \cdot X(X^TX)^{-1} = (X^TX)^{-1}X^T\cdot \sigma^2 I \cdot X(X^TX)^{-1} 
\]</span></p>
<p><span class="math display">\[
= \sigma^2 (X^TX)^{-1}
\]</span></p>
<p>The <a href="https://en.wikipedia.org/wiki/Law_of_total_variance">unconditional covariance matrix can be computed as</a></p>
<p><span class="math display">\[
\text{Var}[\hat{\beta}] = \text{E}[\text{Var}[\hat{\beta}|X]] + \text{Var}[\text{E}[\hat{\beta}|X]] 
\]</span></p>
<p><span class="math display">\[
= \sigma^2\text{E}[(X^TX)^{-1}] + \text{Var}[\beta] = \sigma^2\text{E}[(X^TX)^{-1}]
\]</span></p>
<p>This is harder to compute because it depends on the distribution of the random matrix <span class="math inline">\(X\)</span>. Regardless, we can see that what controls the variance of the estimator <span class="math inline">\(\hat{\beta}\)</span> (whether conditional or not) is the inverse matrix <span class="math inline">\((X^TX)^{-1}\)</span>. This is interesting because it shows that the matrix we identified as the Hessian of the OLS cost function (<span class="math inline">\(X^TX\)</span>) is also the matrix that controls the covariance of the OLS estimator.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>If any of the eigenvalues of the matrix <span class="math inline">\(X^TX\)</span> were “close” to <span class="math inline">\(0\)</span> then the eigenvalues of the inverse will be very large, causing the variance of <span class="math inline">\(\hat{\beta}\)</span> to be very large. If you’re familiar with VIFs, this is what causes large <a href="https://en.wikipedia.org/wiki/Variance_inflation_factor">variance inflation factors</a>.</p>
<p>Regularization is used to reduce the variance in this estimator. If we denote the regularized estimator by:
<span class="math display">\[
\hat{\beta}_{reg} = (X^TX + \lambda I)^{-1}X^TY
\]</span>
Then this estimator is biased away from <span class="math inline">\(\beta\)</span>. To see this we first compute the conditional mean:</p>
<p><span class="math display">\[
\text{E}[\hat{\beta}_{reg}|X] = (X^TX + \lambda I)^{-1}X^TX\beta
\]</span></p>
<p><span class="math display">\[
= (X^TX + \lambda I)^{-1}(X^TX + \lambda I)\beta - \lambda (X^TX + \lambda I)^{-1}\beta
\]</span></p>
<p><span class="math display">\[
= \beta - \lambda (X^TX + \lambda I)^{-1}\beta
\]</span></p>
<p>Hence
<span class="math display">\[
\text{E}[\hat{\beta}_{reg}] = \beta - \lambda\text{E}\big[(X^TX + \lambda I)^{-1}\big]\beta
\]</span>
which is “<span class="math inline">\(\beta\)</span> minus something” and hence not equal to <span class="math inline">\(\beta\)</span>. However the effect on the variance is better:</p>
<p><span class="math display">\[
\text{Var}[\hat{\beta}_{reg}|X] = (X^TX + \lambda I)^{-1}X^T\cdot\text{Var}[Y|X]\cdot X(X^TX + \lambda I)^{-1}
\]</span></p>
<p><span class="math display">\[
= \sigma^2(X^TX + \lambda I)^{-1}X^TX(X^TX + \lambda I)^{-1}
\]</span></p>
<p><span class="math display">\[
= \sigma^2(X^TX + \lambda I)^{-1}(X^TX+\lambda I)(X^TX + \lambda I)^{-1} - \sigma^2\lambda(X^TX + \lambda I)^{-2}
\]</span></p>
<p><span class="math display">\[
=  \sigma^2(X^TX + \lambda I)^{-1} - \sigma^2\lambda(X^TX + \lambda I)^{-2}
\]</span>
This variance formula may look messy but the gist is that instead of inverting <span class="math inline">\(X^TX\)</span> we are inverting <span class="math inline">\(X^TX + \lambda I\)</span>. The matrix <span class="math inline">\(X^TX + \lambda I\)</span> has larger eigenvalues than the matrix <span class="math inline">\(X^TX\)</span>. Therefore <span class="math inline">\(\text{Var}[\hat{\beta}_{reg}|X] = \sigma^2(X^TX + \lambda I)^{-1} - \sigma^2\lambda(X^TX + \lambda I)^{-2}\)</span> is smaller than <span class="math inline">\(\text{Var}[\hat{\beta}|X] = \sigma^2 (X^TX)^{-1}\)</span> in the sense that it has smaller eigenvalues. <strong>Thus regularization has increased bias, but reduced variance.</strong> Similar effects hold for more complex models than OLS, but instead of chasing formulas the read should try cooking up some numerical examples via Monte Carlo.</p>
</div>
<div id="what-about-the-bayesian-view-point" class="section level2">
<h2>What about the Bayesian view point?</h2>
<p>A very natural perspective on regularization can be found in Bayesian modeling, where regularization terms amount to simply specifying prior distributions. However, this is standard Bayesian theory and this post is already long enough :P</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Indeed suppose <span class="math inline">\(v\in \mathbb{R}^p\)</span> is any vector. Then <span class="math inline">\(\langle v, X^TXv\rangle = \langle Xv,Xv\rangle = ||Xv||^2 \ge 0\)</span>. Hence <span class="math inline">\(X^TX\)</span> is always non-negative definite.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>This is a general feature of Maximum Likelihood estimators called “<em>asymptotic efficiency</em>”, where the covariance matrix of the MLE estimator approaches a “best possible” covariance matrix as the sample size increases. Essentially the best possible covariance matrix that an unbiased estimator of <span class="math inline">\(\beta\)</span> can have is given given by the <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">Cramer-Rao Bound</a> and is determined by the inverse of the <a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher Information matrix</a>, whose <span class="math inline">\(ij\)</span>-component is <span class="math inline">\(-\text{E}[\partial^2\log(p(Y, \vec{X}| \beta))/\partial \beta_i\partial \beta_j]\)</span>. The beauty is that the Hessian of the negative loglikelihood is the sample estimator of this Fisher Information (where expectation is replaced by average over samples). This is why the Hessian is showing up as the determining factor in estimator covariance.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
</content:encoded>	
    </item>
    
    <item>
      <title>Expectation Maximization, Part 1: Motivation and Recipe</title>
      <link>/post/003_em1/main/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/003_em1/main/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is the first in a series of posts on Expectation Maximization (EM) type algorithms. Our goal will be to motivate some of the theory behind these algorithms. In later posts we will implement examples in C++, often with the help of the &lt;a href=&#34;http://eigen.tuxfamily.org/index.php?title=Main_Page&#34;&gt;Eigen&lt;/a&gt; linear algebra library.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-likelihood&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximum likelihood&lt;/h2&gt;
&lt;p&gt;A large subset of statistics is concerned with determining properties of a distribution by using data that is assumed to be generated by that distribution. A common example is &lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34;&gt;Maximum Likelihood Estimation&lt;/a&gt; (MLE). Here one assumes that a vector of observed data &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\in\mathbb{R}^N\)&lt;/span&gt; is the realization of a random vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; with a probability density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{X} \ | \ \theta)\)&lt;/span&gt; that depends on a vector of parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. MLE amounts to estimating &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; with the value that makes this probability density has high as possible for the observed data:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} := \text{argmax}_{\theta} \ \ p(\vec{x} \ | \ \theta)
\]&lt;/span&gt;
As a function of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the density &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}(\theta; \vec{x}) := p(\vec{x} \ | \ \theta)\)&lt;/span&gt; is called the likelihood. Because probability densities are positive for the realized values &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, the above problem is equivalent to maximizing the logarithm of the likelihood:
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{x} \ | \ \theta))
\]&lt;/span&gt;
(The main practical reason behind this log transformation is that it often makes the problem easier numerically. The theoretical advantage is that it ties MLE to the theory of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fisher_information&#34;&gt;Fisher Information&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dependence-structures-and-problems-with-hidden-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dependence structures and problems with hidden variables&lt;/h2&gt;
&lt;p&gt;The situation in the last section can be summarized by the simple dependence structure (or &lt;em&gt;Markov&lt;/em&gt; diagram) &lt;span class=&#34;math inline&#34;&gt;\(\theta \to \vec{X}\)&lt;/span&gt;. That is, given the value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; we can determined the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, namely &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{X} \ | \ \theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, in many applications we may only have partial observations of the data we want, with some of the relevant information remaining unobserved/hidden. For example, suppose 100 identical and independent dice are thrown in an experiment. The dice are not necessarily uniformly weighted, with probabilities of landing 1,2,…,6 given by &lt;span class=&#34;math inline&#34;&gt;\(\theta = [p_1, p_2,...,p_6]\)&lt;/span&gt;. Suppose the dice land with values represented by &lt;span class=&#34;math inline&#34;&gt;\(\vec{X} = [X_1, X_2, ... X_{100}]\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; being the number the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; die lands on. Suppose also that in the experiment we are only able to observe whether each die landed on an even or odd number. That is, we observe a vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}\)&lt;/span&gt; given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = X_i \mod 2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i \in \{1, 2, ... 100\}\)&lt;/span&gt;. In this case the dependence structure is a little more complex: &lt;span class=&#34;math inline&#34;&gt;\(\theta \to \vec{X} \to \vec{Y}\)&lt;/span&gt;. That is, once we know the value of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; we can fully specify the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}\)&lt;/span&gt; without knowing the value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. We would have the Markov property for densities:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(\vec{y} \ | \ \vec{x}, \theta) = p(\vec{y} \ | \ \vec{x})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In general, we have a dependence structure given by &lt;span class=&#34;math inline&#34;&gt;\(\theta \to \vec{X} \to \vec{Y}\)&lt;/span&gt;, we observe only &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}=\vec{y}\)&lt;/span&gt; and we want to estimate the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. The MLE estimator would be:
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{y} \ | \ \theta))
\]&lt;/span&gt;
All of the theory of MLE applies in this case. In the example above this would be relatively easy. However, there are times this maximization problem is very difficult. Often the density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{y} \ | \ \theta)\)&lt;/span&gt; may be much more complicated than the density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x} \ | \ \theta)\)&lt;/span&gt; of the hidden data that we wish we had.&lt;/p&gt;
&lt;p&gt;In such as situation, if we knew &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; then we can replace the above problem with &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt;. In fact, we wouldn’t even need to know exactly what the value of &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; is but only what the value of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; is for a given &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-general-recipe-for-em-algorithms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A general recipe for EM algorithms&lt;/h2&gt;
&lt;p&gt;The idea of EM is indeed to try and maximize &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{y} \ | \ \theta))\)&lt;/span&gt;, but because we do not know &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; to instead use an approximation/estimate of it.&lt;/p&gt;
&lt;p&gt;How to approximate such an expression? The quantity &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; is a random variable (depending on the unknown value &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;). To estimate it in a meaningful way we need to use the most informative distribution related to &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt;. Because we know &lt;span class=&#34;math inline&#34;&gt;\(\vec{y}\)&lt;/span&gt; the best such distribution is &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The problem is this distribution (or any other) will necessarily depend &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, which we do not know! At first this seems like a circular trap, because to estimate &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta)\)&lt;/span&gt; we need to know &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, but to estimate &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; we need to know &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt;. However the trap hints at a solution: simply alternate between estimating the random variable &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; using a current guess of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and then use this updated estimate of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; to update our guess of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. More formally we can summarize EM in 5 steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(m = 0\)&lt;/span&gt;. Make an initial estimate &lt;span class=&#34;math inline&#34;&gt;\(\theta_m\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Given the observed data &lt;span class=&#34;math inline&#34;&gt;\(\vec{y}\)&lt;/span&gt; and pretending for the moment that our current guess &lt;span class=&#34;math inline&#34;&gt;\(\theta_m\)&lt;/span&gt; is correct, construct the conditional probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt; of the hidden data &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; given all known information.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Using the distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt; construct an estimator/approximation of the desired log-likelihood &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt; for arbitrary &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. We denote this approximation by &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Set &lt;span class=&#34;math inline&#34;&gt;\(\theta_{m+1}\)&lt;/span&gt; equal to a value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that maximizes the current approximation &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m)\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Return to step 2 and repeat until some stopping criteria is met.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Practically speaking, this algorithm would be applied when each of these steps is significantly easier than the original MLE problem of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{y} \ | \ \theta))\)&lt;/span&gt;. As a general example, this is often the case when the model is linear with respect to &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt;, but the information loss of going from &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}\)&lt;/span&gt; is nonlinear and non-invertible (we’ll give examples in later posts).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;constructing-an-estimator-for-logpvecx-theta&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Constructing an estimator for &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{X} \ | \ \theta))\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;How do we fill in the blank left by step 3 above? That is, how do we use the probability density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt; to estimate the value of the random variable &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{X} \ | \ \theta))\)&lt;/span&gt;? Two possibilities come to mind.&lt;/p&gt;
&lt;div id=&#34;point-estimate-type-em&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Point-estimate type EM&lt;/h3&gt;
&lt;p&gt;One possibility is to let
&lt;span class=&#34;math display&#34;&gt;\[
\vec{x}_m = \vec{x}_m(\vec{y}, \theta_m) := \text{argmax}_{\vec{x}} \ p(\vec{x}|\vec{y},\theta_m)
\]&lt;/span&gt;
and then define
&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta | \theta_m) := \log(p(\vec{x}_m \ | \ \theta))
\]&lt;/span&gt;
This is called point-estimate EM. Here we use “the most likely” value of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; as determined by the density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt; and then impute this value into our log-likelihood that we want to maximize. Another possibility would be to let&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{x}_m = \vec{x}_m(\vec{y}, \theta_m) := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \vec{X}\big]
\]&lt;/span&gt;
and let &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; be as before.&lt;/p&gt;
&lt;p&gt;The idea of these type of EM algorithms is to first estimate the missing data &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; and then impute the result into &lt;span class=&#34;math inline&#34;&gt;\(log(p(\vec{x}\ |\ \theta))\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;expectation-em-i.e.-standard-em&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Expectation EM (i.e. standard EM)&lt;/h3&gt;
&lt;p&gt;There is a theoretically more elegant way. As mentioned earlier, we do not in fact need an estimate of the missing data &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt;. One of the best ways to estimate the value of a random variable with respect to a conditional distribution is to simply compute the conditional expectation of that variable with respect to that conditional distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta | \theta_m) \ := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] = \int_{\mathcal{X}} \log(p(\vec{x} \ | \ \theta)) \ p(\vec{x}|\vec{y},\theta_m) \ d\vec{x} 
\]&lt;/span&gt;
Here we’re computing the mean of &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{X} \ | \ \theta))\)&lt;/span&gt; with respect to the density &lt;span class=&#34;math inline&#34;&gt;\(p(\vec{x}|\vec{y},\theta_m)\)&lt;/span&gt;. As is common when using expectations, this second method has some advantages we’ll see later. When we refer to EM we will always mean this case, unless otherwise specified.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;qthetatheta_m-for-i.i.d.-samples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m)\)&lt;/span&gt; for I.I.D. samples&lt;/h2&gt;
&lt;p&gt;So far everything has been rather general, applying to any random vectors &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\vec{Y}\)&lt;/span&gt;. Many problems assume that data are generated independently and identically distributed (I.I.D.) so it’s helpful to have a formulation for this particular case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proposition:&lt;/strong&gt; Suppose that the components of &lt;span class=&#34;math inline&#34;&gt;\(\vec{X}\)&lt;/span&gt; are IID (given &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;), that is:
&lt;span class=&#34;math display&#34;&gt;\[
p(\vec{x}|\theta) = \prod_{i = 1}^N p(x_i|\theta) \  \qquad \forall x, \theta
\]&lt;/span&gt;
Suppose also that the dependence structure &lt;span class=&#34;math inline&#34;&gt;\(\theta \to \vec{X} \to \vec{Y}\)&lt;/span&gt; splits into subgraphs as &lt;span class=&#34;math inline&#34;&gt;\(\theta \to X_i \to Y_i\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i = 1, 2, ..., N\)&lt;/span&gt;. This just means that given &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;, the distribution &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; is independent of all other variables:
&lt;span class=&#34;math display&#34;&gt;\[
p(y_i|\vec{x}, \theta, y_1, y_2, ..., y_{i-1}, y_{i+1}, ..., y_N) = p(y_i|x_i)
\]&lt;/span&gt;
Then &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m):= \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m} \big[ \log(p(\vec{X} \ | \ \theta)) \big]\)&lt;/span&gt; can be written as:
&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta|\theta_m) = \sum_{i=1}^N Q_i(\theta|\theta_m)
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
Q_i(\theta|\theta_m) := \ \text{E}_{X_i \ | \ Y_i \ = \ y_i, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big].
\]&lt;/span&gt;
&lt;strong&gt;Proof:&lt;/strong&gt; The proof begins by showing that the joint elements &lt;span class=&#34;math inline&#34;&gt;\((X_i,Y_i)\)&lt;/span&gt; are independent across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, that is:
&lt;span class=&#34;math display&#34;&gt;\[
p(\vec{x},\vec{y}|\theta) = \prod_{i = 1}^N p(x_i, y_i|\theta). 
\]&lt;/span&gt;
To prove this we start by applying the multiplication theorem for probability densities:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(\vec{x},\vec{y}|\theta) = p(y_1|y_2,y_3, ..., y_N, \vec{x},\theta)...p(y_N|\vec{x},\theta)p(\vec{x}|\theta) \qquad \text{(by the multiplication theorem)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= p(y_1|x_1,\theta)...p(y_N|x_N,\theta)p(\vec{x}|\theta) \qquad \text{(by conditional independence but keeping theta)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= p(\vec{x}|\theta)\prod_{i=1}^Np(y_i|x_i,\theta) 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
=\prod_{i=1}^Np(y_i|x_i,\theta)p(x_i|\theta) \qquad \text{(by independence of the x&amp;#39;s)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \prod_{i=1}^Np(y_i,x_i|\theta).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Next we have that for each &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  p(x_i|\vec{y},\theta) = \frac{p(x_i,\vec{y}|\theta)}{p(\vec{y}|\theta)} \qquad \text{(by Bayes)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \frac{\int p(\vec{x},\vec{y}|\theta)dx_1...dx_{i-1}dx_{i+1}...dx_n}{\int p(\vec{x},\vec{y}|\theta)d\vec{x}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \frac{\int \prod_{j=1}^Np(y_j,x_j|\theta)dx_1...dx_{i-1}dx_{i+1}...dx_n}{\int \prod_{j=1}^Np(y_j,x_j|\theta)d\vec{x}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \frac{p(x_i,y_i|\theta) \prod_{j=1, j \ne i}^N \int p(y_j,x_j|\theta)dx_j}{\prod_{j=1}^N\int p(y_j,x_j|\theta) dx_j}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \frac{p(x_i,y_i|\theta) \prod_{j=1, j \ne i}^N p(y_j|\theta)}{\prod_{j=1}^N p(y_j|\theta)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \frac{p(x_i,y_i|\theta)}{p(y_i|\theta)}
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
= p(x_i|y_i,\theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence &lt;span class=&#34;math inline&#34;&gt;\(p(x_i|\vec{y},\theta) = p(x_i|y_i,\theta)\)&lt;/span&gt;. Therefore we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta, \theta_m) = \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(\prod_{i = 1}^N p(X_i \ | \ \theta)) \big]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \sum_{i = 1}^N \log(p(X_i \ | \ \theta)) \big]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \sum_{i = 1}^N \text{E}_{X_i \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
= \sum_{i = 1}^N \text{E}_{X_i \ | \ Y_i \ = \ y_i, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big] = \sum_{i=1}^N Q_i(\theta|\theta_m)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where we used &lt;span class=&#34;math inline&#34;&gt;\(p(x_i|\vec{y},\theta) = p(x_i|y_i,\theta)\)&lt;/span&gt; in the 2nd to last equality. &lt;strong&gt;QED&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-a-posteriori-em-and-regularizing-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximum A Posteriori EM and regularizing priors&lt;/h2&gt;
&lt;p&gt;The EM algorithm is easily extendable to regularized MLE. This is usually referred to as Maximum A Posteriori (MAP) in the Bayesian setting, which we adopt. Here the penalty term is interpreted as the log of the prior density on &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, and the function to be maximized is the posterior density of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; given the data. By Bayes’ Theorem
&lt;span class=&#34;math display&#34;&gt;\[
p(\theta|\vec{y}) \propto p(\vec{y}|\theta)p(\theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the proportionality constant is independent of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Thus the MAP estimator is
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta}_{MAP} := \text{argmax}_{\theta} \ \log(p(\theta|\vec{y})) \ = \text{argmax}_{\theta} \ \log(p(\vec{y}|\theta)) + \log(p(\theta))
\]&lt;/span&gt;
The way to extend EM to this situation is clear (at least formally): Simply replace the maximization step (step 4 above) in EM with maximizing &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m) + \log(p(\theta))\)&lt;/span&gt; instead of simply &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta|\theta_m)\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta_{m+1} := \text{argmax}_{\theta} \ \ \ Q(\theta|\theta_m) + \log(p(\theta))
\]&lt;/span&gt;
where as before &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q(\theta | \theta_m) \ := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] = \int_{\mathcal{X}} \log(p(\vec{x} \ | \ \theta)) \ p(\vec{x}|\vec{y},\theta_m) \ d\vec{x} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;monotonicity-of-the-em-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monotonicity of the EM algorithm&lt;/h2&gt;
&lt;p&gt;At this point the reader has all the theory needed to begin applying EM where they believe it’s a good fit. Before we end the post though let’s mention at least one result that shows that EM is indeed a generalization of MLE to the case of hidden data: under relatively weak assumptions of the algorithm causes the log-likelihood &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{y}|\theta_m))\)&lt;/span&gt; to be an &lt;a href=&#34;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Proof_of_correctness&#34;&gt;increasing sequence in &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Step 2 allows for a whole family of such algorithms, one for each possible approximator to &lt;span class=&#34;math inline&#34;&gt;\(\log(p(\vec{x} \ | \ \theta))\)&lt;/span&gt;. Step 4 can also be generalized. Since the value of &lt;span class=&#34;math inline&#34;&gt;\(\theta_{m+1}\)&lt;/span&gt; maximizes &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta | \theta_m)\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta_{m+1} | \theta_m) \ge Q(\theta_m | \theta_m)\)&lt;/span&gt;. Instead of seeking to maximize &lt;span class=&#34;math inline&#34;&gt;\(Q(\theta | \theta_m)\)&lt;/span&gt; we may simply seek a value of &lt;span class=&#34;math inline&#34;&gt;\(\theta_{m+1}\)&lt;/span&gt; that improves on &lt;span class=&#34;math inline&#34;&gt;\(\theta_m\)&lt;/span&gt; in the sense of this inequality. For step 5, the stopping criteria are up to the implementer.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      <content:encoded>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This is the first in a series of posts on Expectation Maximization (EM) type algorithms. Our goal will be to motivate some of the theory behind these algorithms. In later posts we will implement examples in C++, often with the help of the <a href="http://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a> linear algebra library.</p>
</div>
<div id="maximum-likelihood" class="section level2">
<h2>Maximum likelihood</h2>
<p>A large subset of statistics is concerned with determining properties of a distribution by using data that is assumed to be generated by that distribution. A common example is <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximum Likelihood Estimation</a> (MLE). Here one assumes that a vector of observed data <span class="math inline">\(\vec{x}\in\mathbb{R}^N\)</span> is the realization of a random vector <span class="math inline">\(\vec{X}\)</span> with a probability density <span class="math inline">\(p(\vec{X} \ | \ \theta)\)</span> that depends on a vector of parameters <span class="math inline">\(\theta\)</span>. MLE amounts to estimating <span class="math inline">\(\theta\)</span> with the value that makes this probability density has high as possible for the observed data:</p>
<p><span class="math display">\[
\hat{\theta} := \text{argmax}_{\theta} \ \ p(\vec{x} \ | \ \theta)
\]</span>
As a function of <span class="math inline">\(\theta\)</span>, the density <span class="math inline">\(\mathcal{L}(\theta; \vec{x}) := p(\vec{x} \ | \ \theta)\)</span> is called the likelihood. Because probability densities are positive for the realized values <span class="math inline">\(\vec{x}\)</span> of <span class="math inline">\(\vec{X}\)</span>, the above problem is equivalent to maximizing the logarithm of the likelihood:
<span class="math display">\[
\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{x} \ | \ \theta))
\]</span>
(The main practical reason behind this log transformation is that it often makes the problem easier numerically. The theoretical advantage is that it ties MLE to the theory of the <a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher Information</a>).</p>
</div>
<div id="dependence-structures-and-problems-with-hidden-variables" class="section level2">
<h2>Dependence structures and problems with hidden variables</h2>
<p>The situation in the last section can be summarized by the simple dependence structure (or <em>Markov</em> diagram) <span class="math inline">\(\theta \to \vec{X}\)</span>. That is, given the value of <span class="math inline">\(\theta\)</span> we can determined the distribution of <span class="math inline">\(\vec{X}\)</span>, namely <span class="math inline">\(p(\vec{X} \ | \ \theta)\)</span>.</p>
<p>However, in many applications we may only have partial observations of the data we want, with some of the relevant information remaining unobserved/hidden. For example, suppose 100 identical and independent dice are thrown in an experiment. The dice are not necessarily uniformly weighted, with probabilities of landing 1,2,…,6 given by <span class="math inline">\(\theta = [p_1, p_2,...,p_6]\)</span>. Suppose the dice land with values represented by <span class="math inline">\(\vec{X} = [X_1, X_2, ... X_{100}]\)</span> with <span class="math inline">\(X_i\)</span> being the number the <span class="math inline">\(i^{th}\)</span> die lands on. Suppose also that in the experiment we are only able to observe whether each die landed on an even or odd number. That is, we observe a vector <span class="math inline">\(\vec{Y}\)</span> given by</p>
<p><span class="math display">\[
Y_i = X_i \mod 2
\]</span></p>
<p>for <span class="math inline">\(i \in \{1, 2, ... 100\}\)</span>. In this case the dependence structure is a little more complex: <span class="math inline">\(\theta \to \vec{X} \to \vec{Y}\)</span>. That is, once we know the value of <span class="math inline">\(\vec{X}\)</span> we can fully specify the distribution of <span class="math inline">\(\vec{Y}\)</span> without knowing the value of <span class="math inline">\(\theta\)</span>. We would have the Markov property for densities:</p>
<p><span class="math display">\[
p(\vec{y} \ | \ \vec{x}, \theta) = p(\vec{y} \ | \ \vec{x})
\]</span></p>
<p>In general, we have a dependence structure given by <span class="math inline">\(\theta \to \vec{X} \to \vec{Y}\)</span>, we observe only <span class="math inline">\(\vec{Y}=\vec{y}\)</span> and we want to estimate the parameters <span class="math inline">\(\theta\)</span>. The MLE estimator would be:
<span class="math display">\[
\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{y} \ | \ \theta))
\]</span>
All of the theory of MLE applies in this case. In the example above this would be relatively easy. However, there are times this maximization problem is very difficult. Often the density <span class="math inline">\(p(\vec{y} \ | \ \theta)\)</span> may be much more complicated than the density <span class="math inline">\(p(\vec{x} \ | \ \theta)\)</span> of the hidden data that we wish we had.</p>
<p>In such as situation, if we knew <span class="math inline">\(\vec{x}\)</span> then we can replace the above problem with <span class="math inline">\(\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{x} \ | \ \theta))\)</span>. In fact, we wouldn’t even need to know exactly what the value of <span class="math inline">\(\vec{x}\)</span> is but only what the value of <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> is for a given <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="a-general-recipe-for-em-algorithms" class="section level2">
<h2>A general recipe for EM algorithms</h2>
<p>The idea of EM is indeed to try and maximize <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> instead of <span class="math inline">\(\log(p(\vec{y} \ | \ \theta))\)</span>, but because we do not know <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> to instead use an approximation/estimate of it.</p>
<p>How to approximate such an expression? The quantity <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> is a random variable (depending on the unknown value <span class="math inline">\(\vec{x}\)</span> of <span class="math inline">\(\vec{X}\)</span>). To estimate it in a meaningful way we need to use the most informative distribution related to <span class="math inline">\(\vec{x}\)</span>. Because we know <span class="math inline">\(\vec{y}\)</span> the best such distribution is <span class="math inline">\(p(\vec{x}|\vec{y},\theta)\)</span>.</p>
<p>The problem is this distribution (or any other) will necessarily depend <span class="math inline">\(\theta\)</span>, which we do not know! At first this seems like a circular trap, because to estimate <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> using <span class="math inline">\(p(\vec{x}|\vec{y},\theta)\)</span> we need to know <span class="math inline">\(\theta\)</span>, but to estimate <span class="math inline">\(\theta\)</span> we need to know <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span>. However the trap hints at a solution: simply alternate between estimating the random variable <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> using a current guess of <span class="math inline">\(\theta\)</span> and then use this updated estimate of <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> to update our guess of <span class="math inline">\(\theta\)</span>. More formally we can summarize EM in 5 steps:</p>
<ul>
<li><p><strong>Step 1:</strong> Let <span class="math inline">\(m = 0\)</span>. Make an initial estimate <span class="math inline">\(\theta_m\)</span> for <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><strong>Step 2:</strong> Given the observed data <span class="math inline">\(\vec{y}\)</span> and pretending for the moment that our current guess <span class="math inline">\(\theta_m\)</span> is correct, construct the conditional probability distribution <span class="math inline">\(p(\vec{x}|\vec{y},\theta_m)\)</span> of the hidden data <span class="math inline">\(\vec{x}\)</span> given all known information.</p></li>
<li><p><strong>Step 3:</strong> Using the distribution <span class="math inline">\(p(\vec{x}|\vec{y},\theta_m)\)</span> construct an estimator/approximation of the desired log-likelihood <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span> for arbitrary <span class="math inline">\(\theta\)</span>. We denote this approximation by <span class="math inline">\(Q(\theta|\theta_m)\)</span>.</p></li>
<li><p><strong>Step 4:</strong> Set <span class="math inline">\(\theta_{m+1}\)</span> equal to a value of <span class="math inline">\(\theta\)</span> that maximizes the current approximation <span class="math inline">\(Q(\theta|\theta_m)\)</span> of <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span>.</p></li>
<li><p><strong>Step 5:</strong> Return to step 2 and repeat until some stopping criteria is met.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p></li>
</ul>
<p>Practically speaking, this algorithm would be applied when each of these steps is significantly easier than the original MLE problem of <span class="math inline">\(\hat{\theta} := \text{argmax}_{\theta} \ \ \log(p(\vec{y} \ | \ \theta))\)</span>. As a general example, this is often the case when the model is linear with respect to <span class="math inline">\(\vec{X}\)</span>, but the information loss of going from <span class="math inline">\(\vec{X}\)</span> to <span class="math inline">\(\vec{Y}\)</span> is nonlinear and non-invertible (we’ll give examples in later posts).</p>
</div>
<div id="constructing-an-estimator-for-logpvecx-theta" class="section level2">
<h2>Constructing an estimator for <span class="math inline">\(\log(p(\vec{X} \ | \ \theta))\)</span></h2>
<p>How do we fill in the blank left by step 3 above? That is, how do we use the probability density <span class="math inline">\(p(\vec{x}|\vec{y},\theta_m)\)</span> to estimate the value of the random variable <span class="math inline">\(\log(p(\vec{X} \ | \ \theta))\)</span>? Two possibilities come to mind.</p>
<div id="point-estimate-type-em" class="section level3">
<h3>Point-estimate type EM</h3>
<p>One possibility is to let
<span class="math display">\[
\vec{x}_m = \vec{x}_m(\vec{y}, \theta_m) := \text{argmax}_{\vec{x}} \ p(\vec{x}|\vec{y},\theta_m)
\]</span>
and then define
<span class="math display">\[
Q(\theta | \theta_m) := \log(p(\vec{x}_m \ | \ \theta))
\]</span>
This is called point-estimate EM. Here we use “the most likely” value of <span class="math inline">\(\vec{X}\)</span> as determined by the density <span class="math inline">\(p(\vec{x}|\vec{y},\theta_m)\)</span> and then impute this value into our log-likelihood that we want to maximize. Another possibility would be to let</p>
<p><span class="math display">\[
\vec{x}_m = \vec{x}_m(\vec{y}, \theta_m) := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \vec{X}\big]
\]</span>
and let <span class="math inline">\(Q\)</span> be as before.</p>
<p>The idea of these type of EM algorithms is to first estimate the missing data <span class="math inline">\(\vec{x}\)</span> and then impute the result into <span class="math inline">\(log(p(\vec{x}\ |\ \theta))\)</span>.</p>
</div>
<div id="expectation-em-i.e.-standard-em" class="section level3">
<h3>Expectation EM (i.e. standard EM)</h3>
<p>There is a theoretically more elegant way. As mentioned earlier, we do not in fact need an estimate of the missing data <span class="math inline">\(\vec{x}\)</span>. One of the best ways to estimate the value of a random variable with respect to a conditional distribution is to simply compute the conditional expectation of that variable with respect to that conditional distribution:</p>
<p><span class="math display">\[
Q(\theta | \theta_m) \ := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] = \int_{\mathcal{X}} \log(p(\vec{x} \ | \ \theta)) \ p(\vec{x}|\vec{y},\theta_m) \ d\vec{x} 
\]</span>
Here we’re computing the mean of <span class="math inline">\(\log(p(\vec{X} \ | \ \theta))\)</span> with respect to the density <span class="math inline">\(p(\vec{x}|\vec{y},\theta_m)\)</span>. As is common when using expectations, this second method has some advantages we’ll see later. When we refer to EM we will always mean this case, unless otherwise specified.</p>
</div>
</div>
<div id="qthetatheta_m-for-i.i.d.-samples" class="section level2">
<h2><span class="math inline">\(Q(\theta|\theta_m)\)</span> for I.I.D. samples</h2>
<p>So far everything has been rather general, applying to any random vectors <span class="math inline">\(\vec{X}\)</span> and <span class="math inline">\(\vec{Y}\)</span>. Many problems assume that data are generated independently and identically distributed (I.I.D.) so it’s helpful to have a formulation for this particular case.</p>
<p><strong>Proposition:</strong> Suppose that the components of <span class="math inline">\(\vec{X}\)</span> are IID (given <span class="math inline">\(\theta\)</span>), that is:
<span class="math display">\[
p(\vec{x}|\theta) = \prod_{i = 1}^N p(x_i|\theta) \  \qquad \forall x, \theta
\]</span>
Suppose also that the dependence structure <span class="math inline">\(\theta \to \vec{X} \to \vec{Y}\)</span> splits into subgraphs as <span class="math inline">\(\theta \to X_i \to Y_i\)</span> for all <span class="math inline">\(i = 1, 2, ..., N\)</span>. This just means that given <span class="math inline">\(X_i\)</span>, the distribution <span class="math inline">\(Y_i\)</span> is independent of all other variables:
<span class="math display">\[
p(y_i|\vec{x}, \theta, y_1, y_2, ..., y_{i-1}, y_{i+1}, ..., y_N) = p(y_i|x_i)
\]</span>
Then <span class="math inline">\(Q(\theta|\theta_m):= \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m} \big[ \log(p(\vec{X} \ | \ \theta)) \big]\)</span> can be written as:
<span class="math display">\[
Q(\theta|\theta_m) = \sum_{i=1}^N Q_i(\theta|\theta_m)
\]</span>
where
<span class="math display">\[
Q_i(\theta|\theta_m) := \ \text{E}_{X_i \ | \ Y_i \ = \ y_i, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big].
\]</span>
<strong>Proof:</strong> The proof begins by showing that the joint elements <span class="math inline">\((X_i,Y_i)\)</span> are independent across <span class="math inline">\(i\)</span>, that is:
<span class="math display">\[
p(\vec{x},\vec{y}|\theta) = \prod_{i = 1}^N p(x_i, y_i|\theta). 
\]</span>
To prove this we start by applying the multiplication theorem for probability densities:</p>
<p><span class="math display">\[
p(\vec{x},\vec{y}|\theta) = p(y_1|y_2,y_3, ..., y_N, \vec{x},\theta)...p(y_N|\vec{x},\theta)p(\vec{x}|\theta) \qquad \text{(by the multiplication theorem)}
\]</span></p>
<p><span class="math display">\[
= p(y_1|x_1,\theta)...p(y_N|x_N,\theta)p(\vec{x}|\theta) \qquad \text{(by conditional independence but keeping theta)}
\]</span></p>
<p><span class="math display">\[
= p(\vec{x}|\theta)\prod_{i=1}^Np(y_i|x_i,\theta) 
\]</span></p>
<p><span class="math display">\[
=\prod_{i=1}^Np(y_i|x_i,\theta)p(x_i|\theta) \qquad \text{(by independence of the x&#39;s)}
\]</span></p>
<p><span class="math display">\[
= \prod_{i=1}^Np(y_i,x_i|\theta).
\]</span></p>
<p>Next we have that for each <span class="math inline">\(i\)</span></p>
<p><span class="math display">\[
  p(x_i|\vec{y},\theta) = \frac{p(x_i,\vec{y}|\theta)}{p(\vec{y}|\theta)} \qquad \text{(by Bayes)}
\]</span></p>
<p><span class="math display">\[
= \frac{\int p(\vec{x},\vec{y}|\theta)dx_1...dx_{i-1}dx_{i+1}...dx_n}{\int p(\vec{x},\vec{y}|\theta)d\vec{x}}
\]</span></p>
<p><span class="math display">\[
= \frac{\int \prod_{j=1}^Np(y_j,x_j|\theta)dx_1...dx_{i-1}dx_{i+1}...dx_n}{\int \prod_{j=1}^Np(y_j,x_j|\theta)d\vec{x}}
\]</span></p>
<p><span class="math display">\[
= \frac{p(x_i,y_i|\theta) \prod_{j=1, j \ne i}^N \int p(y_j,x_j|\theta)dx_j}{\prod_{j=1}^N\int p(y_j,x_j|\theta) dx_j}
\]</span></p>
<p><span class="math display">\[
= \frac{p(x_i,y_i|\theta) \prod_{j=1, j \ne i}^N p(y_j|\theta)}{\prod_{j=1}^N p(y_j|\theta)}
\]</span></p>
<p><span class="math display">\[
= \frac{p(x_i,y_i|\theta)}{p(y_i|\theta)}
\]</span>
<span class="math display">\[
= p(x_i|y_i,\theta)
\]</span></p>
<p>Hence <span class="math inline">\(p(x_i|\vec{y},\theta) = p(x_i|y_i,\theta)\)</span>. Therefore we have</p>
<p><span class="math display">\[
Q(\theta, \theta_m) = \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big]
\]</span></p>
<p><span class="math display">\[
= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(\prod_{i = 1}^N p(X_i \ | \ \theta)) \big]
\]</span></p>
<p><span class="math display">\[
= \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \sum_{i = 1}^N \log(p(X_i \ | \ \theta)) \big]
\]</span></p>
<p><span class="math display">\[
= \sum_{i = 1}^N \text{E}_{X_i \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big]
\]</span></p>
<p><span class="math display">\[
= \sum_{i = 1}^N \text{E}_{X_i \ | \ Y_i \ = \ y_i, \ \theta_m}  \big[ \log(p(X_i \ | \ \theta)) \big] = \sum_{i=1}^N Q_i(\theta|\theta_m)
\]</span></p>
<p>Where we used <span class="math inline">\(p(x_i|\vec{y},\theta) = p(x_i|y_i,\theta)\)</span> in the 2nd to last equality. <strong>QED</strong></p>
</div>
<div id="maximum-a-posteriori-em-and-regularizing-priors" class="section level2">
<h2>Maximum A Posteriori EM and regularizing priors</h2>
<p>The EM algorithm is easily extendable to regularized MLE. This is usually referred to as Maximum A Posteriori (MAP) in the Bayesian setting, which we adopt. Here the penalty term is interpreted as the log of the prior density on <span class="math inline">\(\theta\)</span>, and the function to be maximized is the posterior density of <span class="math inline">\(\theta\)</span> given the data. By Bayes’ Theorem
<span class="math display">\[
p(\theta|\vec{y}) \propto p(\vec{y}|\theta)p(\theta)
\]</span></p>
<p>where the proportionality constant is independent of <span class="math inline">\(\theta\)</span>. Thus the MAP estimator is
<span class="math display">\[
\hat{\theta}_{MAP} := \text{argmax}_{\theta} \ \log(p(\theta|\vec{y})) \ = \text{argmax}_{\theta} \ \log(p(\vec{y}|\theta)) + \log(p(\theta))
\]</span>
The way to extend EM to this situation is clear (at least formally): Simply replace the maximization step (step 4 above) in EM with maximizing <span class="math inline">\(Q(\theta|\theta_m) + \log(p(\theta))\)</span> instead of simply <span class="math inline">\(Q(\theta|\theta_m)\)</span>:</p>
<p><span class="math display">\[
\theta_{m+1} := \text{argmax}_{\theta} \ \ \ Q(\theta|\theta_m) + \log(p(\theta))
\]</span>
where as before <span class="math inline">\(Q\)</span> is given by</p>
<p><span class="math display">\[
Q(\theta | \theta_m) \ := \ \text{E}_{\vec{X} \ | \ \vec{Y} \ = \ \vec{y}, \ \theta_m}  \big[ \log(p(\vec{X} \ | \ \theta)) \big] = \int_{\mathcal{X}} \log(p(\vec{x} \ | \ \theta)) \ p(\vec{x}|\vec{y},\theta_m) \ d\vec{x} 
\]</span></p>
</div>
<div id="monotonicity-of-the-em-algorithm" class="section level2">
<h2>Monotonicity of the EM algorithm</h2>
<p>At this point the reader has all the theory needed to begin applying EM where they believe it’s a good fit. Before we end the post though let’s mention at least one result that shows that EM is indeed a generalization of MLE to the case of hidden data: under relatively weak assumptions of the algorithm causes the log-likelihood <span class="math inline">\(\log(p(\vec{y}|\theta_m))\)</span> to be an <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Proof_of_correctness">increasing sequence in <span class="math inline">\(m\)</span></a>.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Step 2 allows for a whole family of such algorithms, one for each possible approximator to <span class="math inline">\(\log(p(\vec{x} \ | \ \theta))\)</span>. Step 4 can also be generalized. Since the value of <span class="math inline">\(\theta_{m+1}\)</span> maximizes <span class="math inline">\(Q(\theta | \theta_m)\)</span> then <span class="math inline">\(Q(\theta_{m+1} | \theta_m) \ge Q(\theta_m | \theta_m)\)</span>. Instead of seeking to maximize <span class="math inline">\(Q(\theta | \theta_m)\)</span> we may simply seek a value of <span class="math inline">\(\theta_{m+1}\)</span> that improves on <span class="math inline">\(\theta_m\)</span> in the sense of this inequality. For step 5, the stopping criteria are up to the implementer.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
</content:encoded>	
    </item>
    
  </channel>
</rss>
